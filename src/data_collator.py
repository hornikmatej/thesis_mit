from typing import List, Dict, Union, Any
import os
import subprocess
import logging
import re

import torch
import pandas as pd
import numpy as np
import io
import wandb
import soundfile as sf

logger_collator = logging.getLogger(__name__)


def parse_sclite_dtl_file(dtl_file_path: str):
    """
    Parses the sclite .dtl output file and extracts relevant metrics.

    Args:
        dtl_file_path: The path to the .dtl file generated by sclite.

    Returns:
        A dictionary containing the parsed metrics (substitutions, deletions, insertions).
        Returns None if parsing fails or the file doesn't exist.
    """
    metrics = {}
    try:
        with open(dtl_file_path, "r") as f:
            sclite_output = f.read()

        # Use regular expressions to find the percentages (same as before)
        sub_match = re.search(r"Percent Substitution\s*=\s*([\d.]+)%", sclite_output)
        del_match = re.search(r"Percent Deletions\s*=\s*([\d.]+)%", sclite_output)
        ins_match = re.search(r"Percent Insertions\s*=\s*([\d.]+)%", sclite_output)

        if sub_match:
            metrics["substitutions"] = float(sub_match.group(1))
        if del_match:
            metrics["deletions"] = float(del_match.group(1))
        if ins_match:
            metrics["insertions"] = float(ins_match.group(1))

        # Extract Sentence error rate
        sent_err_match = re.search(r"with errors\s+([\d.]+)%", sclite_output)
        if sent_err_match:
            metrics["sentence_errors"] = float(sent_err_match.group(1))

        # Extract Word error rate
        word_err_match = re.search(
            r"Percent Total Error\s*=\s*([\d.]+)%", sclite_output
        )
        if word_err_match:
            metrics["word_errors"] = float(word_err_match.group(1))

        # Extract Word accuracy
        word_acc_match = re.search(
            r"Percent Word Accuracy\s*=\s*([\d.]+)%", sclite_output
        )
        if word_acc_match:
            metrics["word_accuracy"] = float(word_acc_match.group(1))

    except FileNotFoundError:
        logger_collator.warning(f"Sclite .dtl file not found: {dtl_file_path}")
        return None
    except (ValueError, AttributeError) as e:
        logger_collator.warning(f"Failed to parse sclite output: {e}")
        return None

    return metrics


class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Enhanced data collator that includes additional debugging and evaluation capabilities using SCTK.

    Args:
        processor: The processor used for processing the data
        decoder_start_token_id: The begin-of-sentence token ID of the decoder
        forward_attention_mask: Whether to return attention_mask
        debug_output_dir: Directory to save debug files
        sclite_path: Path to the SCLITE executable
    """

    def __init__(
        self,
        processor: Any,
        decoder_start_token_id: int,
        forward_attention_mask: bool,
        log_level: int,
        debug_output_dir: str = "debug_output",
        sclite_path: str = "/home/matej/fitvut/dp_mit/SCTK/bin/sclite",
    ):
        self.processor = processor
        self.decoder_start_token_id = decoder_start_token_id
        self.forward_attention_mask = forward_attention_mask
        self.debug_output_dir = debug_output_dir
        self.sclite_path = sclite_path
        self.sample_counter = 0
        self.log_level = log_level
        # Create debug output directory if it doesn't exist
        os.makedirs(debug_output_dir, exist_ok=True)

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # Original collation logic
        model_input_name = self.processor.model_input_names[0]
        input_features = [
            {model_input_name: feature[model_input_name]} for feature in features
        ]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        if self.log_level == logging.DEBUG:
            if not self.validate_input_features(input_features):
                raise ValueError("Invalid input features")
            if not self.validate_labels(label_features):
                raise ValueError("Invalid label features")

        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )

        if self.forward_attention_mask:
            batch["attention_mask"] = torch.LongTensor(
                [feature["attention_mask"] for feature in features]
            )

        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # remove the start token from the labels
        # if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
        #     labels = labels[:, 1:]

        batch["labels"] = labels

        if self.log_level == logging.DEBUG:
            debug_info = self.debug_sample(batch, features, every_n_batches=100)
            logger_collator.warning(f"Debug info: {debug_info}")

        # Save the audio sample for debugging
        audio_path = os.path.join(
            self.debug_output_dir, f"debug_sample_{self.sample_counter}.mp3"
        )
        os.makedirs(os.path.dirname(audio_path), exist_ok=True)
        if self.log_level == logging.DEBUG:
            with io.BytesIO() as buffer:
                logger_collator.debug(f"{batch=}")
                sf.write(buffer, batch["input_values"][0], 16000, format="MP3")
                with open(audio_path, "wb") as f:
                    f.write(buffer.getvalue())
                # Save the decoded text for debugging
                decoded_text_path = os.path.join(
                    self.debug_output_dir, f"decoded_text_{self.sample_counter}.txt"
                )
                with open(decoded_text_path, "w") as f:
                    sample_labels = batch["labels"][0][batch["labels"][0] != -100]
                    decoded_text = self.processor.tokenizer.decode(sample_labels)
                    f.write(decoded_text)
                logger_collator.debug(
                    f"Saving sample audio to {audio_path} and decoded text to {decoded_text_path}"
                )
        return batch

    def validate_input_features(self, input_features):
        """Validate input features"""
        if not input_features:
            logger_collator.error("Empty input features!")
            return False

        for feat in input_features:
            if self.processor.model_input_names[0] not in feat:
                logger_collator.error(
                    f"Missing {self.processor.model_input_names[0]} in input features"
                )
                return False

            input_values = feat[self.processor.model_input_names[0]]
            if not isinstance(input_values, (np.ndarray, list)):
                logger_collator.error(
                    f"Invalid input_values type: {type(input_values)}"
                )
                return False

            # Check for NaN or infinity values
            if isinstance(input_values, np.ndarray) and (
                np.isnan(input_values).any() or np.isinf(input_values).any()
            ):
                logger_collator.error("Found NaN or infinity values in input_values")
                return False

        return True

    def validate_labels(self, label_features):
        """Validate label features"""
        if not label_features:
            logger_collator.error("Empty label features!")
            return False

        for feat in label_features:
            if "input_ids" not in feat:
                logger_collator.error("Missing input_ids in label features")
                return False

            if not isinstance(feat["input_ids"], (list, np.ndarray)):
                logger_collator.error(f"Invalid labels type: {type(feat['input_ids'])}")
                return False

        return True

    def debug_sample(self, batch, features, every_n_batches=100):
        """Save detailed debug information for a batch"""
        if self.sample_counter % every_n_batches == 0:  # Debug every 100th batch
            debug_info = {
                "batch_size": len(features),
                "input_shape": batch["input_values"].shape,
                "label_shape": batch["labels"].shape,
                "input_stats": {
                    "mean": batch["input_values"].float().mean().item(),
                    "std": batch["input_values"].float().std().item(),
                    "min": batch["input_values"].float().min().item(),
                    "max": batch["input_values"].float().max().item(),
                },
                "label_stats": {
                    "sequence_lengths": (batch["labels"] != -100).sum(1).tolist(),
                    "unique_tokens": len(
                        torch.unique(batch["labels"][batch["labels"] != -100])
                    ),
                },
                "memory_usage": {
                    "input_values_mb": batch["input_values"].element_size()
                    * batch["input_values"].nelement()
                    / (1024 * 1024),
                    "labels_mb": batch["labels"].element_size()
                    * batch["labels"].nelement()
                    / (1024 * 1024),
                },
            }

            # Decode a sample of labels to check tokenization
            if hasattr(self.processor, "tokenizer"):
                sample_labels = batch["labels"][0][batch["labels"][0] != -100]
                decoded_text = self.processor.tokenizer.decode(sample_labels)
                logger_collator.info(f"Sample decoded text: {decoded_text}")

        self.sample_counter += 1
        return debug_info if self.sample_counter % every_n_batches == 0 else None

    def save_debug_info(
        self, pred_str: List[str], label_str: List[str], batch_idx: int = 0
    ):
        """
        Save debug information and run SCLITE evaluation.

        Args:
            pred_str: List of prediction strings
            label_str: List of reference/label strings
            batch_idx: Batch index for unique file naming
        """
        # Save predictions and references to CSV
        debug_path = os.path.join(self.debug_output_dir, f"debug_batch_{batch_idx}.csv")
        df = pd.DataFrame({"label": label_str, "prediction": pred_str})
        df.to_csv(debug_path, index=False)

        # Create SCLITE format files (.trn)
        sclite_files = [
            debug_path.replace(".csv", f"_{type}.trn") for type in ["hyp", "ref"]
        ]

        # Save hypothesis and reference files in SCLITE format
        for strings, file_to_save in zip([pred_str, label_str], sclite_files):
            with open(file_to_save, "w") as file_handler:
                for index, string in enumerate(strings):
                    file_handler.write(f"{string} (utterance_{index})\n")

        # Run SCLITE evaluation
        dtl_file_path = debug_path.replace(
            ".csv", "_hyp.trn.dtl"
        )  # Construct .dtl file path
        sclite_cmd = f"{self.sclite_path} -F -D -i wsj -r {sclite_files[1]} trn -h {sclite_files[0]} trn -o snt sum dtl"
        logger_collator.info(f"Running SCLITE evaluation with command: {sclite_cmd}")
        process = subprocess.Popen(sclite_cmd.split())  # nosec

        try:
            process.wait(30)  # Wait up to 30 seconds for SCLITE to complete
        except subprocess.TimeoutExpired:
            process.kill()
            logger_collator.warning("Sclite evaluation timed out.")

        # Parse sclite output from the .dtl file
        metrics = parse_sclite_dtl_file(dtl_file_path)

        # Log to WandB
        if metrics:
            wandb.log(metrics, step=batch_idx)  # Log with batch index as step
            logger_collator.info(f"Logged SCLITE metrics to WandB: {metrics}")
        else:
            logger_collator.warning(
                "Skipping WandB logging due to parsing failure or missing file."
            )

        return debug_path, sclite_files
