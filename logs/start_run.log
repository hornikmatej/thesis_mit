Singularity> python -m poetry run ./run_voxpopuli.sh
/auto/brno2/home/xhorni20/dp_mit/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
10/15/2024 01:21:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True
10/15/2024 01:21:50 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=400,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=input_length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./seq2seq_wav2vec2_bart-base/training/runs/Oct15_01-21-50_fau1.natur.cuni.cz,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=4,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=2.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=./seq2seq_wav2vec2_bart-base/training,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./seq2seq_wav2vec2_bart-base/training,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=150,
weight_decay=0.0,
)
voxpopuli.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.84k/8.84k [00:00<00:00, 41.7MB/s]
README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.7k/10.7k [00:00<00:00, 56.2MB/s]
data/n_files.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29k/1.29k [00:00<00:00, 8.57MB/s]
asr_dev.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637k/637k [00:00<00:00, 6.52MB/s]
asr_test.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 649k/649k [00:00<00:00, 6.47MB/s]
asr_train.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69.1M/69.1M [00:00<00:00, 82.9MB/s]
test_part_0.tar.gz:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 461M/595M [00:11<00:03, 40.3MB/s]
test_part_0.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595M/595M [00:15<00:00, 39.6MB/s]
dev_part_0.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591M/591M [00:15<00:00, 37.6MB/s]
train_part_0.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:26<00:00, 63.5MB/s]
train_part_1.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.71G/1.71G [00:17<00:00, 99.0MB/s]
train_part_2.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:16<00:00, 99.2MB/s]
train_part_3.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.71G/1.71G [00:17<00:00, 97.0MB/s]
train_part_4.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:17<00:00, 98.4MB/s]
train_part_5.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 94.1MB/s]
train_part_6.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 97.0MB/s]
train_part_7.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 96.5MB/s]
train_part_8.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:17<00:00, 95.8MB/s]
train_part_9.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 98.9MB/s]
train_part_10.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 95.8MB/s]
train_part_11.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.67G/1.67G [00:16<00:00, 100MB/s]
train_part_12.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 96.5MB/s]
train_part_13.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 99.1MB/s]
train_part_14.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 96.4MB/s]
train_part_15.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:17<00:00, 96.6MB/s]
train_part_16.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 97.0MB/s]
train_part_17.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 95.9MB/s]
train_part_18.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.71G/1.71G [00:17<00:00, 96.3MB/s]
train_part_19.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 97.4MB/s]
train_part_20.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 95.7MB/s]
train_part_21.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 97.7MB/s]
train_part_22.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.66G/1.66G [00:17<00:00, 96.5MB/s]
train_part_23.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.71G/1.71G [00:17<00:00, 96.0MB/s]
train_part_24.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.73G/1.73G [00:17<00:00, 97.5MB/s]
train_part_25.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 98.9MB/s]
train_part_26.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 97.8MB/s]
train_part_27.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:17<00:00, 95.7MB/s]
train_part_28.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 96.0MB/s]
train_part_29.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 98.0MB/s]
train_part_30.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68G/1.68G [00:17<00:00, 96.4MB/s]
train_part_31.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 98.8MB/s]
train_part_32.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.69G/1.69G [00:17<00:00, 96.4MB/s]
train_part_33.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.66G/1.66G [00:17<00:00, 97.2MB/s]
train_part_34.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.70G/1.70G [00:17<00:00, 95.9MB/s]
train_part_35.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.67G/1.67G [00:16<00:00, 98.6MB/s]
train_part_36.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 850M/850M [00:08<00:00, 101MB/s]
Setting num_proc from 32 back to 1 for the train split to disable multiprocessing as it only contains one shard.
10/15/2024 01:48:12 - WARNING - datasets.builder - Setting num_proc from 32 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 182482 examples [11:02, 275.46 examples/s]
Setting num_proc from 32 back to 1 for the validation split to disable multiprocessing as it only contains one shard.
10/15/2024 01:59:15 - WARNING - datasets.builder - Setting num_proc from 32 back to 1 for the validation split to disable multiprocessing as it only contains one shard.
Generating validation split: 1753 examples [00:06, 275.55 examples/s]
Setting num_proc from 32 back to 1 for the test split to disable multiprocessing as it only contains one shard.
10/15/2024 01:59:21 - WARNING - datasets.builder - Setting num_proc from 32 back to 1 for the test split to disable multiprocessing as it only contains one shard.
Generating test split: 1842 examples [00:06, 290.26 examples/s]
[INFO|configuration_utils.py:673] 2024-10-15 01:59:29,569 >> loading configuration file ./seq2seq_wav2vec2_bart-base/config.json
[INFO|configuration_utils.py:742] 2024-10-15 01:59:29,726 >> Model config SpeechEncoderDecoderConfig {
  "_name_or_path": "./seq2seq_wav2vec2_bart-base",
  "architectures": [
    "SpeechEncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "facebook/bart-base",
    "activation_dropout": 0.1,
    "activation_function": "gelu",
    "add_bias_logits": false,
    "add_cross_attention": true,
    "add_final_layer_norm": false,
    "architectures": [
      "BartModel"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "classif_dropout": 0.1,
    "classifier_dropout": 0.0,
    "cross_attention_hidden_size": null,
    "d_model": 768,
    "decoder_attention_heads": 12,
    "decoder_ffn_dim": 3072,
    "decoder_layerdrop": 0.0,
    "decoder_layers": 6,
    "decoder_start_token_id": 2,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": true,
    "encoder_attention_heads": 12,
    "encoder_ffn_dim": 3072,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 6,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": 0,
    "forced_eos_token_id": 2,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1",
      "2": "LABEL_2"
    },
    "init_std": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1,
      "LABEL_2": 2
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 1024,
    "min_length": 0,
    "model_type": "bart",
    "no_repeat_ngram_size": 3,
    "normalize_before": false,
    "normalize_embedding": true,
    "num_beam_groups": 1,
    "num_beams": 4,
    "num_hidden_layers": 6,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_embedding": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": {
      "summarization": {
        "length_penalty": 1.0,
        "max_length": 128,
        "min_length": 12,
        "num_beams": 4
      },
      "summarization_cnn": {
        "length_penalty": 2.0,
        "max_length": 142,
        "min_length": 56,
        "num_beams": 4
      },
      "summarization_xsum": {
        "length_penalty": 1.0,
        "max_length": 62,
        "min_length": 11,
        "num_beams": 6
      }
    },
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": "float32",
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 50265
  },
  "decoder_start_token_id": 0,
  "encoder": {
    "_name_or_path": "facebook/wav2vec2-base",
    "activation_dropout": 0.0,
    "adapter_attn_dim": null,
    "adapter_kernel_size": 3,
    "adapter_stride": 2,
    "add_adapter": true,
    "add_cross_attention": false,
    "apply_spec_augment": true,
    "architectures": [
      "Wav2Vec2ForPreTraining"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 1,
    "chunk_size_feed_forward": 0,
    "classifier_proj_size": 256,
    "codevector_dim": 256,
    "contrastive_logits_temperature": 0.1,
    "conv_bias": false,
    "conv_dim": [
      512,
      512,
      512,
      512,
      512,
      512,
      512
    ],
    "conv_kernel": [
      10,
      3,
      3,
      3,
      3,
      2,
      2
    ],
    "conv_stride": [
      5,
      2,
      2,
      2,
      2,
      2,
      2
    ],
    "cross_attention_hidden_size": null,
    "ctc_loss_reduction": "sum",
    "ctc_zero_infinity": false,
    "decoder_start_token_id": null,
    "diversity_loss_weight": 0.1,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "do_stable_layer_norm": false,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "feat_extract_activation": "gelu",
    "feat_extract_norm": "group",
    "feat_proj_dropout": 0.0,
    "feat_quantizer_dropout": 0.0,
    "final_dropout": 0.0,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "freeze_feat_extract_train": true,
    "hidden_act": "gelu",
    "hidden_dropout": 0.1,
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "layerdrop": 0.0,
    "length_penalty": 1.0,
    "mask_channel_length": 10,
    "mask_channel_min_space": 1,
    "mask_channel_other": 0.0,
    "mask_channel_prob": 0.0,
    "mask_channel_selection": "static",
    "mask_feature_length": 10,
    "mask_feature_min_masks": 0,
    "mask_feature_prob": 0.0,
    "mask_time_length": 10,
    "mask_time_min_masks": 2,
    "mask_time_min_space": 1,
    "mask_time_other": 0.0,
    "mask_time_prob": 0.0,
    "mask_time_selection": "static",
    "max_length": 20,
    "min_length": 0,
    "model_type": "wav2vec2",
    "no_mask_channel_overlap": false,
    "no_mask_time_overlap": false,
    "no_repeat_ngram_size": 0,
    "num_adapter_layers": 3,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_codevector_groups": 2,
    "num_codevectors_per_group": 320,
    "num_conv_pos_embedding_groups": 16,
    "num_conv_pos_embeddings": 128,
    "num_feat_extract_layers": 7,
    "num_hidden_layers": 12,
    "num_negatives": 100,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_size": 768,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "proj_codevector_dim": 256,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": null,
    "tdnn_dilation": [
      1,
      2,
      3,
      1,
      1
    ],
    "tdnn_dim": [
      512,
      512,
      512,
      512,
      1500
    ],
    "tdnn_kernel": [
      5,
      3,
      3,
      1,
      1
    ],
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_weighted_layer_sum": false,
    "vocab_size": 32,
    "xvector_output_dim": 512
  },
  "eos_token_id": 2,
  "is_encoder_decoder": true,
  "max_length": null,
  "model_type": "speech-encoder-decoder",
  "pad_token_id": 1,
  "processor_class": "Wav2Vec2Processor",
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": false
}

[INFO|feature_extraction_utils.py:547] 2024-10-15 01:59:29,737 >> loading configuration file ./seq2seq_wav2vec2_bart-base/preprocessor_config.json
[INFO|feature_extraction_utils.py:596] 2024-10-15 01:59:29,749 >> Feature extractor Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 01:59:29,920 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3729] 2024-10-15 01:59:30,178 >> loading weights file ./seq2seq_wav2vec2_bart-base/model.safetensors
[WARNING|logging.py:328] 2024-10-15 01:59:30,225 >> SpeechEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
[INFO|configuration_utils.py:1099] 2024-10-15 01:59:30,226 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 2,
  "pad_token_id": 1,
  "use_cache": false
}

[INFO|configuration_utils.py:1099] 2024-10-15 01:59:37,677 >> Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1
}

[WARNING|modeling_utils.py:4564] 2024-10-15 01:59:39,566 >> Some weights of the model checkpoint at ./seq2seq_wav2vec2_bart-base were not used when initializing SpeechEncoderDecoderModel: ['encoder.masked_spec_embed']
- This IS expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4582] 2024-10-15 01:59:39,566 >> All the weights of SpeechEncoderDecoderModel were initialized from the model checkpoint at ./seq2seq_wav2vec2_bart-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechEncoderDecoderModel for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-10-15 01:59:39,579 >> loading configuration file ./seq2seq_wav2vec2_bart-base/generation_config.json
[INFO|configuration_utils.py:1099] 2024-10-15 01:59:39,580 >> Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 0,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "max_length": 40,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "use_cache": false
}

preprocess train dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182482/182482 [04:20<00:00, 699.55 examples/s]
preprocess train dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1753/1753 [00:29<00:00, 59.43 examples/s]
Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182482/182482 [00:00<00:00, 420505.16 examples/s]
Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1753/1753 [00:00<00:00, 4537.65 examples/s]
Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.49k/4.49k [00:00<00:00, 2.59MB/s]
[INFO|feature_extraction_utils.py:435] 2024-10-15 02:04:39,011 >> Feature extractor saved in ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|tokenization_utils_base.py:2641] 2024-10-15 02:04:39,035 >> tokenizer config file saved in ./seq2seq_wav2vec2_bart-base/training/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-10-15 02:04:39,047 >> Special tokens file saved in ./seq2seq_wav2vec2_bart-base/training/special_tokens_map.json
[INFO|configuration_utils.py:410] 2024-10-15 02:04:39,269 >> Configuration saved in ./seq2seq_wav2vec2_bart-base/training/config.json
[INFO|image_processing_base.py:373] 2024-10-15 02:04:39,284 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|feature_extraction_utils.py:547] 2024-10-15 02:04:39,291 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|configuration_utils.py:673] 2024-10-15 02:04:39,307 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/config.json
[INFO|configuration_utils.py:742] 2024-10-15 02:04:39,311 >> Model config SpeechEncoderDecoderConfig {
  "_name_or_path": "./seq2seq_wav2vec2_bart-base/training",
  "architectures": [
    "SpeechEncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "facebook/bart-base",
    "activation_dropout": 0.1,
    "activation_function": "gelu",
    "add_bias_logits": false,
    "add_cross_attention": true,
    "add_final_layer_norm": false,
    "architectures": [
      "BartModel"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "classif_dropout": 0.1,
    "classifier_dropout": 0.0,
    "cross_attention_hidden_size": null,
    "d_model": 768,
    "decoder_attention_heads": 12,
    "decoder_ffn_dim": 3072,
    "decoder_layerdrop": 0.0,
    "decoder_layers": 6,
    "decoder_start_token_id": 2,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": true,
    "encoder_attention_heads": 12,
    "encoder_ffn_dim": 3072,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 6,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": 0,
    "forced_eos_token_id": 2,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1",
      "2": "LABEL_2"
    },
    "init_std": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1,
      "LABEL_2": 2
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 1024,
    "min_length": 0,
    "model_type": "bart",
    "no_repeat_ngram_size": 3,
    "normalize_before": false,
    "normalize_embedding": true,
    "num_beam_groups": 1,
    "num_beams": 4,
    "num_hidden_layers": 6,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_embedding": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": {
      "summarization": {
        "length_penalty": 1.0,
        "max_length": 128,
        "min_length": 12,
        "num_beams": 4
      },
      "summarization_cnn": {
        "length_penalty": 2.0,
        "max_length": 142,
        "min_length": 56,
        "num_beams": 4
      },
      "summarization_xsum": {
        "length_penalty": 1.0,
        "max_length": 62,
        "min_length": 11,
        "num_beams": 6
      }
    },
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": "float32",
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 50265
  },
  "decoder_start_token_id": 0,
  "encoder": {
    "_name_or_path": "facebook/wav2vec2-base",
    "activation_dropout": 0.0,
    "adapter_attn_dim": null,
    "adapter_kernel_size": 3,
    "adapter_stride": 2,
    "add_adapter": true,
    "add_cross_attention": false,
    "apply_spec_augment": true,
    "architectures": [
      "Wav2Vec2ForPreTraining"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 1,
    "chunk_size_feed_forward": 0,
    "classifier_proj_size": 256,
    "codevector_dim": 256,
    "contrastive_logits_temperature": 0.1,
    "conv_bias": false,
    "conv_dim": [
      512,
      512,
      512,
      512,
      512,
      512,
      512
    ],
    "conv_kernel": [
      10,
      3,
      3,
      3,
      3,
      2,
      2
    ],
    "conv_stride": [
      5,
      2,
      2,
      2,
      2,
      2,
      2
    ],
    "cross_attention_hidden_size": null,
    "ctc_loss_reduction": "sum",
    "ctc_zero_infinity": false,
    "decoder_start_token_id": null,
    "diversity_loss_weight": 0.1,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "do_stable_layer_norm": false,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "feat_extract_activation": "gelu",
    "feat_extract_norm": "group",
    "feat_proj_dropout": 0.0,
    "feat_quantizer_dropout": 0.0,
    "final_dropout": 0.0,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "freeze_feat_extract_train": true,
    "hidden_act": "gelu",
    "hidden_dropout": 0.1,
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "layerdrop": 0.0,
    "length_penalty": 1.0,
    "mask_channel_length": 10,
    "mask_channel_min_space": 1,
    "mask_channel_other": 0.0,
    "mask_channel_prob": 0.0,
    "mask_channel_selection": "static",
    "mask_feature_length": 10,
    "mask_feature_min_masks": 0,
    "mask_feature_prob": 0.0,
    "mask_time_length": 10,
    "mask_time_min_masks": 2,
    "mask_time_min_space": 1,
    "mask_time_other": 0.0,
    "mask_time_prob": 0.0,
    "mask_time_selection": "static",
    "max_length": 20,
    "min_length": 0,
    "model_type": "wav2vec2",
    "no_mask_channel_overlap": false,
    "no_mask_time_overlap": false,
    "no_repeat_ngram_size": 0,
    "num_adapter_layers": 3,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_codevector_groups": 2,
    "num_codevectors_per_group": 320,
    "num_conv_pos_embedding_groups": 16,
    "num_conv_pos_embeddings": 128,
    "num_feat_extract_layers": 7,
    "num_hidden_layers": 12,
    "num_negatives": 100,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_size": 768,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "proj_codevector_dim": 256,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": null,
    "tdnn_dilation": [
      1,
      2,
      3,
      1,
      1
    ],
    "tdnn_dim": [
      512,
      512,
      512,
      512,
      1500
    ],
    "tdnn_kernel": [
      5,
      3,
      3,
      1,
      1
    ],
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_weighted_layer_sum": false,
    "vocab_size": 32,
    "xvector_output_dim": 512
  },
  "eos_token_id": 2,
  "is_encoder_decoder": true,
  "max_length": null,
  "model_type": "speech-encoder-decoder",
  "pad_token_id": 1,
  "processor_class": "Wav2Vec2Processor",
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": false
}

[INFO|feature_extraction_utils.py:547] 2024-10-15 02:04:39,396 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|feature_extraction_utils.py:596] 2024-10-15 02:04:39,396 >> Feature extractor Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,414 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,414 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,414 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,415 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,415 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 02:04:39,415 >> loading file tokenizer_config.json
[INFO|processing_utils.py:744] 2024-10-15 02:04:39,689 >> Processor Wav2Vec2Processor:
- feature_extractor: Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

- tokenizer: BartTokenizerFast(name_or_path='./seq2seq_wav2vec2_bart-base/training', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
        0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),
}

{
  "processor_class": "Wav2Vec2Processor"
}

[INFO|trainer.py:667] 2024-10-15 02:04:40,584 >> Using auto half precision backend
[INFO|trainer.py:831] 2024-10-15 02:04:40,711 >> The following columns in the training set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:2243] 2024-10-15 02:04:52,719 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-10-15 02:04:52,719 >>   Num examples = 167,046
[INFO|trainer.py:2245] 2024-10-15 02:04:52,719 >>   Num Epochs = 2
[INFO|trainer.py:2246] 2024-10-15 02:04:52,719 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2249] 2024-10-15 02:04:52,719 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2250] 2024-10-15 02:04:52,719 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2251] 2024-10-15 02:04:52,719 >>   Total optimization steps = 10,442
[INFO|trainer.py:2252] 2024-10-15 02:04:52,720 >>   Number of trainable parameters = 196,895,616
  0%|                                                                                      | 0/10442 [00:00<?, ?it/s][WARNING|logging.py:313] 2024-10-15 02:04:58,199 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:328] 2024-10-15 02:05:08,080 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/auto/brno2/home/xhorni20/dp_mit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
[WARNING|modeling_utils.py:1279] 2024-10-15 02:05:11,135 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
{'loss': 11.1601, 'grad_norm': 8.304616928100586, 'learning_rate': 8e-06, 'epoch': 0.0}
{'loss': 10.7752, 'grad_norm': 4.495971202850342, 'learning_rate': 1.6e-05, 'epoch': 0.0}
{'loss': 10.3164, 'grad_norm': inf, 'learning_rate': 2.2e-05, 'epoch': 0.0}
{'loss': 9.9522, 'grad_norm': 5.06443452835083, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.0}
{'loss': 9.5132, 'grad_norm': 4.766197204589844, 'learning_rate': 3.8e-05, 'epoch': 0.0}
{'loss': 9.0252, 'grad_norm': 3.7966315746307373, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.0}
{'loss': 8.7124, 'grad_norm': 3.6276473999023438, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.01}
{'loss': 8.3262, 'grad_norm': 3.597728967666626, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.01}
{'loss': 8.2934, 'grad_norm': 11.227667808532715, 'learning_rate': 7e-05, 'epoch': 0.01}
{'loss': 7.7656, 'grad_norm': 3.830345869064331, 'learning_rate': 7.8e-05, 'epoch': 0.01}
{'loss': 7.2844, 'grad_norm': 3.811908006668091, 'learning_rate': 8.6e-05, 'epoch': 0.01}
{'loss': 6.8084, 'grad_norm': 4.949711799621582, 'learning_rate': 9.4e-05, 'epoch': 0.01}
{'loss': 6.9646, 'grad_norm': 19.1245174407959, 'learning_rate': 0.000102, 'epoch': 0.01}
{'loss': 7.3567, 'grad_norm': 2.804020643234253, 'learning_rate': 0.00010999999999999998, 'epoch': 0.01}
{'loss': 7.2308, 'grad_norm': 3.3579187393188477, 'learning_rate': 0.00011799999999999998, 'epoch': 0.01}
{'loss': 7.0636, 'grad_norm': 2.4167447090148926, 'learning_rate': 0.00012599999999999997, 'epoch': 0.01}
{'loss': 6.8481, 'grad_norm': 2.39412260055542, 'learning_rate': 0.00013399999999999998, 'epoch': 0.01}
{'loss': 6.7599, 'grad_norm': 2.2137250900268555, 'learning_rate': 0.00014199999999999998, 'epoch': 0.01}
{'loss': 6.7137, 'grad_norm': 2.341707468032837, 'learning_rate': 0.00015, 'epoch': 0.01}
{'loss': 6.6792, 'grad_norm': 4.323137283325195, 'learning_rate': 0.00015799999999999996, 'epoch': 0.02}
{'loss': 6.5087, 'grad_norm': 2.2629082202911377, 'learning_rate': 0.000166, 'epoch': 0.02}
{'loss': 6.5351, 'grad_norm': 2.8060667514801025, 'learning_rate': 0.00017399999999999997, 'epoch': 0.02}
{'loss': 6.4944, 'grad_norm': 3.752655029296875, 'learning_rate': 0.00017999999999999998, 'epoch': 0.02}
{'loss': 6.3622, 'grad_norm': 4.783125877380371, 'learning_rate': 0.000188, 'epoch': 0.02}
{'loss': 5.9636, 'grad_norm': 8.188688278198242, 'learning_rate': 0.00019599999999999997, 'epoch': 0.02}
{'loss': 7.1195, 'grad_norm': 3.240891933441162, 'learning_rate': 0.000204, 'epoch': 0.02}
{'loss': 6.9301, 'grad_norm': 2.230933666229248, 'learning_rate': 0.00021199999999999998, 'epoch': 0.02}
{'loss': 6.8692, 'grad_norm': 1.8021714687347412, 'learning_rate': 0.00021999999999999995, 'epoch': 0.02}
{'loss': 6.864, 'grad_norm': 1.5271077156066895, 'learning_rate': 0.00022799999999999999, 'epoch': 0.02}
{'loss': 6.7039, 'grad_norm': 1.9036097526550293, 'learning_rate': 0.00023599999999999996, 'epoch': 0.02}
{'loss': 6.8547, 'grad_norm': 1.6969438791275024, 'learning_rate': 0.000244, 'epoch': 0.02}
{'loss': 6.827, 'grad_norm': 1.7409549951553345, 'learning_rate': 0.00025199999999999995, 'epoch': 0.02}
{'loss': 6.67, 'grad_norm': 3.5618743896484375, 'learning_rate': 0.00026, 'epoch': 0.03}
{'loss': 6.7562, 'grad_norm': 1.9208701848983765, 'learning_rate': 0.00026799999999999995, 'epoch': 0.03}
{'loss': 6.5932, 'grad_norm': 2.21156644821167, 'learning_rate': 0.000276, 'epoch': 0.03}
{'loss': 6.5306, 'grad_norm': 2.37733793258667, 'learning_rate': 0.00028399999999999996, 'epoch': 0.03}
{'loss': 6.0131, 'grad_norm': 3.753512382507324, 'learning_rate': 0.000292, 'epoch': 0.03}
{'loss': 6.9963, 'grad_norm': 24.93895149230957, 'learning_rate': 0.0003, 'epoch': 0.03}
{'loss': 7.349, 'grad_norm': 2.465757369995117, 'learning_rate': 0.00029988340458608623, 'epoch': 0.03}
{'loss': 6.9066, 'grad_norm': 1.6105623245239258, 'learning_rate': 0.00029976680917217255, 'epoch': 0.03}
{'loss': 6.7943, 'grad_norm': 1.3875298500061035, 'learning_rate': 0.0002996502137582588, 'epoch': 0.03}
{'loss': 6.6357, 'grad_norm': 1.7615985870361328, 'learning_rate': 0.0002995336183443451, 'epoch': 0.03}
{'loss': 6.6957, 'grad_norm': 1.6901236772537231, 'learning_rate': 0.0002994170229304314, 'epoch': 0.03}
{'loss': 6.6484, 'grad_norm': 1.4162858724594116, 'learning_rate': 0.0002993004275165177, 'epoch': 0.03}
{'loss': 6.4769, 'grad_norm': 1.5198041200637817, 'learning_rate': 0.00029918383210260395, 'epoch': 0.03}
{'loss': 6.5031, 'grad_norm': 1.9332016706466675, 'learning_rate': 0.0002990672366886902, 'epoch': 0.04}
{'loss': 6.4107, 'grad_norm': 1.8714452981948853, 'learning_rate': 0.00029895064127477653, 'epoch': 0.04}
{'loss': 6.3281, 'grad_norm': 2.1060259342193604, 'learning_rate': 0.0002988340458608628, 'epoch': 0.04}
{'loss': 6.0889, 'grad_norm': 3.0789732933044434, 'learning_rate': 0.00029871745044694905, 'epoch': 0.04}
{'loss': 5.3649, 'grad_norm': 4.529678821563721, 'learning_rate': 0.00029860085503303536, 'epoch': 0.04}
{'loss': 9.1135, 'grad_norm': 5.899762153625488, 'learning_rate': 0.0002985134084726, 'epoch': 0.04}
{'loss': 7.1051, 'grad_norm': 4.980312347412109, 'learning_rate': 0.00029839681305868633, 'epoch': 0.04}
{'loss': 6.8631, 'grad_norm': 1.9771478176116943, 'learning_rate': 0.0002982802176447726, 'epoch': 0.04}
{'loss': 6.796, 'grad_norm': 1.6708228588104248, 'learning_rate': 0.0002981636222308589, 'epoch': 0.04}
{'loss': 6.6595, 'grad_norm': 1.5517549514770508, 'learning_rate': 0.00029804702681694516, 'epoch': 0.04}
{'loss': 6.5504, 'grad_norm': 1.1532543897628784, 'learning_rate': 0.0002979304314030315, 'epoch': 0.04}
{'loss': 6.625, 'grad_norm': 1.4914063215255737, 'learning_rate': 0.00029781383598911774, 'epoch': 0.04}
{'loss': 6.5665, 'grad_norm': 1.5133397579193115, 'learning_rate': 0.00029769724057520405, 'epoch': 0.04}
{'loss': 6.4176, 'grad_norm': 2.1147079467773438, 'learning_rate': 0.0002975806451612903, 'epoch': 0.05}
{'loss': 6.315, 'grad_norm': 2.0369343757629395, 'learning_rate': 0.00029746404974737657, 'epoch': 0.05}
{'loss': 6.3569, 'grad_norm': 13.102888107299805, 'learning_rate': 0.0002973474543334629, 'epoch': 0.05}
{'loss': 6.3334, 'grad_norm': 4.792037487030029, 'learning_rate': 0.00029723085891954914, 'epoch': 0.05}
{'loss': 6.4483, 'grad_norm': 8.251165390014648, 'learning_rate': 0.0002971142635056354, 'epoch': 0.05}
{'loss': 6.9746, 'grad_norm': 1.8854749202728271, 'learning_rate': 0.0002969976680917217, 'epoch': 0.05}
{'loss': 6.7554, 'grad_norm': 1.9590842723846436, 'learning_rate': 0.000296881072677808, 'epoch': 0.05}
{'loss': 6.7914, 'grad_norm': 1.1897215843200684, 'learning_rate': 0.00029676447726389424, 'epoch': 0.05}
{'loss': 6.6813, 'grad_norm': 1.192203164100647, 'learning_rate': 0.00029664788184998055, 'epoch': 0.05}
{'loss': 6.592, 'grad_norm': 1.6340330839157104, 'learning_rate': 0.0002965312864360668, 'epoch': 0.05}
{'loss': 6.5775, 'grad_norm': 1.602149486541748, 'learning_rate': 0.00029641469102215307, 'epoch': 0.05}
{'loss': 6.4724, 'grad_norm': 3.7117812633514404, 'learning_rate': 0.0002962980956082394, 'epoch': 0.05}
{'loss': 6.3687, 'grad_norm': 1.8323229551315308, 'learning_rate': 0.00029618150019432564, 'epoch': 0.05}
{'loss': 6.2609, 'grad_norm': 1.8381730318069458, 'learning_rate': 0.00029609405363389035, 'epoch': 0.06}
{'loss': 6.103, 'grad_norm': 3.2588236331939697, 'learning_rate': 0.00029597745821997667, 'epoch': 0.06}
{'loss': 6.0135, 'grad_norm': 2.9210691452026367, 'learning_rate': 0.0002958608628060629, 'epoch': 0.06}
{'loss': 5.3412, 'grad_norm': 3.3195135593414307, 'learning_rate': 0.00029574426739214924, 'epoch': 0.06}
{'loss': 8.964, 'grad_norm': 13.648259162902832, 'learning_rate': 0.0002956276719782355, 'epoch': 0.06}
{'loss': 6.9291, 'grad_norm': 1.3321856260299683, 'learning_rate': 0.0002955110765643218, 'epoch': 0.06}
{'loss': 6.7715, 'grad_norm': 1.2835049629211426, 'learning_rate': 0.0002953944811504081, 'epoch': 0.06}
{'loss': 6.7124, 'grad_norm': 1.4322363138198853, 'learning_rate': 0.00029527788573649433, 'epoch': 0.06}
{'loss': 6.5889, 'grad_norm': 1.1978038549423218, 'learning_rate': 0.00029516129032258065, 'epoch': 0.06}
{'loss': 6.4667, 'grad_norm': 1.1618397235870361, 'learning_rate': 0.0002950446949086669, 'epoch': 0.06}
{'loss': 6.3723, 'grad_norm': 1.3733490705490112, 'learning_rate': 0.00029492809949475317, 'epoch': 0.06}
{'loss': 6.3573, 'grad_norm': 1.4694024324417114, 'learning_rate': 0.0002948115040808395, 'epoch': 0.06}
{'loss': 6.1854, 'grad_norm': 1.787363052368164, 'learning_rate': 0.00029469490866692574, 'epoch': 0.06}
{'loss': 6.0552, 'grad_norm': 1.7893723249435425, 'learning_rate': 0.000294578313253012, 'epoch': 0.07}
{'loss': 5.8349, 'grad_norm': 1.8148869276046753, 'learning_rate': 0.00029446171783909826, 'epoch': 0.07}
{'loss': 5.4889, 'grad_norm': 3.1426641941070557, 'learning_rate': 0.0002943451224251846, 'epoch': 0.07}
{'loss': 7.6969, 'grad_norm': 29.24631690979004, 'learning_rate': 0.00029422852701127083, 'epoch': 0.07}
{'loss': 8.5869, 'grad_norm': 12.148164749145508, 'learning_rate': 0.00029411193159735715, 'epoch': 0.07}
{'loss': 6.9105, 'grad_norm': 1.6044280529022217, 'learning_rate': 0.0002939953361834434, 'epoch': 0.07}
{'loss': 6.7068, 'grad_norm': 1.8322848081588745, 'learning_rate': 0.0002938787407695297, 'epoch': 0.07}
{'loss': 6.6471, 'grad_norm': 1.3611012697219849, 'learning_rate': 0.000293762145355616, 'epoch': 0.07}
{'loss': 6.5081, 'grad_norm': 1.157507061958313, 'learning_rate': 0.00029364554994170224, 'epoch': 0.07}
{'loss': 6.4307, 'grad_norm': 1.1144137382507324, 'learning_rate': 0.00029352895452778856, 'epoch': 0.07}
{'loss': 6.4203, 'grad_norm': 1.1515183448791504, 'learning_rate': 0.0002934123591138748, 'epoch': 0.07}
{'loss': 6.2343, 'grad_norm': 1.3262650966644287, 'learning_rate': 0.00029329576369996113, 'epoch': 0.07}
{'loss': 6.1456, 'grad_norm': 1.3851407766342163, 'learning_rate': 0.0002931791682860474, 'epoch': 0.07}
{'loss': 6.0301, 'grad_norm': 1.4940346479415894, 'learning_rate': 0.0002930625728721337, 'epoch': 0.08}
{'loss': 5.933, 'grad_norm': 3.189323902130127, 'learning_rate': 0.00029294597745821996, 'epoch': 0.08}
{'loss': 5.3789, 'grad_norm': 2.6592142581939697, 'learning_rate': 0.0002928293820443062, 'epoch': 0.08}
  4%|â–ˆâ–ˆâ–Š                                                                       | 400/10442 [24:04<6:29:29,  2.33s/it][INFO|trainer.py:831] 2024-10-15 02:28:57,041 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 02:28:57,051 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 02:28:57,051 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 02:28:57,051 >>   Batch size = 32
{'eval_loss': 7.1999664306640625, 'eval_wer': 0.9554751949372811, 'eval_runtime': 246.4774, 'eval_samples_per_second': 6.455, 'eval_steps_per_second': 0.203, 'epoch': 0.08}
{'loss': 7.5763, 'grad_norm': 7.586808204650879, 'learning_rate': 0.00029271278663039254, 'epoch': 0.08}
{'loss': 6.835, 'grad_norm': 2.456888198852539, 'learning_rate': 0.0002925961912164788, 'epoch': 0.08}
{'loss': 6.7386, 'grad_norm': 1.4174203872680664, 'learning_rate': 0.00029247959580256506, 'epoch': 0.08}
{'loss': 6.5912, 'grad_norm': 1.2990691661834717, 'learning_rate': 0.00029236300038865137, 'epoch': 0.08}
{'loss': 6.6087, 'grad_norm': 0.938909649848938, 'learning_rate': 0.00029224640497473763, 'epoch': 0.08}
{'loss': 6.4435, 'grad_norm': 1.2348058223724365, 'learning_rate': 0.0002921298095608239, 'epoch': 0.08}
{'loss': 6.3492, 'grad_norm': 1.1359241008758545, 'learning_rate': 0.0002920132141469102, 'epoch': 0.08}
{'loss': 6.2802, 'grad_norm': 1.4035857915878296, 'learning_rate': 0.00029189661873299646, 'epoch': 0.08}
{'loss': 6.1563, 'grad_norm': 1.3517645597457886, 'learning_rate': 0.0002917800233190827, 'epoch': 0.08}
{'loss': 6.0573, 'grad_norm': 1.6163667440414429, 'learning_rate': 0.00029166342790516904, 'epoch': 0.08}
{'loss': 5.9141, 'grad_norm': 1.5317142009735107, 'learning_rate': 0.0002915468324912553, 'epoch': 0.09}
{'loss': 5.6274, 'grad_norm': 6.946915149688721, 'learning_rate': 0.0002914302370773416, 'epoch': 0.09}
{'loss': 6.1372, 'grad_norm': 10.427433013916016, 'learning_rate': 0.00029131364166342787, 'epoch': 0.09}
{'loss': 6.9844, 'grad_norm': 1.5629479885101318, 'learning_rate': 0.0002911970462495142, 'epoch': 0.09}
{'loss': 6.5738, 'grad_norm': 0.9879793524742126, 'learning_rate': 0.00029108045083560044, 'epoch': 0.09}
{'loss': 6.5228, 'grad_norm': 1.0711290836334229, 'learning_rate': 0.0002909638554216867, 'epoch': 0.09}
{'loss': 6.5095, 'grad_norm': 2.2817513942718506, 'learning_rate': 0.000290847260007773, 'epoch': 0.09}
{'loss': 6.3321, 'grad_norm': 1.7191269397735596, 'learning_rate': 0.0002907306645938593, 'epoch': 0.09}
{'loss': 6.323, 'grad_norm': 1.480741024017334, 'learning_rate': 0.0002906140691799456, 'epoch': 0.09}
{'loss': 6.2614, 'grad_norm': 1.1859917640686035, 'learning_rate': 0.00029049747376603185, 'epoch': 0.09}
{'loss': 6.2718, 'grad_norm': 1.400084376335144, 'learning_rate': 0.00029038087835211817, 'epoch': 0.09}
{'loss': 6.0905, 'grad_norm': 2.609553098678589, 'learning_rate': 0.0002902642829382044, 'epoch': 0.09}
{'loss': 5.8483, 'grad_norm': 2.0026652812957764, 'learning_rate': 0.0002901476875242907, 'epoch': 0.09}
{'loss': 5.6906, 'grad_norm': 1.685900330543518, 'learning_rate': 0.000290031092110377, 'epoch': 0.1}
{'loss': 5.1439, 'grad_norm': 2.902935743331909, 'learning_rate': 0.00028991449669646326, 'epoch': 0.1}
{'loss': 7.0849, 'grad_norm': 4.018589496612549, 'learning_rate': 0.0002897979012825495, 'epoch': 0.1}
{'loss': 6.5236, 'grad_norm': 1.196341872215271, 'learning_rate': 0.0002896813058686358, 'epoch': 0.1}
{'loss': 6.4943, 'grad_norm': 1.245099663734436, 'learning_rate': 0.0002895647104547221, 'epoch': 0.1}
{'loss': 6.4581, 'grad_norm': 1.1539177894592285, 'learning_rate': 0.00028944811504080835, 'epoch': 0.1}
{'loss': 6.2676, 'grad_norm': 1.1866790056228638, 'learning_rate': 0.0002893315196268946, 'epoch': 0.1}
{'loss': 6.1664, 'grad_norm': 1.1187245845794678, 'learning_rate': 0.0002892149242129809, 'epoch': 0.1}
{'loss': 6.1492, 'grad_norm': 1.2066049575805664, 'learning_rate': 0.0002890983287990672, 'epoch': 0.1}
{'loss': 6.1456, 'grad_norm': 1.288220763206482, 'learning_rate': 0.0002889817333851535, 'epoch': 0.1}
{'loss': 6.0392, 'grad_norm': 1.4441434144973755, 'learning_rate': 0.00028886513797123976, 'epoch': 0.1}
{'loss': 5.9581, 'grad_norm': 1.7071257829666138, 'learning_rate': 0.0002887485425573261, 'epoch': 0.1}
{'loss': 5.7863, 'grad_norm': 1.821227788925171, 'learning_rate': 0.00028863194714341233, 'epoch': 0.1}
{'loss': 5.3335, 'grad_norm': 2.6041197776794434, 'learning_rate': 0.0002885153517294986, 'epoch': 0.1}
{'loss': 6.1351, 'grad_norm': 13.029084205627441, 'learning_rate': 0.0002883987563155849, 'epoch': 0.11}
{'loss': 6.8906, 'grad_norm': 2.135995388031006, 'learning_rate': 0.00028828216090167117, 'epoch': 0.11}
{'loss': 6.5749, 'grad_norm': 1.7623012065887451, 'learning_rate': 0.0002881655654877575, 'epoch': 0.11}
{'loss': 6.3536, 'grad_norm': 1.1581388711929321, 'learning_rate': 0.00028804897007384374, 'epoch': 0.11}
{'loss': 6.3231, 'grad_norm': 1.3352023363113403, 'learning_rate': 0.00028793237465993005, 'epoch': 0.11}
{'loss': 6.1889, 'grad_norm': 1.2757922410964966, 'learning_rate': 0.0002878157792460163, 'epoch': 0.11}
{'loss': 6.1183, 'grad_norm': 1.2854989767074585, 'learning_rate': 0.0002876991838321026, 'epoch': 0.11}
{'loss': 6.1507, 'grad_norm': 1.2558302879333496, 'learning_rate': 0.0002875825884181889, 'epoch': 0.11}
{'loss': 6.0128, 'grad_norm': 1.6786171197891235, 'learning_rate': 0.00028746599300427515, 'epoch': 0.11}
{'loss': 5.8216, 'grad_norm': 1.5706251859664917, 'learning_rate': 0.0002873493975903614, 'epoch': 0.11}
{'loss': 5.7106, 'grad_norm': 1.5612428188323975, 'learning_rate': 0.0002872328021764477, 'epoch': 0.11}
{'loss': 5.5612, 'grad_norm': 1.8001375198364258, 'learning_rate': 0.000287116206762534, 'epoch': 0.11}
{'loss': 5.0592, 'grad_norm': 3.9459707736968994, 'learning_rate': 0.00028699961134862024, 'epoch': 0.11}
{'loss': 6.9474, 'grad_norm': 4.887152194976807, 'learning_rate': 0.00028688301593470655, 'epoch': 0.12}
{'loss': 6.4411, 'grad_norm': 1.4818551540374756, 'learning_rate': 0.0002867664205207928, 'epoch': 0.12}
{'loss': 6.1896, 'grad_norm': 1.3343523740768433, 'learning_rate': 0.0002866498251068791, 'epoch': 0.12}
{'loss': 6.2497, 'grad_norm': 1.3813813924789429, 'learning_rate': 0.0002865332296929654, 'epoch': 0.12}
{'loss': 6.1083, 'grad_norm': 1.174686312675476, 'learning_rate': 0.00028641663427905165, 'epoch': 0.12}
{'loss': 5.9447, 'grad_norm': 1.2624053955078125, 'learning_rate': 0.00028630003886513796, 'epoch': 0.12}
{'loss': 5.8893, 'grad_norm': 1.4473145008087158, 'learning_rate': 0.0002861834434512242, 'epoch': 0.12}
{'loss': 5.8361, 'grad_norm': 1.5273711681365967, 'learning_rate': 0.0002860668480373105, 'epoch': 0.12}
{'loss': 5.8115, 'grad_norm': 1.5030101537704468, 'learning_rate': 0.0002859502526233968, 'epoch': 0.12}
{'loss': 5.5997, 'grad_norm': 1.6870687007904053, 'learning_rate': 0.00028583365720948306, 'epoch': 0.12}
{'loss': 5.3839, 'grad_norm': 1.773374080657959, 'learning_rate': 0.00028571706179556937, 'epoch': 0.12}
{'loss': 5.0527, 'grad_norm': 2.290297508239746, 'learning_rate': 0.00028560046638165563, 'epoch': 0.12}
{'loss': 5.8296, 'grad_norm': 10.330039024353027, 'learning_rate': 0.00028548387096774194, 'epoch': 0.12}
{'loss': 6.4281, 'grad_norm': 2.368107557296753, 'learning_rate': 0.0002853672755538282, 'epoch': 0.13}
{'loss': 6.1669, 'grad_norm': 2.0483388900756836, 'learning_rate': 0.00028525068013991446, 'epoch': 0.13}
{'loss': 6.027, 'grad_norm': 1.5317227840423584, 'learning_rate': 0.0002851340847260008, 'epoch': 0.13}
{'loss': 5.9607, 'grad_norm': 1.6586618423461914, 'learning_rate': 0.00028501748931208704, 'epoch': 0.13}
{'loss': 5.9156, 'grad_norm': 1.395774483680725, 'learning_rate': 0.0002849008938981733, 'epoch': 0.13}
{'loss': 5.8958, 'grad_norm': 1.2559551000595093, 'learning_rate': 0.0002847842984842596, 'epoch': 0.13}
{'loss': 5.7571, 'grad_norm': 1.6615188121795654, 'learning_rate': 0.00028466770307034587, 'epoch': 0.13}
{'loss': 5.7325, 'grad_norm': 1.4001047611236572, 'learning_rate': 0.00028455110765643213, 'epoch': 0.13}
{'loss': 5.7051, 'grad_norm': 1.5073376893997192, 'learning_rate': 0.00028443451224251844, 'epoch': 0.13}
{'loss': 5.4104, 'grad_norm': 1.7566713094711304, 'learning_rate': 0.0002843179168286047, 'epoch': 0.13}
{'loss': 5.3163, 'grad_norm': 2.6979572772979736, 'learning_rate': 0.00028420132141469096, 'epoch': 0.13}
{'loss': 4.8403, 'grad_norm': 3.0469284057617188, 'learning_rate': 0.0002840847260007773, 'epoch': 0.13}
{'loss': 6.7653, 'grad_norm': 7.808982849121094, 'learning_rate': 0.00028396813058686354, 'epoch': 0.13}
{'loss': 6.3321, 'grad_norm': 1.9745194911956787, 'learning_rate': 0.00028385153517294985, 'epoch': 0.14}
{'loss': 6.0497, 'grad_norm': 1.46137535572052, 'learning_rate': 0.0002837349397590361, 'epoch': 0.14}
{'loss': 5.8712, 'grad_norm': 1.4579188823699951, 'learning_rate': 0.0002836183443451224, 'epoch': 0.14}
{'loss': 5.8699, 'grad_norm': 1.4695544242858887, 'learning_rate': 0.0002835017489312087, 'epoch': 0.14}
{'loss': 5.8604, 'grad_norm': 1.3645788431167603, 'learning_rate': 0.00028338515351729494, 'epoch': 0.14}
{'loss': 5.713, 'grad_norm': 1.3785120248794556, 'learning_rate': 0.00028326855810338126, 'epoch': 0.14}
{'loss': 5.6728, 'grad_norm': 1.3789528608322144, 'learning_rate': 0.0002831519626894675, 'epoch': 0.14}
{'loss': 5.5925, 'grad_norm': 2.378326892852783, 'learning_rate': 0.00028303536727555383, 'epoch': 0.14}
{'loss': 5.5393, 'grad_norm': 1.7788817882537842, 'learning_rate': 0.0002829187718616401, 'epoch': 0.14}
{'loss': 5.2824, 'grad_norm': 1.9192168712615967, 'learning_rate': 0.0002828021764477264, 'epoch': 0.14}
{'loss': 5.101, 'grad_norm': 2.503117322921753, 'learning_rate': 0.00028268558103381266, 'epoch': 0.14}
{'loss': 5.7596, 'grad_norm': 20.1933650970459, 'learning_rate': 0.0002825689856198989, 'epoch': 0.14}
{'loss': 6.3563, 'grad_norm': 3.1228768825531006, 'learning_rate': 0.00028245239020598524, 'epoch': 0.14}
{'loss': 5.9278, 'grad_norm': 1.3735475540161133, 'learning_rate': 0.0002823357947920715, 'epoch': 0.15}
{'loss': 5.855, 'grad_norm': 1.265862226486206, 'learning_rate': 0.00028221919937815776, 'epoch': 0.15}
{'loss': 5.7493, 'grad_norm': 1.3209246397018433, 'learning_rate': 0.000282102603964244, 'epoch': 0.15}
{'loss': 5.7211, 'grad_norm': 1.6709439754486084, 'learning_rate': 0.00028198600855033033, 'epoch': 0.15}
{'loss': 5.7436, 'grad_norm': 1.4406743049621582, 'learning_rate': 0.0002818694131364166, 'epoch': 0.15}
{'loss': 5.7831, 'grad_norm': 1.6934490203857422, 'learning_rate': 0.00028175281772250285, 'epoch': 0.15}
{'loss': 5.6209, 'grad_norm': 1.5562711954116821, 'learning_rate': 0.00028163622230858917, 'epoch': 0.15}
{'loss': 5.4882, 'grad_norm': 2.0858521461486816, 'learning_rate': 0.0002815196268946754, 'epoch': 0.15}
{'loss': 5.3501, 'grad_norm': 1.7117747068405151, 'learning_rate': 0.00028140303148076174, 'epoch': 0.15}
{'loss': 5.1723, 'grad_norm': 2.1648340225219727, 'learning_rate': 0.000281286436066848, 'epoch': 0.15}
{'loss': 4.7345, 'grad_norm': 3.261624813079834, 'learning_rate': 0.0002811698406529343, 'epoch': 0.15}
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                    | 800/10442 [51:31<6:10:01,  2.30s/it][INFO|trainer.py:831] 2024-10-15 02:56:24,219 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 02:56:24,231 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 02:56:24,231 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 02:56:24,231 >>   Batch size = 32
{'eval_loss': 5.804616928100586, 'eval_wer': 0.9570290428296983, 'eval_runtime': 252.9218, 'eval_samples_per_second': 6.29, 'eval_steps_per_second': 0.198, 'epoch': 0.15}
{'loss': 6.4192, 'grad_norm': 5.234653949737549, 'learning_rate': 0.00028105324523902057, 'epoch': 0.15}
{'loss': 6.0194, 'grad_norm': 1.5674024820327759, 'learning_rate': 0.00028093664982510683, 'epoch': 0.15}
{'loss': 5.8771, 'grad_norm': 1.8274157047271729, 'learning_rate': 0.00028082005441119315, 'epoch': 0.16}
{'loss': 5.7763, 'grad_norm': 1.3641809225082397, 'learning_rate': 0.0002807034589972794, 'epoch': 0.16}
{'loss': 5.776, 'grad_norm': 1.4281184673309326, 'learning_rate': 0.0002805868635833657, 'epoch': 0.16}
{'loss': 5.6401, 'grad_norm': 1.451259970664978, 'learning_rate': 0.000280470268169452, 'epoch': 0.16}
{'loss': 5.5221, 'grad_norm': 1.3406111001968384, 'learning_rate': 0.0002803536727555383, 'epoch': 0.16}
{'loss': 5.5813, 'grad_norm': 1.4365301132202148, 'learning_rate': 0.00028023707734162455, 'epoch': 0.16}
{'loss': 5.4712, 'grad_norm': 1.4747563600540161, 'learning_rate': 0.0002801204819277108, 'epoch': 0.16}
{'loss': 5.2341, 'grad_norm': 1.709490418434143, 'learning_rate': 0.00028000388651379713, 'epoch': 0.16}
{'loss': 5.2837, 'grad_norm': 2.0217061042785645, 'learning_rate': 0.0002798872910998834, 'epoch': 0.16}
{'loss': 4.928, 'grad_norm': 2.1310534477233887, 'learning_rate': 0.00027977069568596965, 'epoch': 0.16}
{'loss': 5.432, 'grad_norm': 6.86228609085083, 'learning_rate': 0.00027965410027205596, 'epoch': 0.16}
{'loss': 6.0575, 'grad_norm': 2.213747501373291, 'learning_rate': 0.0002795375048581422, 'epoch': 0.16}
{'loss': 5.8497, 'grad_norm': 1.5241873264312744, 'learning_rate': 0.0002794209094442285, 'epoch': 0.16}
{'loss': 5.8128, 'grad_norm': 1.1640446186065674, 'learning_rate': 0.0002793043140303148, 'epoch': 0.17}
{'loss': 5.6979, 'grad_norm': 1.2395800352096558, 'learning_rate': 0.00027918771861640105, 'epoch': 0.17}
{'loss': 5.6424, 'grad_norm': 1.3659673929214478, 'learning_rate': 0.0002790711232024873, 'epoch': 0.17}
{'loss': 5.562, 'grad_norm': 1.2684636116027832, 'learning_rate': 0.00027895452778857363, 'epoch': 0.17}
{'loss': 5.512, 'grad_norm': 1.5340358018875122, 'learning_rate': 0.0002788379323746599, 'epoch': 0.17}
{'loss': 5.4321, 'grad_norm': 1.4408067464828491, 'learning_rate': 0.0002787213369607462, 'epoch': 0.17}
{'loss': 5.434, 'grad_norm': 1.6588040590286255, 'learning_rate': 0.00027860474154683246, 'epoch': 0.17}
{'loss': 5.3135, 'grad_norm': 1.809424638748169, 'learning_rate': 0.0002784881461329187, 'epoch': 0.17}
{'loss': 5.2017, 'grad_norm': 2.1988675594329834, 'learning_rate': 0.00027837155071900503, 'epoch': 0.17}
{'loss': 4.6915, 'grad_norm': 5.699565887451172, 'learning_rate': 0.0002782549553050913, 'epoch': 0.17}
{'loss': 6.2681, 'grad_norm': 4.810584545135498, 'learning_rate': 0.0002781383598911776, 'epoch': 0.17}
{'loss': 5.9235, 'grad_norm': 1.5990878343582153, 'learning_rate': 0.00027802176447726387, 'epoch': 0.17}
{'loss': 5.7076, 'grad_norm': 1.5220935344696045, 'learning_rate': 0.0002779051690633502, 'epoch': 0.17}
{'loss': 5.7158, 'grad_norm': 1.4060008525848389, 'learning_rate': 0.00027778857364943644, 'epoch': 0.18}
{'loss': 5.6594, 'grad_norm': 1.4093598127365112, 'learning_rate': 0.0002776719782355227, 'epoch': 0.18}
{'loss': 5.4308, 'grad_norm': 1.3295193910598755, 'learning_rate': 0.000277555382821609, 'epoch': 0.18}
{'loss': 5.5478, 'grad_norm': 1.4228663444519043, 'learning_rate': 0.0002774387874076953, 'epoch': 0.18}
{'loss': 5.541, 'grad_norm': 2.30922269821167, 'learning_rate': 0.00027732219199378154, 'epoch': 0.18}
{'loss': 5.4603, 'grad_norm': 1.6647660732269287, 'learning_rate': 0.00027720559657986785, 'epoch': 0.18}
{'loss': 5.1768, 'grad_norm': 1.7968757152557373, 'learning_rate': 0.0002770890011659541, 'epoch': 0.18}
{'loss': 5.0527, 'grad_norm': 2.160843849182129, 'learning_rate': 0.00027697240575204037, 'epoch': 0.18}
{'loss': 4.8485, 'grad_norm': 2.3450989723205566, 'learning_rate': 0.0002768558103381267, 'epoch': 0.18}
{'loss': 5.289, 'grad_norm': 6.427948951721191, 'learning_rate': 0.00027673921492421294, 'epoch': 0.18}
{'loss': 5.9462, 'grad_norm': 2.2005982398986816, 'learning_rate': 0.0002766226195102992, 'epoch': 0.18}
{'loss': 5.7262, 'grad_norm': 1.4582395553588867, 'learning_rate': 0.0002765060240963855, 'epoch': 0.18}
{'loss': 5.5865, 'grad_norm': 1.2756036520004272, 'learning_rate': 0.0002763894286824718, 'epoch': 0.18}
{'loss': 5.6393, 'grad_norm': 1.4166827201843262, 'learning_rate': 0.0002762728332685581, 'epoch': 0.19}
{'loss': 5.5041, 'grad_norm': 1.4389835596084595, 'learning_rate': 0.00027615623785464435, 'epoch': 0.19}
{'loss': 5.4508, 'grad_norm': 1.3648557662963867, 'learning_rate': 0.0002760396424407306, 'epoch': 0.19}
{'loss': 5.503, 'grad_norm': 1.450016975402832, 'learning_rate': 0.0002759230470268169, 'epoch': 0.19}
{'loss': 5.3551, 'grad_norm': 1.6768230199813843, 'learning_rate': 0.0002758064516129032, 'epoch': 0.19}
{'loss': 5.3153, 'grad_norm': 1.7657990455627441, 'learning_rate': 0.0002756898561989895, 'epoch': 0.19}
{'loss': 5.1113, 'grad_norm': 1.8048698902130127, 'learning_rate': 0.00027557326078507576, 'epoch': 0.19}
{'loss': 4.9452, 'grad_norm': 2.212282180786133, 'learning_rate': 0.00027545666537116207, 'epoch': 0.19}
{'loss': 4.6269, 'grad_norm': 2.8851242065429688, 'learning_rate': 0.00027534006995724833, 'epoch': 0.19}
{'loss': 6.0431, 'grad_norm': 4.320241451263428, 'learning_rate': 0.00027522347454333464, 'epoch': 0.19}
{'loss': 5.8052, 'grad_norm': 1.8126238584518433, 'learning_rate': 0.0002751068791294209, 'epoch': 0.19}
{'loss': 5.6365, 'grad_norm': 1.493329644203186, 'learning_rate': 0.00027499028371550716, 'epoch': 0.19}
{'loss': 5.535, 'grad_norm': 2.4310290813446045, 'learning_rate': 0.0002748736883015935, 'epoch': 0.19}
{'loss': 5.6204, 'grad_norm': 1.3844408988952637, 'learning_rate': 0.00027475709288767974, 'epoch': 0.2}
{'loss': 5.562, 'grad_norm': 1.4553430080413818, 'learning_rate': 0.000274640497473766, 'epoch': 0.2}
{'loss': 5.3442, 'grad_norm': 1.455756425857544, 'learning_rate': 0.0002745239020598523, 'epoch': 0.2}
{'loss': 5.4267, 'grad_norm': 1.4636812210083008, 'learning_rate': 0.00027440730664593857, 'epoch': 0.2}
{'loss': 5.2791, 'grad_norm': 1.5335172414779663, 'learning_rate': 0.00027429071123202483, 'epoch': 0.2}
{'loss': 5.1967, 'grad_norm': 1.739384651184082, 'learning_rate': 0.0002741741158181111, 'epoch': 0.2}
{'loss': 5.1288, 'grad_norm': 1.9058281183242798, 'learning_rate': 0.0002740575204041974, 'epoch': 0.2}
{'loss': 4.969, 'grad_norm': 2.1441991329193115, 'learning_rate': 0.00027394092499028366, 'epoch': 0.2}
{'loss': 5.4017, 'grad_norm': 4.413954734802246, 'learning_rate': 0.00027382432957637, 'epoch': 0.2}
{'loss': 5.8196, 'grad_norm': 1.7771110534667969, 'learning_rate': 0.00027370773416245624, 'epoch': 0.2}
{'loss': 5.6367, 'grad_norm': 1.2613385915756226, 'learning_rate': 0.00027359113874854255, 'epoch': 0.2}
{'loss': 5.5778, 'grad_norm': 1.2380179166793823, 'learning_rate': 0.0002734745433346288, 'epoch': 0.2}
{'loss': 5.4875, 'grad_norm': 1.4088821411132812, 'learning_rate': 0.00027335794792071507, 'epoch': 0.2}
{'loss': 5.4894, 'grad_norm': 1.192718267440796, 'learning_rate': 0.0002732413525068014, 'epoch': 0.21}
{'loss': 5.4926, 'grad_norm': 1.5986641645431519, 'learning_rate': 0.00027312475709288765, 'epoch': 0.21}
{'loss': 5.2973, 'grad_norm': 1.5597714185714722, 'learning_rate': 0.00027300816167897396, 'epoch': 0.21}
{'loss': 5.3252, 'grad_norm': 1.5431872606277466, 'learning_rate': 0.0002728915662650602, 'epoch': 0.21}
{'loss': 5.1723, 'grad_norm': 1.6584434509277344, 'learning_rate': 0.00027277497085114653, 'epoch': 0.21}
{'loss': 5.1404, 'grad_norm': 1.9418342113494873, 'learning_rate': 0.0002726583754372328, 'epoch': 0.21}
{'loss': 4.8755, 'grad_norm': 2.263601303100586, 'learning_rate': 0.00027254178002331905, 'epoch': 0.21}
{'loss': 4.6545, 'grad_norm': 3.240853786468506, 'learning_rate': 0.00027242518460940537, 'epoch': 0.21}
{'loss': 6.0883, 'grad_norm': 5.774112701416016, 'learning_rate': 0.0002723085891954916, 'epoch': 0.21}
{'loss': 5.8152, 'grad_norm': 2.9307503700256348, 'learning_rate': 0.0002721919937815779, 'epoch': 0.21}
{'loss': 5.6686, 'grad_norm': 1.4372769594192505, 'learning_rate': 0.0002720753983676642, 'epoch': 0.21}
{'loss': 5.4433, 'grad_norm': 21.982303619384766, 'learning_rate': 0.00027195880295375046, 'epoch': 0.21}
{'loss': 5.4336, 'grad_norm': 1.3488342761993408, 'learning_rate': 0.0002718422075398367, 'epoch': 0.21}
{'loss': 5.421, 'grad_norm': 1.7148211002349854, 'learning_rate': 0.00027172561212592303, 'epoch': 0.22}
{'loss': 5.4213, 'grad_norm': 1.8798048496246338, 'learning_rate': 0.0002716090167120093, 'epoch': 0.22}
{'loss': 5.2127, 'grad_norm': 1.5377250909805298, 'learning_rate': 0.00027149242129809555, 'epoch': 0.22}
{'loss': 5.095, 'grad_norm': 1.742052674293518, 'learning_rate': 0.00027137582588418187, 'epoch': 0.22}
{'loss': 5.209, 'grad_norm': 1.6699856519699097, 'learning_rate': 0.0002712592304702681, 'epoch': 0.22}
{'loss': 5.0746, 'grad_norm': 1.881036639213562, 'learning_rate': 0.00027114263505635444, 'epoch': 0.22}
{'loss': 4.7064, 'grad_norm': 2.512126922607422, 'learning_rate': 0.0002710260396424407, 'epoch': 0.22}
{'loss': 5.2637, 'grad_norm': 5.953099250793457, 'learning_rate': 0.00027090944422852696, 'epoch': 0.22}
{'loss': 5.8144, 'grad_norm': 2.4824516773223877, 'learning_rate': 0.0002707928488146133, 'epoch': 0.22}
{'loss': 5.5996, 'grad_norm': 1.5889270305633545, 'learning_rate': 0.00027067625340069953, 'epoch': 0.22}
{'loss': 5.5356, 'grad_norm': 1.3363316059112549, 'learning_rate': 0.00027055965798678585, 'epoch': 0.22}
{'loss': 5.4944, 'grad_norm': 1.3701130151748657, 'learning_rate': 0.0002704430625728721, 'epoch': 0.22}
{'loss': 5.3438, 'grad_norm': 1.367102861404419, 'learning_rate': 0.0002703264671589584, 'epoch': 0.22}
{'loss': 5.2996, 'grad_norm': 1.3517810106277466, 'learning_rate': 0.0002702098717450447, 'epoch': 0.23}
{'loss': 5.235, 'grad_norm': 1.5445404052734375, 'learning_rate': 0.00027009327633113094, 'epoch': 0.23}
{'loss': 5.2141, 'grad_norm': 1.5997340679168701, 'learning_rate': 0.00026997668091721726, 'epoch': 0.23}
{'loss': 5.0608, 'grad_norm': 1.5330337285995483, 'learning_rate': 0.0002698600855033035, 'epoch': 0.23}
{'loss': 5.1069, 'grad_norm': 1.8203823566436768, 'learning_rate': 0.00026974349008938983, 'epoch': 0.23}
{'loss': 4.9361, 'grad_norm': 2.0798585414886475, 'learning_rate': 0.0002696268946754761, 'epoch': 0.23}
{'loss': 4.4152, 'grad_norm': 4.1229472160339355, 'learning_rate': 0.00026951029926156235, 'epoch': 0.23}
 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 1200/10442 [1:19:10<5:51:11,  2.28s/it][INFO|trainer.py:831] 2024-10-15 03:24:03,576 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 03:24:03,587 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 03:24:03,587 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 03:24:03,587 >>   Batch size = 32
{'eval_loss': 5.419749736785889, 'eval_wer': 0.9760707424567748, 'eval_runtime': 257.3082, 'eval_samples_per_second': 6.183, 'eval_steps_per_second': 0.194, 'epoch': 0.23}
{'loss': 5.8367, 'grad_norm': 3.830702066421509, 'learning_rate': 0.0002693937038476486, 'epoch': 0.23}
{'loss': 5.5341, 'grad_norm': 1.5398108959197998, 'learning_rate': 0.0002692771084337349, 'epoch': 0.23}
{'loss': 5.5135, 'grad_norm': 1.3623601198196411, 'learning_rate': 0.0002691605130198212, 'epoch': 0.23}
{'loss': 5.4283, 'grad_norm': 1.3541374206542969, 'learning_rate': 0.00026904391760590744, 'epoch': 0.23}
{'loss': 5.3981, 'grad_norm': 1.3367362022399902, 'learning_rate': 0.00026892732219199376, 'epoch': 0.23}
{'loss': 5.3626, 'grad_norm': 1.352735161781311, 'learning_rate': 0.00026881072677808, 'epoch': 0.23}
{'loss': 5.2724, 'grad_norm': 1.3870501518249512, 'learning_rate': 0.00026869413136416633, 'epoch': 0.24}
{'loss': 5.2008, 'grad_norm': 1.5171642303466797, 'learning_rate': 0.0002685775359502526, 'epoch': 0.24}
{'loss': 4.9995, 'grad_norm': 1.6703064441680908, 'learning_rate': 0.00026846094053633885, 'epoch': 0.24}
{'loss': 5.1108, 'grad_norm': 2.038086414337158, 'learning_rate': 0.00026834434512242516, 'epoch': 0.24}
{'loss': 4.9483, 'grad_norm': 1.8260092735290527, 'learning_rate': 0.0002682277497085114, 'epoch': 0.24}
{'loss': 4.7182, 'grad_norm': 2.245277166366577, 'learning_rate': 0.00026811115429459774, 'epoch': 0.24}
{'loss': 5.1888, 'grad_norm': 5.318166732788086, 'learning_rate': 0.000267994558880684, 'epoch': 0.24}
{'loss': 5.7918, 'grad_norm': 2.2763423919677734, 'learning_rate': 0.0002678779634667703, 'epoch': 0.24}
{'loss': 5.5138, 'grad_norm': 1.4490184783935547, 'learning_rate': 0.00026776136805285657, 'epoch': 0.24}
{'loss': 5.4316, 'grad_norm': 1.372033953666687, 'learning_rate': 0.00026764477263894283, 'epoch': 0.24}
{'loss': 5.4159, 'grad_norm': 1.3604340553283691, 'learning_rate': 0.00026752817722502914, 'epoch': 0.24}
{'loss': 5.4256, 'grad_norm': 3.9672691822052, 'learning_rate': 0.0002674115818111154, 'epoch': 0.24}
{'loss': 5.2502, 'grad_norm': 1.4326220750808716, 'learning_rate': 0.0002672949863972017, 'epoch': 0.24}
{'loss': 5.2559, 'grad_norm': 1.4476362466812134, 'learning_rate': 0.000267178390983288, 'epoch': 0.25}
{'loss': 5.0953, 'grad_norm': 1.606113076210022, 'learning_rate': 0.00026706179556937424, 'epoch': 0.25}
{'loss': 5.1872, 'grad_norm': 1.8622422218322754, 'learning_rate': 0.00026694520015546055, 'epoch': 0.25}
{'loss': 4.9424, 'grad_norm': 1.6668227910995483, 'learning_rate': 0.0002668286047415468, 'epoch': 0.25}
{'loss': 4.8056, 'grad_norm': 2.18464994430542, 'learning_rate': 0.00026671200932763307, 'epoch': 0.25}
{'loss': 4.3906, 'grad_norm': 3.2759673595428467, 'learning_rate': 0.0002665954139137194, 'epoch': 0.25}
{'loss': 5.8773, 'grad_norm': 4.0775251388549805, 'learning_rate': 0.00026647881849980564, 'epoch': 0.25}
{'loss': 5.6918, 'grad_norm': 1.7101197242736816, 'learning_rate': 0.0002663622230858919, 'epoch': 0.25}
{'loss': 5.461, 'grad_norm': 1.3309930562973022, 'learning_rate': 0.0002662456276719782, 'epoch': 0.25}
{'loss': 5.3975, 'grad_norm': 1.2353166341781616, 'learning_rate': 0.0002661290322580645, 'epoch': 0.25}
{'loss': 5.2868, 'grad_norm': 1.4060609340667725, 'learning_rate': 0.0002660124368441508, 'epoch': 0.25}
{'loss': 5.2892, 'grad_norm': 1.4098695516586304, 'learning_rate': 0.00026589584143023705, 'epoch': 0.25}
{'loss': 5.3175, 'grad_norm': 1.6467684507369995, 'learning_rate': 0.0002657792460163233, 'epoch': 0.25}
{'loss': 5.1025, 'grad_norm': 1.4836148023605347, 'learning_rate': 0.0002656626506024096, 'epoch': 0.26}
{'loss': 5.0619, 'grad_norm': 1.449891448020935, 'learning_rate': 0.0002655460551884959, 'epoch': 0.26}
{'loss': 5.0387, 'grad_norm': 1.8143517971038818, 'learning_rate': 0.0002654294597745822, 'epoch': 0.26}
{'loss': 5.006, 'grad_norm': 1.985313892364502, 'learning_rate': 0.00026531286436066846, 'epoch': 0.26}
{'loss': 4.838, 'grad_norm': 2.39658784866333, 'learning_rate': 0.00026519626894675477, 'epoch': 0.26}
{'loss': 5.1371, 'grad_norm': 4.723371982574463, 'learning_rate': 0.00026507967353284103, 'epoch': 0.26}
{'loss': 5.6125, 'grad_norm': 2.051189422607422, 'learning_rate': 0.0002649630781189273, 'epoch': 0.26}
{'loss': 5.4441, 'grad_norm': 1.2982041835784912, 'learning_rate': 0.0002648464827050136, 'epoch': 0.26}
{'loss': 5.4251, 'grad_norm': 1.3921759128570557, 'learning_rate': 0.00026472988729109987, 'epoch': 0.26}
{'loss': 5.2739, 'grad_norm': 1.5065016746520996, 'learning_rate': 0.0002646132918771861, 'epoch': 0.26}
{'loss': 5.2423, 'grad_norm': 1.429443120956421, 'learning_rate': 0.00026449669646327244, 'epoch': 0.26}
{'loss': 5.1878, 'grad_norm': 1.5392154455184937, 'learning_rate': 0.0002643801010493587, 'epoch': 0.26}
{'loss': 5.0235, 'grad_norm': 1.5920594930648804, 'learning_rate': 0.00026426350563544496, 'epoch': 0.26}
{'loss': 5.0918, 'grad_norm': 1.526092529296875, 'learning_rate': 0.0002641469102215313, 'epoch': 0.27}
{'loss': 5.0462, 'grad_norm': 1.7688708305358887, 'learning_rate': 0.00026403031480761753, 'epoch': 0.27}
{'loss': 4.9688, 'grad_norm': 1.7978931665420532, 'learning_rate': 0.0002639137193937038, 'epoch': 0.27}
{'loss': 4.8338, 'grad_norm': 2.141981840133667, 'learning_rate': 0.0002637971239797901, 'epoch': 0.27}
{'loss': 4.4807, 'grad_norm': 3.3525631427764893, 'learning_rate': 0.00026368052856587637, 'epoch': 0.27}
{'loss': 5.8023, 'grad_norm': 3.9432103633880615, 'learning_rate': 0.0002635639331519627, 'epoch': 0.27}
{'loss': 5.5094, 'grad_norm': 1.5656150579452515, 'learning_rate': 0.00026344733773804894, 'epoch': 0.27}
{'loss': 5.359, 'grad_norm': 1.3323715925216675, 'learning_rate': 0.0002633307423241352, 'epoch': 0.27}
{'loss': 5.375, 'grad_norm': 1.277880311012268, 'learning_rate': 0.0002632141469102215, 'epoch': 0.27}
{'loss': 5.2695, 'grad_norm': 1.4258018732070923, 'learning_rate': 0.0002630975514963078, 'epoch': 0.27}
{'loss': 5.2188, 'grad_norm': 1.503006100654602, 'learning_rate': 0.0002629809560823941, 'epoch': 0.27}
{'loss': 5.1889, 'grad_norm': 1.4114634990692139, 'learning_rate': 0.00026286436066848035, 'epoch': 0.27}
{'loss': 5.1386, 'grad_norm': 1.6468452215194702, 'learning_rate': 0.00026274776525456666, 'epoch': 0.27}
{'loss': 4.9879, 'grad_norm': 1.6099079847335815, 'learning_rate': 0.0002626311698406529, 'epoch': 0.28}
{'loss': 5.0033, 'grad_norm': 1.775223731994629, 'learning_rate': 0.0002625145744267392, 'epoch': 0.28}
{'loss': 4.8317, 'grad_norm': 1.865523338317871, 'learning_rate': 0.0002623979790128255, 'epoch': 0.28}
{'loss': 4.5382, 'grad_norm': 2.863466501235962, 'learning_rate': 0.00026228138359891175, 'epoch': 0.28}
{'loss': 4.921, 'grad_norm': 4.52092170715332, 'learning_rate': 0.00026216478818499807, 'epoch': 0.28}
{'loss': 5.6089, 'grad_norm': 2.0467309951782227, 'learning_rate': 0.00026204819277108433, 'epoch': 0.28}
{'loss': 5.4636, 'grad_norm': 1.3884738683700562, 'learning_rate': 0.0002619315973571706, 'epoch': 0.28}
{'loss': 5.3004, 'grad_norm': 1.3842896223068237, 'learning_rate': 0.00026181500194325685, 'epoch': 0.28}
{'loss': 5.26, 'grad_norm': 1.4058656692504883, 'learning_rate': 0.00026169840652934316, 'epoch': 0.28}
{'loss': 5.22, 'grad_norm': 1.459294319152832, 'learning_rate': 0.0002615818111154294, 'epoch': 0.28}
{'loss': 5.2204, 'grad_norm': 1.4622218608856201, 'learning_rate': 0.0002614652157015157, 'epoch': 0.28}
{'loss': 5.2531, 'grad_norm': 1.5515373945236206, 'learning_rate': 0.000261348620287602, 'epoch': 0.28}
{'loss': 5.0727, 'grad_norm': 1.4548943042755127, 'learning_rate': 0.00026123202487368826, 'epoch': 0.28}
{'loss': 4.989, 'grad_norm': 1.7082189321517944, 'learning_rate': 0.00026111542945977457, 'epoch': 0.29}
{'loss': 4.7884, 'grad_norm': 1.8503934144973755, 'learning_rate': 0.00026099883404586083, 'epoch': 0.29}
{'loss': 4.7545, 'grad_norm': 1.9356002807617188, 'learning_rate': 0.0002608822386319471, 'epoch': 0.29}
{'loss': 4.4465, 'grad_norm': 3.3653454780578613, 'learning_rate': 0.0002607656432180334, 'epoch': 0.29}
{'loss': 5.6236, 'grad_norm': 2.9192962646484375, 'learning_rate': 0.00026064904780411966, 'epoch': 0.29}
{'loss': 5.4719, 'grad_norm': 1.5548874139785767, 'learning_rate': 0.000260532452390206, 'epoch': 0.29}
{'loss': 5.2703, 'grad_norm': 1.301802635192871, 'learning_rate': 0.00026041585697629224, 'epoch': 0.29}
{'loss': 5.2235, 'grad_norm': 1.206459641456604, 'learning_rate': 0.00026029926156237855, 'epoch': 0.29}
{'loss': 5.1731, 'grad_norm': 1.366025447845459, 'learning_rate': 0.0002601826661484648, 'epoch': 0.29}
{'loss': 5.0474, 'grad_norm': 1.475782871246338, 'learning_rate': 0.00026006607073455107, 'epoch': 0.29}
{'loss': 5.0367, 'grad_norm': 1.5242449045181274, 'learning_rate': 0.0002599494753206374, 'epoch': 0.29}
{'loss': 5.0876, 'grad_norm': 1.536307454109192, 'learning_rate': 0.00025983287990672364, 'epoch': 0.29}
{'loss': 5.0397, 'grad_norm': 1.7880650758743286, 'learning_rate': 0.00025971628449280996, 'epoch': 0.29}
{'loss': 4.962, 'grad_norm': 1.7590512037277222, 'learning_rate': 0.0002595996890788962, 'epoch': 0.29}
{'loss': 4.8025, 'grad_norm': 1.883832335472107, 'learning_rate': 0.0002594830936649825, 'epoch': 0.3}
{'loss': 4.5228, 'grad_norm': 2.8481266498565674, 'learning_rate': 0.0002593664982510688, 'epoch': 0.3}
{'loss': 4.9484, 'grad_norm': 4.125312328338623, 'learning_rate': 0.00025924990283715505, 'epoch': 0.3}
{'loss': 5.5041, 'grad_norm': 1.9104901552200317, 'learning_rate': 0.0002591333074232413, 'epoch': 0.3}
{'loss': 5.3225, 'grad_norm': 1.3760372400283813, 'learning_rate': 0.0002590167120093276, 'epoch': 0.3}
{'loss': 5.1694, 'grad_norm': 1.3510268926620483, 'learning_rate': 0.0002589001165954139, 'epoch': 0.3}
{'loss': 5.2182, 'grad_norm': 1.338463306427002, 'learning_rate': 0.00025878352118150014, 'epoch': 0.3}
{'loss': 5.129, 'grad_norm': 1.4644348621368408, 'learning_rate': 0.00025866692576758646, 'epoch': 0.3}
{'loss': 5.1067, 'grad_norm': 1.485675573348999, 'learning_rate': 0.0002585503303536727, 'epoch': 0.3}
{'loss': 5.1499, 'grad_norm': 1.4431830644607544, 'learning_rate': 0.00025843373493975903, 'epoch': 0.3}
{'loss': 4.8988, 'grad_norm': 1.6267973184585571, 'learning_rate': 0.0002583171395258453, 'epoch': 0.3}
{'loss': 4.9427, 'grad_norm': 1.9969292879104614, 'learning_rate': 0.00025820054411193155, 'epoch': 0.3}
{'loss': 4.7267, 'grad_norm': 1.9992923736572266, 'learning_rate': 0.00025808394869801786, 'epoch': 0.3}
{'loss': 4.6822, 'grad_norm': 2.065433979034424, 'learning_rate': 0.0002579673532841041, 'epoch': 0.31}
{'loss': 4.2812, 'grad_norm': 3.0058603286743164, 'learning_rate': 0.00025785075787019044, 'epoch': 0.31}
 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 1600/10442 [1:46:47<5:38:51,  2.30s/it][INFO|trainer.py:831] 2024-10-15 03:51:40,445 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 03:51:40,455 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 03:51:40,455 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 03:51:40,455 >>   Batch size = 32
{'eval_loss': 5.181146621704102, 'eval_wer': 0.9578765962255622, 'eval_runtime': 250.4313, 'eval_samples_per_second': 6.353, 'eval_steps_per_second': 0.2, 'epoch': 0.31}
{'loss': 5.5788, 'grad_norm': 3.944408416748047, 'learning_rate': 0.0002577341624562767, 'epoch': 0.31}
{'loss': 5.4659, 'grad_norm': 1.482179045677185, 'learning_rate': 0.000257617567042363, 'epoch': 0.31}
{'loss': 5.267, 'grad_norm': 1.4088809490203857, 'learning_rate': 0.00025750097162844927, 'epoch': 0.31}
{'loss': 5.2621, 'grad_norm': 1.327609896659851, 'learning_rate': 0.00025738437621453553, 'epoch': 0.31}
{'loss': 5.2204, 'grad_norm': 1.3825268745422363, 'learning_rate': 0.00025726778080062185, 'epoch': 0.31}
{'loss': 5.2071, 'grad_norm': 1.476986050605774, 'learning_rate': 0.0002571511853867081, 'epoch': 0.31}
{'loss': 5.1341, 'grad_norm': 1.5004441738128662, 'learning_rate': 0.00025703458997279437, 'epoch': 0.31}
{'loss': 5.142, 'grad_norm': 1.4900643825531006, 'learning_rate': 0.0002569179945588807, 'epoch': 0.31}
{'loss': 4.9991, 'grad_norm': 1.7880607843399048, 'learning_rate': 0.00025680139914496694, 'epoch': 0.31}
{'loss': 4.9009, 'grad_norm': 1.86393404006958, 'learning_rate': 0.0002566848037310532, 'epoch': 0.31}
{'loss': 4.663, 'grad_norm': 2.0693023204803467, 'learning_rate': 0.0002565682083171395, 'epoch': 0.31}
{'loss': 4.4761, 'grad_norm': 2.458151340484619, 'learning_rate': 0.00025645161290322577, 'epoch': 0.32}
{'loss': 4.8201, 'grad_norm': 4.552526473999023, 'learning_rate': 0.00025633501748931203, 'epoch': 0.32}
{'loss': 5.6602, 'grad_norm': 2.0393152236938477, 'learning_rate': 0.00025621842207539835, 'epoch': 0.32}
{'loss': 5.2489, 'grad_norm': 1.5209696292877197, 'learning_rate': 0.0002561018266614846, 'epoch': 0.32}
{'loss': 5.2876, 'grad_norm': 1.3888957500457764, 'learning_rate': 0.0002559852312475709, 'epoch': 0.32}
{'loss': 5.117, 'grad_norm': 1.376517415046692, 'learning_rate': 0.0002558686358336572, 'epoch': 0.32}
{'loss': 5.2726, 'grad_norm': 1.4673125743865967, 'learning_rate': 0.00025575204041974344, 'epoch': 0.32}
{'loss': 4.8545, 'grad_norm': 1.5257211923599243, 'learning_rate': 0.00025563544500582975, 'epoch': 0.32}
{'loss': 4.9347, 'grad_norm': 1.5539472103118896, 'learning_rate': 0.000255518849591916, 'epoch': 0.32}
{'loss': 5.021, 'grad_norm': 1.7343415021896362, 'learning_rate': 0.00025540225417800233, 'epoch': 0.32}
{'loss': 4.8679, 'grad_norm': 1.8227218389511108, 'learning_rate': 0.0002552856587640886, 'epoch': 0.32}
{'loss': 4.7469, 'grad_norm': 1.901928186416626, 'learning_rate': 0.0002551690633501749, 'epoch': 0.32}
{'loss': 4.7006, 'grad_norm': 49.0568733215332, 'learning_rate': 0.00025505246793626116, 'epoch': 0.32}
{'loss': 4.3138, 'grad_norm': 3.497955560684204, 'learning_rate': 0.0002549358725223474, 'epoch': 0.33}
{'loss': 5.5364, 'grad_norm': 3.0065486431121826, 'learning_rate': 0.00025481927710843373, 'epoch': 0.33}
{'loss': 5.4137, 'grad_norm': 1.4998207092285156, 'learning_rate': 0.00025470268169452, 'epoch': 0.33}
{'loss': 5.2941, 'grad_norm': 1.222887396812439, 'learning_rate': 0.0002545860862806063, 'epoch': 0.33}
{'loss': 5.1854, 'grad_norm': 1.3465007543563843, 'learning_rate': 0.00025446949086669257, 'epoch': 0.33}
{'loss': 5.0866, 'grad_norm': 1.3262418508529663, 'learning_rate': 0.00025435289545277883, 'epoch': 0.33}
{'loss': 5.0438, 'grad_norm': 1.5372346639633179, 'learning_rate': 0.00025423630003886514, 'epoch': 0.33}
{'loss': 5.1349, 'grad_norm': 1.4641854763031006, 'learning_rate': 0.0002541197046249514, 'epoch': 0.33}
{'loss': 4.9984, 'grad_norm': 1.4881961345672607, 'learning_rate': 0.00025400310921103766, 'epoch': 0.33}
{'loss': 4.8965, 'grad_norm': 1.8009709119796753, 'learning_rate': 0.0002538865137971239, 'epoch': 0.33}
{'loss': 4.76, 'grad_norm': 1.8358711004257202, 'learning_rate': 0.00025376991838321023, 'epoch': 0.33}
{'loss': 4.6667, 'grad_norm': 2.054889440536499, 'learning_rate': 0.0002536533229692965, 'epoch': 0.33}
{'loss': 4.4684, 'grad_norm': 2.4104886054992676, 'learning_rate': 0.0002535367275553828, 'epoch': 0.33}
{'loss': 4.7096, 'grad_norm': 4.028955936431885, 'learning_rate': 0.00025342013214146907, 'epoch': 0.34}
{'loss': 5.5021, 'grad_norm': 2.1288557052612305, 'learning_rate': 0.00025330353672755533, 'epoch': 0.34}
{'loss': 5.1736, 'grad_norm': 1.5355863571166992, 'learning_rate': 0.00025318694131364164, 'epoch': 0.34}
{'loss': 5.1561, 'grad_norm': 1.3252028226852417, 'learning_rate': 0.0002530703458997279, 'epoch': 0.34}
{'loss': 5.1747, 'grad_norm': 1.2764308452606201, 'learning_rate': 0.0002529537504858142, 'epoch': 0.34}
{'loss': 5.1176, 'grad_norm': 1.4755582809448242, 'learning_rate': 0.0002528371550719005, 'epoch': 0.34}
{'loss': 4.9955, 'grad_norm': 1.5621918439865112, 'learning_rate': 0.0002527205596579868, 'epoch': 0.34}
{'loss': 4.9195, 'grad_norm': 1.7123808860778809, 'learning_rate': 0.00025260396424407305, 'epoch': 0.34}
{'loss': 5.05, 'grad_norm': 1.5357387065887451, 'learning_rate': 0.0002524873688301593, 'epoch': 0.34}
{'loss': 4.8115, 'grad_norm': 1.7427173852920532, 'learning_rate': 0.0002523707734162456, 'epoch': 0.34}
{'loss': 4.8952, 'grad_norm': 1.7802228927612305, 'learning_rate': 0.0002522541780023319, 'epoch': 0.34}
{'loss': 4.6094, 'grad_norm': 2.014163017272949, 'learning_rate': 0.0002521375825884182, 'epoch': 0.34}
{'loss': 4.216, 'grad_norm': 3.75899600982666, 'learning_rate': 0.00025202098717450446, 'epoch': 0.34}
{'loss': 5.3298, 'grad_norm': 2.9649314880371094, 'learning_rate': 0.0002519043917605907, 'epoch': 0.35}
{'loss': 5.2769, 'grad_norm': 1.5100077390670776, 'learning_rate': 0.00025178779634667703, 'epoch': 0.35}
{'loss': 5.2496, 'grad_norm': 1.448815107345581, 'learning_rate': 0.0002516712009327633, 'epoch': 0.35}
{'loss': 5.2155, 'grad_norm': 1.5046238899230957, 'learning_rate': 0.00025155460551884955, 'epoch': 0.35}
{'loss': 4.945, 'grad_norm': 1.3583849668502808, 'learning_rate': 0.00025143801010493586, 'epoch': 0.35}
{'loss': 5.0192, 'grad_norm': 1.392796277999878, 'learning_rate': 0.0002513214146910221, 'epoch': 0.35}
{'loss': 4.8872, 'grad_norm': 1.4628987312316895, 'learning_rate': 0.0002512048192771084, 'epoch': 0.35}
{'loss': 4.8016, 'grad_norm': 1.548405408859253, 'learning_rate': 0.0002510882238631947, 'epoch': 0.35}
{'loss': 4.9433, 'grad_norm': 1.7164334058761597, 'learning_rate': 0.00025097162844928096, 'epoch': 0.35}
{'loss': 4.7969, 'grad_norm': 1.8446587324142456, 'learning_rate': 0.00025085503303536727, 'epoch': 0.35}
{'loss': 4.6805, 'grad_norm': 1.8420521020889282, 'learning_rate': 0.00025073843762145353, 'epoch': 0.35}
{'loss': 4.3447, 'grad_norm': 2.531036376953125, 'learning_rate': 0.0002506218422075398, 'epoch': 0.35}
{'loss': 4.6334, 'grad_norm': 3.750230312347412, 'learning_rate': 0.0002505052467936261, 'epoch': 0.35}
{'loss': 5.5161, 'grad_norm': 1.9822344779968262, 'learning_rate': 0.00025038865137971236, 'epoch': 0.36}
{'loss': 5.337, 'grad_norm': 1.4345356225967407, 'learning_rate': 0.0002502720559657987, 'epoch': 0.36}
{'loss': 5.0396, 'grad_norm': 1.3700008392333984, 'learning_rate': 0.00025015546055188494, 'epoch': 0.36}
{'loss': 5.0163, 'grad_norm': 1.3567613363265991, 'learning_rate': 0.00025003886513797125, 'epoch': 0.36}
{'loss': 5.0586, 'grad_norm': 1.463079571723938, 'learning_rate': 0.0002499222697240575, 'epoch': 0.36}
{'loss': 4.8979, 'grad_norm': 1.50339937210083, 'learning_rate': 0.00024980567431014377, 'epoch': 0.36}
{'loss': 5.0207, 'grad_norm': 1.5135948657989502, 'learning_rate': 0.0002496890788962301, 'epoch': 0.36}
{'loss': 4.9216, 'grad_norm': 1.5488686561584473, 'learning_rate': 0.00024957248348231635, 'epoch': 0.36}
{'loss': 4.7899, 'grad_norm': 1.799849510192871, 'learning_rate': 0.0002494558880684026, 'epoch': 0.36}
{'loss': 4.7279, 'grad_norm': 1.9320757389068604, 'learning_rate': 0.0002493392926544889, 'epoch': 0.36}
{'loss': 4.5528, 'grad_norm': 1.9916595220565796, 'learning_rate': 0.0002492226972405752, 'epoch': 0.36}
{'loss': 4.2377, 'grad_norm': 2.881399631500244, 'learning_rate': 0.00024910610182666144, 'epoch': 0.36}
{'loss': 5.3784, 'grad_norm': 2.6328701972961426, 'learning_rate': 0.00024898950641274775, 'epoch': 0.36}
{'loss': 5.2397, 'grad_norm': 1.5891810655593872, 'learning_rate': 0.000248872910998834, 'epoch': 0.37}
{'loss': 5.2347, 'grad_norm': 1.3591605424880981, 'learning_rate': 0.00024875631558492027, 'epoch': 0.37}
{'loss': 5.0738, 'grad_norm': 1.3675886392593384, 'learning_rate': 0.0002486397201710066, 'epoch': 0.37}
{'loss': 5.0515, 'grad_norm': 1.4082709550857544, 'learning_rate': 0.00024852312475709285, 'epoch': 0.37}
{'loss': 4.9696, 'grad_norm': 1.4377617835998535, 'learning_rate': 0.00024840652934317916, 'epoch': 0.37}
{'loss': 5.0528, 'grad_norm': 1.6646302938461304, 'learning_rate': 0.0002482899339292654, 'epoch': 0.37}
{'loss': 4.9223, 'grad_norm': 1.5905441045761108, 'learning_rate': 0.0002481733385153517, 'epoch': 0.37}
{'loss': 4.8727, 'grad_norm': 1.818530559539795, 'learning_rate': 0.000248056743101438, 'epoch': 0.37}
{'loss': 4.7168, 'grad_norm': 1.736615777015686, 'learning_rate': 0.00024794014768752425, 'epoch': 0.37}
{'loss': 4.5077, 'grad_norm': 2.016287326812744, 'learning_rate': 0.00024782355227361057, 'epoch': 0.37}
{'loss': 4.4518, 'grad_norm': 2.451843500137329, 'learning_rate': 0.0002477069568596968, 'epoch': 0.37}
{'loss': 4.7254, 'grad_norm': 3.201421022415161, 'learning_rate': 0.00024759036144578314, 'epoch': 0.37}
{'loss': 5.2177, 'grad_norm': 1.638013482093811, 'learning_rate': 0.0002474737660318694, 'epoch': 0.37}
{'loss': 5.1566, 'grad_norm': 1.3134461641311646, 'learning_rate': 0.00024735717061795566, 'epoch': 0.38}
{'loss': 5.092, 'grad_norm': 1.2873189449310303, 'learning_rate': 0.000247240575204042, 'epoch': 0.38}
{'loss': 5.0604, 'grad_norm': 1.4353729486465454, 'learning_rate': 0.00024712397979012823, 'epoch': 0.38}
{'loss': 4.9066, 'grad_norm': 1.5006927251815796, 'learning_rate': 0.00024700738437621455, 'epoch': 0.38}
{'loss': 5.034, 'grad_norm': 1.5291388034820557, 'learning_rate': 0.0002468907889623008, 'epoch': 0.38}
{'loss': 4.7939, 'grad_norm': 1.6507744789123535, 'learning_rate': 0.00024677419354838707, 'epoch': 0.38}
{'loss': 4.9001, 'grad_norm': 1.6288625001907349, 'learning_rate': 0.0002466575981344734, 'epoch': 0.38}
{'loss': 4.862, 'grad_norm': 1.7590157985687256, 'learning_rate': 0.00024654100272055964, 'epoch': 0.38}
{'loss': 4.7031, 'grad_norm': 1.83420991897583, 'learning_rate': 0.0002464244073066459, 'epoch': 0.38}
{'loss': 4.5983, 'grad_norm': 2.2549712657928467, 'learning_rate': 0.00024630781189273216, 'epoch': 0.38}
{'loss': 4.1908, 'grad_norm': 3.2297518253326416, 'learning_rate': 0.0002461912164788185, 'epoch': 0.38}
 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                         | 2000/10442 [2:14:33<5:27:59,  2.33s/it][INFO|trainer.py:831] 2024-10-15 04:19:26,079 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 04:19:26,091 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 04:19:26,091 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 04:19:26,091 >>   Batch size = 32
{'eval_loss': 4.971185207366943, 'eval_wer': 0.9703073793649, 'eval_runtime': 272.3947, 'eval_samples_per_second': 5.841, 'eval_steps_per_second': 0.184, 'epoch': 0.38}
{'loss': 5.4835, 'grad_norm': 2.589614152908325, 'learning_rate': 0.00024607462106490473, 'epoch': 0.38}
{'loss': 5.212, 'grad_norm': 1.4810121059417725, 'learning_rate': 0.00024595802565099105, 'epoch': 0.38}
{'loss': 5.1161, 'grad_norm': 1.3763861656188965, 'learning_rate': 0.0002458414302370773, 'epoch': 0.39}
{'loss': 5.0773, 'grad_norm': 1.238808274269104, 'learning_rate': 0.00024572483482316357, 'epoch': 0.39}
{'loss': 4.8587, 'grad_norm': 1.4179713726043701, 'learning_rate': 0.0002456082394092499, 'epoch': 0.39}
{'loss': 4.9053, 'grad_norm': 1.4947444200515747, 'learning_rate': 0.00024549164399533614, 'epoch': 0.39}
{'loss': 4.7273, 'grad_norm': 1.616714358329773, 'learning_rate': 0.00024537504858142246, 'epoch': 0.39}
{'loss': 4.8555, 'grad_norm': 1.5818274021148682, 'learning_rate': 0.0002452584531675087, 'epoch': 0.39}
{'loss': 4.785, 'grad_norm': 1.6073482036590576, 'learning_rate': 0.00024514185775359503, 'epoch': 0.39}
{'loss': 4.5995, 'grad_norm': 1.881062388420105, 'learning_rate': 0.0002450252623396813, 'epoch': 0.39}
{'loss': 4.4404, 'grad_norm': 1.9816046953201294, 'learning_rate': 0.00024490866692576755, 'epoch': 0.39}
{'loss': 4.5028, 'grad_norm': 2.6623106002807617, 'learning_rate': 0.00024479207151185386, 'epoch': 0.39}
{'loss': 4.661, 'grad_norm': 3.382277727127075, 'learning_rate': 0.0002446754760979401, 'epoch': 0.39}
{'loss': 5.3247, 'grad_norm': 1.5299463272094727, 'learning_rate': 0.00024455888068402644, 'epoch': 0.39}
{'loss': 5.0794, 'grad_norm': 1.2994675636291504, 'learning_rate': 0.0002444422852701127, 'epoch': 0.39}
{'loss': 4.9743, 'grad_norm': 1.2982277870178223, 'learning_rate': 0.00024432568985619896, 'epoch': 0.4}
{'loss': 5.0632, 'grad_norm': 1.4310129880905151, 'learning_rate': 0.00024420909444228527, 'epoch': 0.4}
{'loss': 4.9095, 'grad_norm': 1.383481740951538, 'learning_rate': 0.0002440924990283715, 'epoch': 0.4}
{'loss': 4.957, 'grad_norm': 1.6309453248977661, 'learning_rate': 0.00024397590361445782, 'epoch': 0.4}
{'loss': 4.8336, 'grad_norm': 1.5302742719650269, 'learning_rate': 0.00024385930820054408, 'epoch': 0.4}
{'loss': 4.7656, 'grad_norm': 1.641721248626709, 'learning_rate': 0.0002437427127866304, 'epoch': 0.4}
{'loss': 4.7444, 'grad_norm': 1.7218044996261597, 'learning_rate': 0.00024362611737271665, 'epoch': 0.4}
{'loss': 4.4945, 'grad_norm': 1.9047220945358276, 'learning_rate': 0.00024350952195880294, 'epoch': 0.4}
{'loss': 4.6435, 'grad_norm': 2.3230581283569336, 'learning_rate': 0.00024339292654488922, 'epoch': 0.4}
{'loss': 4.1911, 'grad_norm': 3.5707435607910156, 'learning_rate': 0.00024327633113097548, 'epoch': 0.4}
{'loss': 5.3843, 'grad_norm': 3.0370616912841797, 'learning_rate': 0.00024315973571706177, 'epoch': 0.4}
{'loss': 5.1576, 'grad_norm': 1.4574341773986816, 'learning_rate': 0.00024304314030314803, 'epoch': 0.4}
{'loss': 5.0369, 'grad_norm': 1.5912487506866455, 'learning_rate': 0.00024292654488923434, 'epoch': 0.4}
{'loss': 4.9569, 'grad_norm': 1.4278650283813477, 'learning_rate': 0.0002428099494753206, 'epoch': 0.41}
{'loss': 4.8937, 'grad_norm': 1.4257746934890747, 'learning_rate': 0.00024269335406140692, 'epoch': 0.41}
{'loss': 4.87, 'grad_norm': 1.52084219455719, 'learning_rate': 0.00024257675864749318, 'epoch': 0.41}
{'loss': 4.8519, 'grad_norm': 1.6129523515701294, 'learning_rate': 0.00024246016323357946, 'epoch': 0.41}
{'loss': 4.8164, 'grad_norm': 1.6173166036605835, 'learning_rate': 0.00024234356781966575, 'epoch': 0.41}
{'loss': 4.7917, 'grad_norm': 1.826269268989563, 'learning_rate': 0.000242226972405752, 'epoch': 0.41}
{'loss': 4.692, 'grad_norm': 1.8946846723556519, 'learning_rate': 0.0002421103769918383, 'epoch': 0.41}
{'loss': 4.5008, 'grad_norm': 1.9779828786849976, 'learning_rate': 0.00024199378157792458, 'epoch': 0.41}
{'loss': 4.2121, 'grad_norm': 2.3950462341308594, 'learning_rate': 0.00024187718616401087, 'epoch': 0.41}
{'loss': 4.5227, 'grad_norm': 3.434048891067505, 'learning_rate': 0.00024176059075009713, 'epoch': 0.41}
{'loss': 5.2357, 'grad_norm': 1.916651964187622, 'learning_rate': 0.00024164399533618345, 'epoch': 0.41}
{'loss': 5.0438, 'grad_norm': 1.4301376342773438, 'learning_rate': 0.0002415273999222697, 'epoch': 0.41}
{'loss': 4.9929, 'grad_norm': 1.3284716606140137, 'learning_rate': 0.00024141080450835596, 'epoch': 0.41}
{'loss': 4.9514, 'grad_norm': 1.318854808807373, 'learning_rate': 0.00024129420909444228, 'epoch': 0.42}
{'loss': 4.8511, 'grad_norm': 1.382538080215454, 'learning_rate': 0.00024117761368052854, 'epoch': 0.42}
{'loss': 4.8601, 'grad_norm': 1.479910135269165, 'learning_rate': 0.00024106101826661483, 'epoch': 0.42}
{'loss': 4.7693, 'grad_norm': 1.5859113931655884, 'learning_rate': 0.0002409444228527011, 'epoch': 0.42}
{'loss': 4.8146, 'grad_norm': 1.7080895900726318, 'learning_rate': 0.0002408278274387874, 'epoch': 0.42}
{'loss': 4.7885, 'grad_norm': 1.8553487062454224, 'learning_rate': 0.00024071123202487366, 'epoch': 0.42}
{'loss': 4.7117, 'grad_norm': 1.8229446411132812, 'learning_rate': 0.00024059463661095995, 'epoch': 0.42}
{'loss': 4.4854, 'grad_norm': 2.4081838130950928, 'learning_rate': 0.00024047804119704623, 'epoch': 0.42}
{'loss': 4.0153, 'grad_norm': 3.395303249359131, 'learning_rate': 0.0002403614457831325, 'epoch': 0.42}
{'loss': 5.4249, 'grad_norm': 3.1225268840789795, 'learning_rate': 0.0002402448503692188, 'epoch': 0.42}
{'loss': 5.2047, 'grad_norm': 1.5632174015045166, 'learning_rate': 0.00024012825495530507, 'epoch': 0.42}
{'loss': 5.009, 'grad_norm': 1.4119279384613037, 'learning_rate': 0.00024001165954139135, 'epoch': 0.42}
{'loss': 5.0616, 'grad_norm': 1.8106129169464111, 'learning_rate': 0.00023989506412747764, 'epoch': 0.42}
{'loss': 4.8944, 'grad_norm': 1.5417265892028809, 'learning_rate': 0.0002397784687135639, 'epoch': 0.43}
{'loss': 4.6959, 'grad_norm': 1.5479892492294312, 'learning_rate': 0.0002396618732996502, 'epoch': 0.43}
{'loss': 4.8455, 'grad_norm': 1.6032732725143433, 'learning_rate': 0.00023954527788573647, 'epoch': 0.43}
{'loss': 4.8095, 'grad_norm': 1.7209237813949585, 'learning_rate': 0.00023942868247182276, 'epoch': 0.43}
{'loss': 4.69, 'grad_norm': 1.7106339931488037, 'learning_rate': 0.00023931208705790902, 'epoch': 0.43}
{'loss': 4.6086, 'grad_norm': 1.9259212017059326, 'learning_rate': 0.00023919549164399533, 'epoch': 0.43}
{'loss': 4.5462, 'grad_norm': 2.223384380340576, 'learning_rate': 0.0002390788962300816, 'epoch': 0.43}
{'loss': 4.2593, 'grad_norm': 2.447493076324463, 'learning_rate': 0.00023896230081616785, 'epoch': 0.43}
{'loss': 4.5615, 'grad_norm': 3.2942605018615723, 'learning_rate': 0.00023884570540225417, 'epoch': 0.43}
{'loss': 5.1125, 'grad_norm': 1.7965641021728516, 'learning_rate': 0.00023872910998834043, 'epoch': 0.43}
{'loss': 5.0816, 'grad_norm': 1.4605727195739746, 'learning_rate': 0.00023861251457442671, 'epoch': 0.43}
{'loss': 5.0352, 'grad_norm': 1.4981558322906494, 'learning_rate': 0.000238495919160513, 'epoch': 0.43}
{'loss': 4.9832, 'grad_norm': 1.3117833137512207, 'learning_rate': 0.0002383793237465993, 'epoch': 0.43}
{'loss': 4.8, 'grad_norm': 1.48018217086792, 'learning_rate': 0.00023826272833268555, 'epoch': 0.44}
{'loss': 4.8552, 'grad_norm': 1.7015373706817627, 'learning_rate': 0.00023814613291877183, 'epoch': 0.44}
{'loss': 4.8536, 'grad_norm': 3.2458324432373047, 'learning_rate': 0.00023802953750485812, 'epoch': 0.44}
{'loss': 4.6401, 'grad_norm': 1.6240841150283813, 'learning_rate': 0.00023791294209094438, 'epoch': 0.44}
{'loss': 4.7333, 'grad_norm': 1.9843896627426147, 'learning_rate': 0.0002377963466770307, 'epoch': 0.44}
{'loss': 4.6996, 'grad_norm': 1.8439795970916748, 'learning_rate': 0.00023767975126311695, 'epoch': 0.44}
{'loss': 4.2921, 'grad_norm': 2.217743158340454, 'learning_rate': 0.00023756315584920327, 'epoch': 0.44}
{'loss': 4.1169, 'grad_norm': 2.951761484146118, 'learning_rate': 0.00023744656043528953, 'epoch': 0.44}
{'loss': 5.3227, 'grad_norm': 2.8057730197906494, 'learning_rate': 0.0002373299650213758, 'epoch': 0.44}
{'loss': 5.1049, 'grad_norm': 1.50455641746521, 'learning_rate': 0.0002372133696074621, 'epoch': 0.44}
{'loss': 4.8623, 'grad_norm': 1.3485277891159058, 'learning_rate': 0.00023709677419354836, 'epoch': 0.44}
{'loss': 4.8453, 'grad_norm': 1.4128303527832031, 'learning_rate': 0.00023698017877963465, 'epoch': 0.44}
{'loss': 4.8512, 'grad_norm': 1.5700891017913818, 'learning_rate': 0.0002368635833657209, 'epoch': 0.44}
{'loss': 4.9415, 'grad_norm': 1.3947982788085938, 'learning_rate': 0.00023674698795180722, 'epoch': 0.45}
{'loss': 4.8925, 'grad_norm': 1.475205421447754, 'learning_rate': 0.00023663039253789348, 'epoch': 0.45}
{'loss': 4.7179, 'grad_norm': 1.666869044303894, 'learning_rate': 0.00023651379712397974, 'epoch': 0.45}
{'loss': 4.7051, 'grad_norm': 1.7142399549484253, 'learning_rate': 0.00023639720171006606, 'epoch': 0.45}
{'loss': 4.6898, 'grad_norm': 1.9369213581085205, 'learning_rate': 0.00023628060629615232, 'epoch': 0.45}
{'loss': 4.5723, 'grad_norm': 2.0354111194610596, 'learning_rate': 0.00023616401088223863, 'epoch': 0.45}
{'loss': 4.4036, 'grad_norm': 2.494687080383301, 'learning_rate': 0.0002360474154683249, 'epoch': 0.45}
{'loss': 4.6183, 'grad_norm': 3.858858823776245, 'learning_rate': 0.00023593082005441118, 'epoch': 0.45}
{'loss': 5.2471, 'grad_norm': 1.9310684204101562, 'learning_rate': 0.00023581422464049746, 'epoch': 0.45}
{'loss': 5.0137, 'grad_norm': 1.3559458255767822, 'learning_rate': 0.00023569762922658372, 'epoch': 0.45}
{'loss': 4.8535, 'grad_norm': 1.518368124961853, 'learning_rate': 0.00023558103381267, 'epoch': 0.45}
{'loss': 4.7426, 'grad_norm': 1.4000355005264282, 'learning_rate': 0.0002354644383987563, 'epoch': 0.45}
{'loss': 4.8958, 'grad_norm': 1.4116029739379883, 'learning_rate': 0.00023534784298484258, 'epoch': 0.45}
{'loss': 4.7924, 'grad_norm': 1.517743468284607, 'learning_rate': 0.00023523124757092884, 'epoch': 0.46}
{'loss': 4.7113, 'grad_norm': 1.6323333978652954, 'learning_rate': 0.00023511465215701516, 'epoch': 0.46}
{'loss': 4.7466, 'grad_norm': 1.7118428945541382, 'learning_rate': 0.00023499805674310142, 'epoch': 0.46}
{'loss': 4.5604, 'grad_norm': 1.7262678146362305, 'learning_rate': 0.0002348814613291877, 'epoch': 0.46}
{'loss': 4.5201, 'grad_norm': 1.8097890615463257, 'learning_rate': 0.000234764865915274, 'epoch': 0.46}
{'loss': 4.4734, 'grad_norm': 2.446481943130493, 'learning_rate': 0.00023464827050136025, 'epoch': 0.46}
{'loss': 3.93, 'grad_norm': 3.6284217834472656, 'learning_rate': 0.00023453167508744654, 'epoch': 0.46}
 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                      | 2400/10442 [2:42:32<5:14:02,  2.34s/it][INFO|trainer.py:831] 2024-10-15 04:47:25,614 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 04:47:25,624 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 04:47:25,624 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 04:47:25,624 >>   Batch size = 32
{'eval_loss': 4.809380054473877, 'eval_wer': 0.964939541191095, 'eval_runtime': 251.1923, 'eval_samples_per_second': 6.334, 'eval_steps_per_second': 0.199, 'epoch': 0.46}
{'loss': 5.4162, 'grad_norm': 3.056370973587036, 'learning_rate': 0.00023441507967353282, 'epoch': 0.46}
{'loss': 5.0754, 'grad_norm': 1.5593115091323853, 'learning_rate': 0.0002342984842596191, 'epoch': 0.46}
{'loss': 4.8451, 'grad_norm': 1.4127391576766968, 'learning_rate': 0.00023418188884570537, 'epoch': 0.46}
{'loss': 4.9068, 'grad_norm': 1.4870041608810425, 'learning_rate': 0.00023406529343179168, 'epoch': 0.46}
{'loss': 4.8036, 'grad_norm': 1.4378163814544678, 'learning_rate': 0.00023394869801787794, 'epoch': 0.46}
{'loss': 4.9118, 'grad_norm': 1.5015422105789185, 'learning_rate': 0.0002338321026039642, 'epoch': 0.46}
{'loss': 4.7279, 'grad_norm': 1.634887456893921, 'learning_rate': 0.00023371550719005052, 'epoch': 0.47}
{'loss': 4.7767, 'grad_norm': 1.805213451385498, 'learning_rate': 0.00023359891177613678, 'epoch': 0.47}
{'loss': 4.6791, 'grad_norm': 1.7015457153320312, 'learning_rate': 0.00023348231636222306, 'epoch': 0.47}
{'loss': 4.5399, 'grad_norm': 1.8082447052001953, 'learning_rate': 0.00023336572094830935, 'epoch': 0.47}
{'loss': 4.2918, 'grad_norm': 2.0021321773529053, 'learning_rate': 0.00023324912553439564, 'epoch': 0.47}
{'loss': 4.279, 'grad_norm': 2.699082612991333, 'learning_rate': 0.0002331325301204819, 'epoch': 0.47}
{'loss': 4.5112, 'grad_norm': 3.166048526763916, 'learning_rate': 0.00023301593470656819, 'epoch': 0.47}
{'loss': 5.1825, 'grad_norm': 1.8310561180114746, 'learning_rate': 0.00023289933929265447, 'epoch': 0.47}
{'loss': 4.9848, 'grad_norm': 1.4523320198059082, 'learning_rate': 0.00023278274387874073, 'epoch': 0.47}
{'loss': 4.9142, 'grad_norm': 1.3383382558822632, 'learning_rate': 0.00023266614846482705, 'epoch': 0.47}
{'loss': 4.8243, 'grad_norm': 1.4002771377563477, 'learning_rate': 0.0002325495530509133, 'epoch': 0.47}
{'loss': 4.9544, 'grad_norm': 1.4843032360076904, 'learning_rate': 0.0002324329576369996, 'epoch': 0.47}
{'loss': 4.8113, 'grad_norm': 1.6812134981155396, 'learning_rate': 0.00023231636222308588, 'epoch': 0.47}
{'loss': 4.7466, 'grad_norm': 1.7008544206619263, 'learning_rate': 0.00023219976680917214, 'epoch': 0.48}
{'loss': 4.7528, 'grad_norm': 1.685205340385437, 'learning_rate': 0.00023208317139525843, 'epoch': 0.48}
{'loss': 4.6139, 'grad_norm': 1.7812883853912354, 'learning_rate': 0.0002319665759813447, 'epoch': 0.48}
{'loss': 4.5529, 'grad_norm': 2.0172464847564697, 'learning_rate': 0.000231849980567431, 'epoch': 0.48}
{'loss': 4.3462, 'grad_norm': 2.198025703430176, 'learning_rate': 0.00023173338515351726, 'epoch': 0.48}
{'loss': 3.9962, 'grad_norm': 4.570800304412842, 'learning_rate': 0.00023161678973960357, 'epoch': 0.48}
{'loss': 5.2671, 'grad_norm': 2.420274496078491, 'learning_rate': 0.00023150019432568983, 'epoch': 0.48}
{'loss': 5.0299, 'grad_norm': 1.5308207273483276, 'learning_rate': 0.0002313835989117761, 'epoch': 0.48}
{'loss': 5.0688, 'grad_norm': 1.3719878196716309, 'learning_rate': 0.0002312670034978624, 'epoch': 0.48}
{'loss': 4.8888, 'grad_norm': 1.3962141275405884, 'learning_rate': 0.00023115040808394867, 'epoch': 0.48}
{'loss': 4.8509, 'grad_norm': 1.4294050931930542, 'learning_rate': 0.00023103381267003498, 'epoch': 0.48}
{'loss': 4.8178, 'grad_norm': 1.6792784929275513, 'learning_rate': 0.00023091721725612124, 'epoch': 0.48}
{'loss': 4.7653, 'grad_norm': 1.4914039373397827, 'learning_rate': 0.00023080062184220753, 'epoch': 0.48}
{'loss': 4.7528, 'grad_norm': 1.5497909784317017, 'learning_rate': 0.0002306840264282938, 'epoch': 0.48}
{'loss': 4.5498, 'grad_norm': 1.7272990942001343, 'learning_rate': 0.00023056743101438007, 'epoch': 0.49}
{'loss': 4.5143, 'grad_norm': 1.8500951528549194, 'learning_rate': 0.00023045083560046636, 'epoch': 0.49}
{'loss': 4.3737, 'grad_norm': 2.0845372676849365, 'learning_rate': 0.00023033424018655262, 'epoch': 0.49}
{'loss': 4.3014, 'grad_norm': 2.3154728412628174, 'learning_rate': 0.00023021764477263893, 'epoch': 0.49}
{'loss': 4.5568, 'grad_norm': 2.8566739559173584, 'learning_rate': 0.0002301010493587252, 'epoch': 0.49}
{'loss': 5.1122, 'grad_norm': 1.7085587978363037, 'learning_rate': 0.0002299844539448115, 'epoch': 0.49}
{'loss': 5.025, 'grad_norm': 1.2910747528076172, 'learning_rate': 0.00022986785853089777, 'epoch': 0.49}
{'loss': 4.9213, 'grad_norm': 1.3230904340744019, 'learning_rate': 0.00022975126311698403, 'epoch': 0.49}
{'loss': 4.8603, 'grad_norm': 1.411503553390503, 'learning_rate': 0.00022963466770307034, 'epoch': 0.49}
{'loss': 4.7608, 'grad_norm': 1.5766085386276245, 'learning_rate': 0.0002295180722891566, 'epoch': 0.49}
{'loss': 4.7829, 'grad_norm': 1.6198604106903076, 'learning_rate': 0.0002294014768752429, 'epoch': 0.49}
{'loss': 4.7115, 'grad_norm': 1.6453455686569214, 'learning_rate': 0.00022928488146132918, 'epoch': 0.49}
{'loss': 4.638, 'grad_norm': 1.8512766361236572, 'learning_rate': 0.00022916828604741546, 'epoch': 0.49}
{'loss': 4.5944, 'grad_norm': 1.845365047454834, 'learning_rate': 0.00022905169063350172, 'epoch': 0.5}
{'loss': 4.4913, 'grad_norm': 1.9912258386611938, 'learning_rate': 0.00022893509521958798, 'epoch': 0.5}
{'loss': 4.3629, 'grad_norm': 2.336824417114258, 'learning_rate': 0.0002288184998056743, 'epoch': 0.5}
{'loss': 3.9429, 'grad_norm': 3.1884446144104004, 'learning_rate': 0.00022870190439176056, 'epoch': 0.5}
{'loss': 5.1413, 'grad_norm': 2.6554811000823975, 'learning_rate': 0.00022858530897784687, 'epoch': 0.5}
{'loss': 4.997, 'grad_norm': 1.4846880435943604, 'learning_rate': 0.00022846871356393313, 'epoch': 0.5}
{'loss': 4.943, 'grad_norm': 1.469236135482788, 'learning_rate': 0.00022835211815001942, 'epoch': 0.5}
{'loss': 4.8064, 'grad_norm': 1.3489643335342407, 'learning_rate': 0.0002282355227361057, 'epoch': 0.5}
{'loss': 4.7334, 'grad_norm': 1.4055683612823486, 'learning_rate': 0.00022811892732219196, 'epoch': 0.5}
{'loss': 4.7526, 'grad_norm': 1.5425238609313965, 'learning_rate': 0.00022800233190827825, 'epoch': 0.5}
{'loss': 4.7611, 'grad_norm': 1.7313886880874634, 'learning_rate': 0.00022788573649436454, 'epoch': 0.5}
{'loss': 4.6196, 'grad_norm': 1.9231009483337402, 'learning_rate': 0.00022776914108045082, 'epoch': 0.5}
{'loss': 4.4815, 'grad_norm': 1.9064191579818726, 'learning_rate': 0.00022765254566653708, 'epoch': 0.5}
{'loss': 4.4765, 'grad_norm': 2.0921623706817627, 'learning_rate': 0.0002275359502526234, 'epoch': 0.51}
{'loss': 4.4124, 'grad_norm': 2.1134414672851562, 'learning_rate': 0.00022741935483870966, 'epoch': 0.51}
{'loss': 4.2397, 'grad_norm': 2.6049795150756836, 'learning_rate': 0.00022730275942479592, 'epoch': 0.51}
{'loss': 4.4265, 'grad_norm': 4.3435282707214355, 'learning_rate': 0.00022718616401088223, 'epoch': 0.51}
{'loss': 5.1106, 'grad_norm': 1.8050744533538818, 'learning_rate': 0.0002270695685969685, 'epoch': 0.51}
{'loss': 4.8893, 'grad_norm': 1.679690957069397, 'learning_rate': 0.00022695297318305478, 'epoch': 0.51}
{'loss': 4.8516, 'grad_norm': 2.6580934524536133, 'learning_rate': 0.00022683637776914106, 'epoch': 0.51}
{'loss': 4.8165, 'grad_norm': 1.4454456567764282, 'learning_rate': 0.00022671978235522735, 'epoch': 0.51}
{'loss': 4.7808, 'grad_norm': 1.4178006649017334, 'learning_rate': 0.0002266031869413136, 'epoch': 0.51}
{'loss': 4.6759, 'grad_norm': 1.5376601219177246, 'learning_rate': 0.00022648659152739992, 'epoch': 0.51}
{'loss': 4.6776, 'grad_norm': 1.5989307165145874, 'learning_rate': 0.00022636999611348618, 'epoch': 0.51}
{'loss': 4.5433, 'grad_norm': 1.662394404411316, 'learning_rate': 0.00022625340069957244, 'epoch': 0.51}
{'loss': 4.4874, 'grad_norm': 1.9391822814941406, 'learning_rate': 0.00022613680528565876, 'epoch': 0.51}
{'loss': 4.4038, 'grad_norm': 2.112367868423462, 'learning_rate': 0.00022602020987174502, 'epoch': 0.52}
{'loss': 4.3154, 'grad_norm': 2.257070302963257, 'learning_rate': 0.0002259036144578313, 'epoch': 0.52}
{'loss': 3.9136, 'grad_norm': 3.082212209701538, 'learning_rate': 0.0002257870190439176, 'epoch': 0.52}
{'loss': 5.142, 'grad_norm': 2.31136417388916, 'learning_rate': 0.00022567042363000388, 'epoch': 0.52}
{'loss': 4.9618, 'grad_norm': 1.5279662609100342, 'learning_rate': 0.00022555382821609014, 'epoch': 0.52}
{'loss': 4.9371, 'grad_norm': 1.362878680229187, 'learning_rate': 0.00022543723280217642, 'epoch': 0.52}
{'loss': 4.8051, 'grad_norm': 1.4199230670928955, 'learning_rate': 0.0002253206373882627, 'epoch': 0.52}
{'loss': 4.7769, 'grad_norm': 1.4621679782867432, 'learning_rate': 0.00022520404197434897, 'epoch': 0.52}
{'loss': 4.7447, 'grad_norm': 1.533892273902893, 'learning_rate': 0.00022508744656043529, 'epoch': 0.52}
{'loss': 4.6923, 'grad_norm': 1.64174485206604, 'learning_rate': 0.00022497085114652155, 'epoch': 0.52}
{'loss': 4.6959, 'grad_norm': 1.6563681364059448, 'learning_rate': 0.00022485425573260786, 'epoch': 0.52}
{'loss': 4.6007, 'grad_norm': 1.8000582456588745, 'learning_rate': 0.00022473766031869412, 'epoch': 0.52}
{'loss': 4.4867, 'grad_norm': 1.870710015296936, 'learning_rate': 0.00022462106490478038, 'epoch': 0.52}
{'loss': 4.4354, 'grad_norm': 2.2285232543945312, 'learning_rate': 0.00022450446949086667, 'epoch': 0.53}
{'loss': 4.2522, 'grad_norm': 2.2229552268981934, 'learning_rate': 0.00022438787407695295, 'epoch': 0.53}
{'loss': 4.5117, 'grad_norm': 3.097238063812256, 'learning_rate': 0.00022427127866303924, 'epoch': 0.53}
{'loss': 5.1824, 'grad_norm': 1.8184252977371216, 'learning_rate': 0.0002241546832491255, 'epoch': 0.53}
{'loss': 4.8948, 'grad_norm': 1.4734121561050415, 'learning_rate': 0.0002240380878352118, 'epoch': 0.53}
{'loss': 4.8795, 'grad_norm': 1.5289186239242554, 'learning_rate': 0.00022392149242129807, 'epoch': 0.53}
{'loss': 4.7315, 'grad_norm': 1.4427344799041748, 'learning_rate': 0.00022380489700738433, 'epoch': 0.53}
{'loss': 4.7047, 'grad_norm': 1.6275874376296997, 'learning_rate': 0.00022368830159347065, 'epoch': 0.53}
{'loss': 4.5961, 'grad_norm': 1.6969836950302124, 'learning_rate': 0.0002235717061795569, 'epoch': 0.53}
{'loss': 4.6322, 'grad_norm': 1.625726580619812, 'learning_rate': 0.00022345511076564322, 'epoch': 0.53}
{'loss': 4.4692, 'grad_norm': 1.711657166481018, 'learning_rate': 0.00022333851535172948, 'epoch': 0.53}
{'loss': 4.493, 'grad_norm': 1.8499003648757935, 'learning_rate': 0.00022322191993781577, 'epoch': 0.53}
{'loss': 4.5638, 'grad_norm': 2.2173309326171875, 'learning_rate': 0.00022310532452390205, 'epoch': 0.53}
{'loss': 4.2448, 'grad_norm': 2.3132424354553223, 'learning_rate': 0.0002229887291099883, 'epoch': 0.54}
{'loss': 3.7331, 'grad_norm': 2.9978909492492676, 'learning_rate': 0.0002228721336960746, 'epoch': 0.54}
 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 2800/10442 [3:10:29<4:58:13,  2.34s/it][INFO|trainer.py:831] 2024-10-15 05:15:21,944 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 05:15:21,955 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 05:15:21,955 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 05:15:21,955 >>   Batch size = 32
{'eval_loss': 4.690848350524902, 'eval_wer': 0.9620861114250198, 'eval_runtime': 265.9686, 'eval_samples_per_second': 5.982, 'eval_steps_per_second': 0.188, 'epoch': 0.54}
{'loss': 5.0654, 'grad_norm': 2.2827084064483643, 'learning_rate': 0.00022275553828216086, 'epoch': 0.54}
{'loss': 4.9323, 'grad_norm': 1.4721990823745728, 'learning_rate': 0.00022263894286824717, 'epoch': 0.54}
{'loss': 4.792, 'grad_norm': 1.4645546674728394, 'learning_rate': 0.00022252234745433343, 'epoch': 0.54}
{'loss': 4.7874, 'grad_norm': 1.5201622247695923, 'learning_rate': 0.00022240575204041975, 'epoch': 0.54}
{'loss': 4.6505, 'grad_norm': 1.5114779472351074, 'learning_rate': 0.000222289156626506, 'epoch': 0.54}
{'loss': 4.7003, 'grad_norm': 1.7012014389038086, 'learning_rate': 0.00022217256121259227, 'epoch': 0.54}
{'loss': 4.6578, 'grad_norm': 1.7867047786712646, 'learning_rate': 0.00022205596579867858, 'epoch': 0.54}
{'loss': 4.6699, 'grad_norm': 1.6730033159255981, 'learning_rate': 0.00022193937038476484, 'epoch': 0.54}
{'loss': 4.5666, 'grad_norm': 1.8136656284332275, 'learning_rate': 0.00022182277497085113, 'epoch': 0.54}
{'loss': 4.4996, 'grad_norm': 1.867037296295166, 'learning_rate': 0.00022170617955693741, 'epoch': 0.54}
{'loss': 4.4187, 'grad_norm': 2.175818681716919, 'learning_rate': 0.0002215895841430237, 'epoch': 0.54}
{'loss': 4.0977, 'grad_norm': 2.482278823852539, 'learning_rate': 0.00022147298872910996, 'epoch': 0.55}
{'loss': 4.3938, 'grad_norm': 2.5419504642486572, 'learning_rate': 0.00022135639331519622, 'epoch': 0.55}
{'loss': 4.9835, 'grad_norm': 1.5398086309432983, 'learning_rate': 0.00022123979790128254, 'epoch': 0.55}
{'loss': 4.8888, 'grad_norm': 1.3661776781082153, 'learning_rate': 0.0002211232024873688, 'epoch': 0.55}
{'loss': 4.7547, 'grad_norm': 1.410007119178772, 'learning_rate': 0.0002210066070734551, 'epoch': 0.55}
{'loss': 4.6702, 'grad_norm': 1.4638046026229858, 'learning_rate': 0.00022089001165954137, 'epoch': 0.55}
{'loss': 4.6667, 'grad_norm': 1.5795029401779175, 'learning_rate': 0.00022077341624562766, 'epoch': 0.55}
{'loss': 4.5891, 'grad_norm': 1.5859111547470093, 'learning_rate': 0.00022065682083171394, 'epoch': 0.55}
{'loss': 4.5312, 'grad_norm': 1.6598011255264282, 'learning_rate': 0.0002205402254178002, 'epoch': 0.55}
{'loss': 4.4512, 'grad_norm': 1.8164883852005005, 'learning_rate': 0.0002204236300038865, 'epoch': 0.55}
{'loss': 4.5047, 'grad_norm': 2.0549862384796143, 'learning_rate': 0.00022030703458997278, 'epoch': 0.55}
{'loss': 4.3917, 'grad_norm': 2.03794527053833, 'learning_rate': 0.00022019043917605906, 'epoch': 0.55}
{'loss': 4.3343, 'grad_norm': 2.1872849464416504, 'learning_rate': 0.00022007384376214532, 'epoch': 0.55}
{'loss': 3.7578, 'grad_norm': 3.451263427734375, 'learning_rate': 0.00021995724834823164, 'epoch': 0.56}
{'loss': 5.133, 'grad_norm': 2.882836103439331, 'learning_rate': 0.0002198406529343179, 'epoch': 0.56}
{'loss': 4.8765, 'grad_norm': 1.5402880907058716, 'learning_rate': 0.00021972405752040416, 'epoch': 0.56}
{'loss': 4.8949, 'grad_norm': 1.516568660736084, 'learning_rate': 0.00021960746210649047, 'epoch': 0.56}
{'loss': 4.6703, 'grad_norm': 1.3915936946868896, 'learning_rate': 0.00021949086669257673, 'epoch': 0.56}
{'loss': 4.7538, 'grad_norm': 1.5003576278686523, 'learning_rate': 0.00021937427127866302, 'epoch': 0.56}
{'loss': 4.7404, 'grad_norm': 1.5032423734664917, 'learning_rate': 0.0002192576758647493, 'epoch': 0.56}
{'loss': 4.6385, 'grad_norm': 1.6483901739120483, 'learning_rate': 0.0002191410804508356, 'epoch': 0.56}
{'loss': 4.6269, 'grad_norm': 1.6395992040634155, 'learning_rate': 0.00021902448503692185, 'epoch': 0.56}
{'loss': 4.5804, 'grad_norm': 1.8650397062301636, 'learning_rate': 0.00021890788962300816, 'epoch': 0.56}
{'loss': 4.417, 'grad_norm': 1.9705126285552979, 'learning_rate': 0.00021879129420909442, 'epoch': 0.56}
{'loss': 4.3871, 'grad_norm': 2.2525553703308105, 'learning_rate': 0.00021867469879518068, 'epoch': 0.56}
{'loss': 4.0928, 'grad_norm': 2.668546438217163, 'learning_rate': 0.000218558103381267, 'epoch': 0.56}
{'loss': 4.4994, 'grad_norm': 3.0742483139038086, 'learning_rate': 0.00021844150796735326, 'epoch': 0.57}
{'loss': 5.0719, 'grad_norm': 1.786099910736084, 'learning_rate': 0.00021832491255343954, 'epoch': 0.57}
{'loss': 4.864, 'grad_norm': 1.4102673530578613, 'learning_rate': 0.00021820831713952583, 'epoch': 0.57}
{'loss': 4.7083, 'grad_norm': 1.8459502458572388, 'learning_rate': 0.00021809172172561212, 'epoch': 0.57}
{'loss': 4.5835, 'grad_norm': 1.5154160261154175, 'learning_rate': 0.00021797512631169838, 'epoch': 0.57}
{'loss': 4.6786, 'grad_norm': 1.586349606513977, 'learning_rate': 0.00021785853089778466, 'epoch': 0.57}
{'loss': 4.6997, 'grad_norm': 1.5788849592208862, 'learning_rate': 0.00021774193548387095, 'epoch': 0.57}
{'loss': 4.667, 'grad_norm': 1.6355009078979492, 'learning_rate': 0.0002176253400699572, 'epoch': 0.57}
{'loss': 4.4327, 'grad_norm': 1.7207224369049072, 'learning_rate': 0.00021750874465604353, 'epoch': 0.57}
{'loss': 4.4965, 'grad_norm': 1.856113076210022, 'learning_rate': 0.00021739214924212978, 'epoch': 0.57}
{'loss': 4.4417, 'grad_norm': 1.9845619201660156, 'learning_rate': 0.0002172755538282161, 'epoch': 0.57}
{'loss': 4.2221, 'grad_norm': 2.4176907539367676, 'learning_rate': 0.00021715895841430236, 'epoch': 0.57}
{'loss': 3.6663, 'grad_norm': 2.8409910202026367, 'learning_rate': 0.00021704236300038862, 'epoch': 0.57}
{'loss': 4.9721, 'grad_norm': 2.6971325874328613, 'learning_rate': 0.00021692576758647493, 'epoch': 0.58}
{'loss': 5.0171, 'grad_norm': 1.484695315361023, 'learning_rate': 0.0002168091721725612, 'epoch': 0.58}
{'loss': 4.7863, 'grad_norm': 1.561716914176941, 'learning_rate': 0.00021669257675864748, 'epoch': 0.58}
{'loss': 4.9092, 'grad_norm': 1.6916910409927368, 'learning_rate': 0.00021657598134473374, 'epoch': 0.58}
{'loss': 4.6751, 'grad_norm': 1.4963781833648682, 'learning_rate': 0.00021645938593082005, 'epoch': 0.58}
{'loss': 4.6175, 'grad_norm': 1.6358513832092285, 'learning_rate': 0.0002163427905169063, 'epoch': 0.58}
{'loss': 4.6812, 'grad_norm': 1.6453977823257446, 'learning_rate': 0.00021622619510299257, 'epoch': 0.58}
{'loss': 4.4102, 'grad_norm': 1.7447590827941895, 'learning_rate': 0.00021610959968907889, 'epoch': 0.58}
{'loss': 4.3661, 'grad_norm': 1.8154140710830688, 'learning_rate': 0.00021599300427516515, 'epoch': 0.58}
{'loss': 4.4207, 'grad_norm': 2.0389139652252197, 'learning_rate': 0.00021587640886125146, 'epoch': 0.58}
{'loss': 4.2386, 'grad_norm': 2.1251742839813232, 'learning_rate': 0.00021575981344733772, 'epoch': 0.58}
{'loss': 4.0573, 'grad_norm': 2.5064101219177246, 'learning_rate': 0.000215643218033424, 'epoch': 0.58}
{'loss': 4.4745, 'grad_norm': 2.51865553855896, 'learning_rate': 0.0002155266226195103, 'epoch': 0.58}
{'loss': 4.921, 'grad_norm': 1.5320008993148804, 'learning_rate': 0.00021541002720559655, 'epoch': 0.59}
{'loss': 4.8374, 'grad_norm': 1.4624379873275757, 'learning_rate': 0.00021529343179168284, 'epoch': 0.59}
{'loss': 4.7682, 'grad_norm': 2.8937926292419434, 'learning_rate': 0.0002151768363777691, 'epoch': 0.59}
{'loss': 4.7918, 'grad_norm': 1.4669259786605835, 'learning_rate': 0.00021506024096385541, 'epoch': 0.59}
{'loss': 4.6789, 'grad_norm': 1.5017446279525757, 'learning_rate': 0.00021494364554994167, 'epoch': 0.59}
{'loss': 4.609, 'grad_norm': 1.587101697921753, 'learning_rate': 0.000214827050136028, 'epoch': 0.59}
{'loss': 4.6076, 'grad_norm': 1.6606485843658447, 'learning_rate': 0.00021471045472211425, 'epoch': 0.59}
{'loss': 4.5457, 'grad_norm': 1.7954659461975098, 'learning_rate': 0.0002145938593082005, 'epoch': 0.59}
{'loss': 4.4256, 'grad_norm': 1.9220600128173828, 'learning_rate': 0.00021447726389428682, 'epoch': 0.59}
{'loss': 4.2901, 'grad_norm': 1.970242977142334, 'learning_rate': 0.00021436066848037308, 'epoch': 0.59}
{'loss': 4.3342, 'grad_norm': 2.4433233737945557, 'learning_rate': 0.00021424407306645937, 'epoch': 0.59}
{'loss': 3.6703, 'grad_norm': 2.7809102535247803, 'learning_rate': 0.00021412747765254565, 'epoch': 0.59}
{'loss': 4.9438, 'grad_norm': 1.9794083833694458, 'learning_rate': 0.00021401088223863194, 'epoch': 0.59}
{'loss': 4.7599, 'grad_norm': 1.3688817024230957, 'learning_rate': 0.0002138942868247182, 'epoch': 0.6}
{'loss': 4.8574, 'grad_norm': 1.4631620645523071, 'learning_rate': 0.0002137776914108045, 'epoch': 0.6}
{'loss': 4.6493, 'grad_norm': 1.3725109100341797, 'learning_rate': 0.00021366109599689077, 'epoch': 0.6}
{'loss': 4.6718, 'grad_norm': 1.4668012857437134, 'learning_rate': 0.00021354450058297703, 'epoch': 0.6}
{'loss': 4.8222, 'grad_norm': 1.6583980321884155, 'learning_rate': 0.00021342790516906335, 'epoch': 0.6}
{'loss': 4.6721, 'grad_norm': 1.6505517959594727, 'learning_rate': 0.0002133113097551496, 'epoch': 0.6}
{'loss': 4.5547, 'grad_norm': 1.891646385192871, 'learning_rate': 0.0002131947143412359, 'epoch': 0.6}
{'loss': 4.5751, 'grad_norm': 1.8157830238342285, 'learning_rate': 0.00021307811892732218, 'epoch': 0.6}
{'loss': 4.3071, 'grad_norm': 1.997801423072815, 'learning_rate': 0.00021296152351340844, 'epoch': 0.6}
{'loss': 4.3327, 'grad_norm': 2.117671012878418, 'learning_rate': 0.00021284492809949473, 'epoch': 0.6}
{'loss': 3.9996, 'grad_norm': 2.5498411655426025, 'learning_rate': 0.00021272833268558102, 'epoch': 0.6}
{'loss': 4.3558, 'grad_norm': 2.705174446105957, 'learning_rate': 0.0002126117372716673, 'epoch': 0.6}
{'loss': 4.9056, 'grad_norm': 1.672064185142517, 'learning_rate': 0.00021249514185775356, 'epoch': 0.6}
{'loss': 4.7808, 'grad_norm': 1.4418781995773315, 'learning_rate': 0.00021237854644383988, 'epoch': 0.61}
{'loss': 4.6736, 'grad_norm': 1.4549715518951416, 'learning_rate': 0.00021226195102992614, 'epoch': 0.61}
{'loss': 4.581, 'grad_norm': 1.4871201515197754, 'learning_rate': 0.0002121453556160124, 'epoch': 0.61}
{'loss': 4.5607, 'grad_norm': 1.4430803060531616, 'learning_rate': 0.0002120287602020987, 'epoch': 0.61}
{'loss': 4.5674, 'grad_norm': 1.607324481010437, 'learning_rate': 0.00021191216478818497, 'epoch': 0.61}
{'loss': 4.6383, 'grad_norm': 1.67564058303833, 'learning_rate': 0.00021179556937427126, 'epoch': 0.61}
{'loss': 4.6254, 'grad_norm': 1.744239091873169, 'learning_rate': 0.00021167897396035754, 'epoch': 0.61}
{'loss': 4.4893, 'grad_norm': 1.8522175550460815, 'learning_rate': 0.00021156237854644383, 'epoch': 0.61}
{'loss': 4.3495, 'grad_norm': 2.1135921478271484, 'learning_rate': 0.0002114457831325301, 'epoch': 0.61}
{'loss': 4.0796, 'grad_norm': 2.3709511756896973, 'learning_rate': 0.00021132918771861638, 'epoch': 0.61}
{'loss': 3.5211, 'grad_norm': 2.8644559383392334, 'learning_rate': 0.00021121259230470266, 'epoch': 0.61}
 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                 | 3200/10442 [3:38:14<4:35:19,  2.28s/it][INFO|trainer.py:831] 2024-10-15 05:43:07,155 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 05:43:07,166 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 05:43:07,166 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 05:43:07,166 >>   Batch size = 32
{'eval_loss': 4.59661865234375, 'eval_wer': 0.9799412362978868, 'eval_runtime': 261.9293, 'eval_samples_per_second': 6.074, 'eval_steps_per_second': 0.191, 'epoch': 0.61}
{'loss': 4.9727, 'grad_norm': 2.0851168632507324, 'learning_rate': 0.00021109599689078892, 'epoch': 0.61}
{'loss': 4.8433, 'grad_norm': 1.591428279876709, 'learning_rate': 0.00021097940147687524, 'epoch': 0.61}
{'loss': 4.6865, 'grad_norm': 1.3459699153900146, 'learning_rate': 0.0002108628060629615, 'epoch': 0.62}
{'loss': 4.7991, 'grad_norm': 1.4684333801269531, 'learning_rate': 0.0002107462106490478, 'epoch': 0.62}
{'loss': 4.7661, 'grad_norm': 1.56289803981781, 'learning_rate': 0.00021062961523513407, 'epoch': 0.62}
{'loss': 4.7196, 'grad_norm': 1.6049304008483887, 'learning_rate': 0.00021051301982122036, 'epoch': 0.62}
{'loss': 4.609, 'grad_norm': 1.6809024810791016, 'learning_rate': 0.00021039642440730662, 'epoch': 0.62}
{'loss': 4.5465, 'grad_norm': 1.675143837928772, 'learning_rate': 0.0002102798289933929, 'epoch': 0.62}
{'loss': 4.4144, 'grad_norm': 1.9018217325210571, 'learning_rate': 0.0002101632335794792, 'epoch': 0.62}
{'loss': 4.4728, 'grad_norm': 1.9847958087921143, 'learning_rate': 0.00021004663816556545, 'epoch': 0.62}
{'loss': 4.2368, 'grad_norm': 2.1374359130859375, 'learning_rate': 0.00020993004275165176, 'epoch': 0.62}
{'loss': 4.1156, 'grad_norm': 2.4470460414886475, 'learning_rate': 0.00020981344733773802, 'epoch': 0.62}
{'loss': 4.4971, 'grad_norm': 2.3285396099090576, 'learning_rate': 0.00020969685192382434, 'epoch': 0.62}
{'loss': 4.7966, 'grad_norm': 1.6118969917297363, 'learning_rate': 0.0002095802565099106, 'epoch': 0.62}
{'loss': 4.7607, 'grad_norm': 1.3946038484573364, 'learning_rate': 0.00020946366109599686, 'epoch': 0.62}
{'loss': 4.7606, 'grad_norm': 1.412749171257019, 'learning_rate': 0.00020934706568208317, 'epoch': 0.63}
{'loss': 4.669, 'grad_norm': 1.4464797973632812, 'learning_rate': 0.00020923047026816943, 'epoch': 0.63}
{'loss': 4.5759, 'grad_norm': 1.5615116357803345, 'learning_rate': 0.00020911387485425572, 'epoch': 0.63}
{'loss': 4.6055, 'grad_norm': 1.65269136428833, 'learning_rate': 0.00020899727944034198, 'epoch': 0.63}
{'loss': 4.7165, 'grad_norm': 1.7868613004684448, 'learning_rate': 0.0002088806840264283, 'epoch': 0.63}
{'loss': 4.3993, 'grad_norm': 1.8978573083877563, 'learning_rate': 0.00020876408861251455, 'epoch': 0.63}
{'loss': 4.4675, 'grad_norm': 1.9744869470596313, 'learning_rate': 0.0002086474931986008, 'epoch': 0.63}
{'loss': 4.3146, 'grad_norm': 2.1048338413238525, 'learning_rate': 0.00020853089778468713, 'epoch': 0.63}
{'loss': 4.1978, 'grad_norm': 2.452221632003784, 'learning_rate': 0.00020841430237077339, 'epoch': 0.63}
{'loss': 3.7453, 'grad_norm': 2.8831448554992676, 'learning_rate': 0.0002082977069568597, 'epoch': 0.63}
{'loss': 5.0285, 'grad_norm': 2.2862019538879395, 'learning_rate': 0.00020818111154294596, 'epoch': 0.63}
{'loss': 4.7385, 'grad_norm': 1.5524476766586304, 'learning_rate': 0.00020806451612903225, 'epoch': 0.63}
{'loss': 4.6614, 'grad_norm': 1.4747134447097778, 'learning_rate': 0.00020794792071511853, 'epoch': 0.63}
{'loss': 4.7055, 'grad_norm': 1.5409635305404663, 'learning_rate': 0.0002078313253012048, 'epoch': 0.64}
{'loss': 4.597, 'grad_norm': 1.5418858528137207, 'learning_rate': 0.00020771472988729108, 'epoch': 0.64}
{'loss': 4.5582, 'grad_norm': 1.6382724046707153, 'learning_rate': 0.00020759813447337737, 'epoch': 0.64}
{'loss': 4.526, 'grad_norm': 1.6515473127365112, 'learning_rate': 0.00020748153905946365, 'epoch': 0.64}
{'loss': 4.5644, 'grad_norm': 1.775121808052063, 'learning_rate': 0.0002073649436455499, 'epoch': 0.64}
{'loss': 4.3942, 'grad_norm': 1.8560725450515747, 'learning_rate': 0.00020724834823163623, 'epoch': 0.64}
{'loss': 4.3155, 'grad_norm': 1.8595532178878784, 'learning_rate': 0.0002071317528177225, 'epoch': 0.64}
{'loss': 4.3604, 'grad_norm': 2.109891891479492, 'learning_rate': 0.00020701515740380875, 'epoch': 0.64}
{'loss': 3.9714, 'grad_norm': 2.5514934062957764, 'learning_rate': 0.00020689856198989506, 'epoch': 0.64}
{'loss': 4.2945, 'grad_norm': 3.0705299377441406, 'learning_rate': 0.00020678196657598132, 'epoch': 0.64}
{'loss': 4.8941, 'grad_norm': 1.975570559501648, 'learning_rate': 0.0002066653711620676, 'epoch': 0.64}
{'loss': 4.8566, 'grad_norm': 1.535586953163147, 'learning_rate': 0.0002065487757481539, 'epoch': 0.64}
{'loss': 4.7095, 'grad_norm': 1.4152158498764038, 'learning_rate': 0.00020643218033424018, 'epoch': 0.64}
{'loss': 4.6558, 'grad_norm': 1.5149424076080322, 'learning_rate': 0.00020631558492032644, 'epoch': 0.65}
{'loss': 4.6205, 'grad_norm': 1.5047523975372314, 'learning_rate': 0.00020619898950641273, 'epoch': 0.65}
{'loss': 4.5081, 'grad_norm': 1.7383291721343994, 'learning_rate': 0.00020608239409249901, 'epoch': 0.65}
{'loss': 4.5971, 'grad_norm': 1.68534517288208, 'learning_rate': 0.00020596579867858527, 'epoch': 0.65}
{'loss': 4.4596, 'grad_norm': 1.8103439807891846, 'learning_rate': 0.0002058492032646716, 'epoch': 0.65}
{'loss': 4.315, 'grad_norm': 1.981582522392273, 'learning_rate': 0.00020573260785075785, 'epoch': 0.65}
{'loss': 4.4543, 'grad_norm': 2.461876153945923, 'learning_rate': 0.00020561601243684413, 'epoch': 0.65}
{'loss': 4.0902, 'grad_norm': 2.3078649044036865, 'learning_rate': 0.00020549941702293042, 'epoch': 0.65}
{'loss': 3.7582, 'grad_norm': 3.3429129123687744, 'learning_rate': 0.00020538282160901668, 'epoch': 0.65}
{'loss': 4.9536, 'grad_norm': 2.4200868606567383, 'learning_rate': 0.00020526622619510297, 'epoch': 0.65}
{'loss': 4.7678, 'grad_norm': 1.5198825597763062, 'learning_rate': 0.00020514963078118925, 'epoch': 0.65}
{'loss': 4.7092, 'grad_norm': 1.5258222818374634, 'learning_rate': 0.00020503303536727554, 'epoch': 0.65}
{'loss': 4.5975, 'grad_norm': 1.45234215259552, 'learning_rate': 0.0002049164399533618, 'epoch': 0.65}
{'loss': 4.6669, 'grad_norm': 1.6599704027175903, 'learning_rate': 0.00020479984453944812, 'epoch': 0.66}
{'loss': 4.4594, 'grad_norm': 1.615775465965271, 'learning_rate': 0.00020468324912553438, 'epoch': 0.66}
{'loss': 4.5386, 'grad_norm': 1.618388295173645, 'learning_rate': 0.00020456665371162064, 'epoch': 0.66}
{'loss': 4.5234, 'grad_norm': 1.6714777946472168, 'learning_rate': 0.00020445005829770695, 'epoch': 0.66}
{'loss': 4.3608, 'grad_norm': 2.1748158931732178, 'learning_rate': 0.0002043334628837932, 'epoch': 0.66}
{'loss': 4.3634, 'grad_norm': 2.025297164916992, 'learning_rate': 0.0002042168674698795, 'epoch': 0.66}
{'loss': 4.0951, 'grad_norm': 2.022594928741455, 'learning_rate': 0.00020410027205596578, 'epoch': 0.66}
{'loss': 4.0984, 'grad_norm': 2.726317882537842, 'learning_rate': 0.00020398367664205207, 'epoch': 0.66}
{'loss': 4.267, 'grad_norm': 2.6199238300323486, 'learning_rate': 0.00020386708122813833, 'epoch': 0.66}
{'loss': 4.804, 'grad_norm': 1.9612172842025757, 'learning_rate': 0.00020375048581422462, 'epoch': 0.66}
{'loss': 4.7208, 'grad_norm': 1.5101317167282104, 'learning_rate': 0.0002036338904003109, 'epoch': 0.66}
{'loss': 4.7824, 'grad_norm': 1.5115584135055542, 'learning_rate': 0.00020351729498639716, 'epoch': 0.66}
{'loss': 4.7026, 'grad_norm': 1.480732798576355, 'learning_rate': 0.00020340069957248348, 'epoch': 0.66}
{'loss': 4.7096, 'grad_norm': 1.4302833080291748, 'learning_rate': 0.00020328410415856974, 'epoch': 0.67}
{'loss': 4.56, 'grad_norm': 1.7058464288711548, 'learning_rate': 0.00020316750874465605, 'epoch': 0.67}
{'loss': 4.4962, 'grad_norm': 1.6961761713027954, 'learning_rate': 0.0002030509133307423, 'epoch': 0.67}
{'loss': 4.3456, 'grad_norm': 1.810327410697937, 'learning_rate': 0.00020293431791682857, 'epoch': 0.67}
{'loss': 4.3552, 'grad_norm': 2.162203550338745, 'learning_rate': 0.00020281772250291486, 'epoch': 0.67}
{'loss': 4.2594, 'grad_norm': 2.156872034072876, 'learning_rate': 0.00020270112708900114, 'epoch': 0.67}
{'loss': 4.0909, 'grad_norm': 2.495281457901001, 'learning_rate': 0.00020258453167508743, 'epoch': 0.67}
{'loss': 3.7292, 'grad_norm': 2.7648744583129883, 'learning_rate': 0.0002024679362611737, 'epoch': 0.67}
{'loss': 4.8355, 'grad_norm': 2.059389591217041, 'learning_rate': 0.00020235134084726, 'epoch': 0.67}
{'loss': 4.6346, 'grad_norm': 1.445589542388916, 'learning_rate': 0.00020223474543334626, 'epoch': 0.67}
{'loss': 4.6331, 'grad_norm': 1.4094969034194946, 'learning_rate': 0.00020211815001943258, 'epoch': 0.67}
{'loss': 4.5504, 'grad_norm': 1.5051093101501465, 'learning_rate': 0.00020200155460551884, 'epoch': 0.67}
{'loss': 4.5971, 'grad_norm': 1.608812689781189, 'learning_rate': 0.0002018849591916051, 'epoch': 0.67}
{'loss': 4.498, 'grad_norm': 1.6562175750732422, 'learning_rate': 0.0002017683637776914, 'epoch': 0.67}
{'loss': 4.3934, 'grad_norm': 1.7712894678115845, 'learning_rate': 0.00020165176836377767, 'epoch': 0.68}
{'loss': 4.4394, 'grad_norm': 1.71323823928833, 'learning_rate': 0.00020153517294986396, 'epoch': 0.68}
{'loss': 4.3448, 'grad_norm': 1.843369722366333, 'learning_rate': 0.00020141857753595024, 'epoch': 0.68}
{'loss': 4.3472, 'grad_norm': 2.2481696605682373, 'learning_rate': 0.00020130198212203653, 'epoch': 0.68}
{'loss': 4.0692, 'grad_norm': 2.306165933609009, 'learning_rate': 0.0002011853867081228, 'epoch': 0.68}
{'loss': 3.9726, 'grad_norm': 2.593479871749878, 'learning_rate': 0.00020106879129420905, 'epoch': 0.68}
{'loss': 4.11, 'grad_norm': 2.695082426071167, 'learning_rate': 0.00020095219588029537, 'epoch': 0.68}
{'loss': 4.9337, 'grad_norm': 1.8384729623794556, 'learning_rate': 0.00020083560046638162, 'epoch': 0.68}
{'loss': 4.8084, 'grad_norm': 1.5017752647399902, 'learning_rate': 0.00020071900505246794, 'epoch': 0.68}
{'loss': 4.6801, 'grad_norm': 1.4549552202224731, 'learning_rate': 0.0002006024096385542, 'epoch': 0.68}
{'loss': 4.5332, 'grad_norm': 1.52511727809906, 'learning_rate': 0.00020048581422464049, 'epoch': 0.68}
{'loss': 4.6807, 'grad_norm': 1.6760635375976562, 'learning_rate': 0.00020036921881072677, 'epoch': 0.68}
{'loss': 4.5926, 'grad_norm': 1.6860066652297974, 'learning_rate': 0.00020025262339681303, 'epoch': 0.68}
{'loss': 4.4866, 'grad_norm': 1.8291716575622559, 'learning_rate': 0.00020013602798289932, 'epoch': 0.69}
{'loss': 4.4956, 'grad_norm': 1.7518877983093262, 'learning_rate': 0.0002000194325689856, 'epoch': 0.69}
{'loss': 4.323, 'grad_norm': 1.9368900060653687, 'learning_rate': 0.0001999028371550719, 'epoch': 0.69}
{'loss': 4.2661, 'grad_norm': 1.9725700616836548, 'learning_rate': 0.00019978624174115815, 'epoch': 0.69}
{'loss': 4.1947, 'grad_norm': 2.2737107276916504, 'learning_rate': 0.00019966964632724447, 'epoch': 0.69}
{'loss': 3.6595, 'grad_norm': 3.073179244995117, 'learning_rate': 0.00019955305091333073, 'epoch': 0.69}
 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 3600/10442 [4:05:13<4:13:27,  2.22s/it][INFO|trainer.py:831] 2024-10-15 06:10:05,982 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 06:10:05,993 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 06:10:05,993 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 06:10:05,993 >>   Batch size = 32
{'eval_loss': 4.513846397399902, 'eval_wer': 0.9703073793649, 'eval_runtime': 271.3162, 'eval_samples_per_second': 5.864, 'eval_steps_per_second': 0.184, 'epoch': 0.69}
{'loss': 4.9935, 'grad_norm': 2.1063926219940186, 'learning_rate': 0.00019943645549941699, 'epoch': 0.69}
{'loss': 4.8415, 'grad_norm': 1.4304391145706177, 'learning_rate': 0.0001993198600855033, 'epoch': 0.69}
{'loss': 4.6923, 'grad_norm': 1.451186180114746, 'learning_rate': 0.00019920326467158956, 'epoch': 0.69}
{'loss': 4.6241, 'grad_norm': 1.526780605316162, 'learning_rate': 0.00019908666925767585, 'epoch': 0.69}
{'loss': 4.6311, 'grad_norm': 1.595216155052185, 'learning_rate': 0.00019897007384376213, 'epoch': 0.69}
{'loss': 4.4443, 'grad_norm': 1.6061962842941284, 'learning_rate': 0.00019885347842984842, 'epoch': 0.69}
{'loss': 4.435, 'grad_norm': 1.6576210260391235, 'learning_rate': 0.00019873688301593468, 'epoch': 0.69}
{'loss': 4.4682, 'grad_norm': 1.7654547691345215, 'learning_rate': 0.00019862028760202097, 'epoch': 0.7}
{'loss': 4.2966, 'grad_norm': 1.8442516326904297, 'learning_rate': 0.00019850369218810725, 'epoch': 0.7}
{'loss': 4.3354, 'grad_norm': 1.9694879055023193, 'learning_rate': 0.0001983870967741935, 'epoch': 0.7}
{'loss': 4.1919, 'grad_norm': 2.1402435302734375, 'learning_rate': 0.00019827050136027983, 'epoch': 0.7}
{'loss': 3.9083, 'grad_norm': 2.6126983165740967, 'learning_rate': 0.0001981539059463661, 'epoch': 0.7}
{'loss': 4.2464, 'grad_norm': 2.03212308883667, 'learning_rate': 0.00019803731053245237, 'epoch': 0.7}
{'loss': 4.8577, 'grad_norm': 1.6547350883483887, 'learning_rate': 0.00019792071511853866, 'epoch': 0.7}
{'loss': 4.7558, 'grad_norm': 1.4176416397094727, 'learning_rate': 0.00019780411970462492, 'epoch': 0.7}
{'loss': 4.6805, 'grad_norm': 1.3378127813339233, 'learning_rate': 0.0001976875242907112, 'epoch': 0.7}
{'loss': 4.6515, 'grad_norm': 1.6008789539337158, 'learning_rate': 0.0001975709288767975, 'epoch': 0.7}
{'loss': 4.4525, 'grad_norm': 1.7437293529510498, 'learning_rate': 0.00019745433346288378, 'epoch': 0.7}
{'loss': 4.4071, 'grad_norm': 1.6524512767791748, 'learning_rate': 0.00019733773804897004, 'epoch': 0.7}
{'loss': 4.5397, 'grad_norm': 1.6374586820602417, 'learning_rate': 0.00019722114263505636, 'epoch': 0.7}
{'loss': 4.2759, 'grad_norm': 1.7448402643203735, 'learning_rate': 0.00019710454722114261, 'epoch': 0.71}
{'loss': 4.4181, 'grad_norm': 2.045414447784424, 'learning_rate': 0.00019698795180722887, 'epoch': 0.71}
{'loss': 4.2229, 'grad_norm': 2.0637576580047607, 'learning_rate': 0.0001968713563933152, 'epoch': 0.71}
{'loss': 4.0549, 'grad_norm': 2.180711269378662, 'learning_rate': 0.00019675476097940145, 'epoch': 0.71}
{'loss': 3.8264, 'grad_norm': 3.828212261199951, 'learning_rate': 0.00019663816556548774, 'epoch': 0.71}
{'loss': 4.8921, 'grad_norm': 1.925801157951355, 'learning_rate': 0.00019652157015157402, 'epoch': 0.71}
{'loss': 4.7539, 'grad_norm': 1.4927529096603394, 'learning_rate': 0.0001964049747376603, 'epoch': 0.71}
{'loss': 4.6766, 'grad_norm': 1.4161890745162964, 'learning_rate': 0.00019628837932374657, 'epoch': 0.71}
{'loss': 4.5886, 'grad_norm': 1.5192798376083374, 'learning_rate': 0.00019617178390983286, 'epoch': 0.71}
{'loss': 4.6733, 'grad_norm': 1.7497460842132568, 'learning_rate': 0.00019605518849591914, 'epoch': 0.71}
{'loss': 4.5524, 'grad_norm': 1.6184535026550293, 'learning_rate': 0.0001959385930820054, 'epoch': 0.71}
{'loss': 4.4661, 'grad_norm': 1.7087328433990479, 'learning_rate': 0.00019582199766809172, 'epoch': 0.71}
{'loss': 4.2967, 'grad_norm': 1.7252357006072998, 'learning_rate': 0.00019570540225417798, 'epoch': 0.71}
{'loss': 4.3792, 'grad_norm': 1.8608061075210571, 'learning_rate': 0.0001955888068402643, 'epoch': 0.72}
{'loss': 4.1697, 'grad_norm': 1.978806734085083, 'learning_rate': 0.00019547221142635055, 'epoch': 0.72}
{'loss': 4.1372, 'grad_norm': 2.213866949081421, 'learning_rate': 0.0001953556160124368, 'epoch': 0.72}
{'loss': 3.8405, 'grad_norm': 2.543262004852295, 'learning_rate': 0.00019523902059852312, 'epoch': 0.72}
{'loss': 4.2008, 'grad_norm': 2.278618335723877, 'learning_rate': 0.00019512242518460938, 'epoch': 0.72}
{'loss': 4.7366, 'grad_norm': 1.6407008171081543, 'learning_rate': 0.00019500582977069567, 'epoch': 0.72}
{'loss': 4.688, 'grad_norm': 1.4470329284667969, 'learning_rate': 0.00019488923435678193, 'epoch': 0.72}
{'loss': 4.5359, 'grad_norm': 1.6475690603256226, 'learning_rate': 0.00019477263894286824, 'epoch': 0.72}
{'loss': 4.53, 'grad_norm': 1.6122756004333496, 'learning_rate': 0.0001946560435289545, 'epoch': 0.72}
{'loss': 4.5659, 'grad_norm': 1.6040579080581665, 'learning_rate': 0.00019453944811504082, 'epoch': 0.72}
{'loss': 4.4809, 'grad_norm': 1.6403939723968506, 'learning_rate': 0.00019442285270112708, 'epoch': 0.72}
{'loss': 4.4312, 'grad_norm': 1.868679165840149, 'learning_rate': 0.00019430625728721334, 'epoch': 0.72}
{'loss': 4.4298, 'grad_norm': 1.7336899042129517, 'learning_rate': 0.00019418966187329965, 'epoch': 0.72}
{'loss': 4.3266, 'grad_norm': 1.8440812826156616, 'learning_rate': 0.0001940730664593859, 'epoch': 0.73}
{'loss': 4.145, 'grad_norm': 2.133647918701172, 'learning_rate': 0.0001939564710454722, 'epoch': 0.73}
{'loss': 4.139, 'grad_norm': 2.4611728191375732, 'learning_rate': 0.00019383987563155848, 'epoch': 0.73}
{'loss': 3.7347, 'grad_norm': 3.2650105953216553, 'learning_rate': 0.00019372328021764477, 'epoch': 0.73}
{'loss': 4.7901, 'grad_norm': 2.1278159618377686, 'learning_rate': 0.00019360668480373103, 'epoch': 0.73}
{'loss': 4.6859, 'grad_norm': 1.501974105834961, 'learning_rate': 0.00019349008938981732, 'epoch': 0.73}
{'loss': 4.6948, 'grad_norm': 1.572512149810791, 'learning_rate': 0.0001933734939759036, 'epoch': 0.73}
{'loss': 4.5821, 'grad_norm': 1.6887775659561157, 'learning_rate': 0.00019325689856198986, 'epoch': 0.73}
{'loss': 4.6489, 'grad_norm': 1.5239620208740234, 'learning_rate': 0.00019314030314807618, 'epoch': 0.73}
{'loss': 4.5066, 'grad_norm': 1.6572192907333374, 'learning_rate': 0.00019302370773416244, 'epoch': 0.73}
{'loss': 4.4404, 'grad_norm': 1.6998412609100342, 'learning_rate': 0.00019290711232024873, 'epoch': 0.73}
{'loss': 4.4423, 'grad_norm': 1.7543222904205322, 'learning_rate': 0.000192790516906335, 'epoch': 0.73}
{'loss': 4.3029, 'grad_norm': 1.9469351768493652, 'learning_rate': 0.00019267392149242127, 'epoch': 0.73}
{'loss': 4.2115, 'grad_norm': 2.0371956825256348, 'learning_rate': 0.00019255732607850756, 'epoch': 0.74}
{'loss': 4.1451, 'grad_norm': 2.177824020385742, 'learning_rate': 0.00019244073066459385, 'epoch': 0.74}
{'loss': 4.0334, 'grad_norm': 2.3906872272491455, 'learning_rate': 0.00019232413525068013, 'epoch': 0.74}
{'loss': 4.106, 'grad_norm': 2.500462293624878, 'learning_rate': 0.0001922075398367664, 'epoch': 0.74}
{'loss': 4.7025, 'grad_norm': 1.4874491691589355, 'learning_rate': 0.0001920909444228527, 'epoch': 0.74}
{'loss': 4.7629, 'grad_norm': 1.4367647171020508, 'learning_rate': 0.00019197434900893897, 'epoch': 0.74}
{'loss': 4.5605, 'grad_norm': 1.5171021223068237, 'learning_rate': 0.00019185775359502523, 'epoch': 0.74}
{'loss': 4.5622, 'grad_norm': 1.4809362888336182, 'learning_rate': 0.00019174115818111154, 'epoch': 0.74}
{'loss': 4.425, 'grad_norm': 1.5450000762939453, 'learning_rate': 0.0001916245627671978, 'epoch': 0.74}
{'loss': 4.4308, 'grad_norm': 1.6420013904571533, 'learning_rate': 0.00019150796735328409, 'epoch': 0.74}
{'loss': 4.4982, 'grad_norm': 1.7667325735092163, 'learning_rate': 0.00019139137193937037, 'epoch': 0.74}
{'loss': 4.5108, 'grad_norm': 1.8267395496368408, 'learning_rate': 0.00019127477652545666, 'epoch': 0.74}
{'loss': 4.3552, 'grad_norm': 1.7906452417373657, 'learning_rate': 0.00019115818111154292, 'epoch': 0.74}
{'loss': 4.3199, 'grad_norm': 2.1912314891815186, 'learning_rate': 0.0001910415856976292, 'epoch': 0.75}
{'loss': 4.1022, 'grad_norm': 2.331362247467041, 'learning_rate': 0.0001909249902837155, 'epoch': 0.75}
{'loss': 3.7609, 'grad_norm': 3.2970967292785645, 'learning_rate': 0.00019080839486980175, 'epoch': 0.75}
{'loss': 4.8387, 'grad_norm': 1.8774232864379883, 'learning_rate': 0.00019069179945588807, 'epoch': 0.75}
{'loss': 4.6503, 'grad_norm': 1.5100914239883423, 'learning_rate': 0.00019057520404197433, 'epoch': 0.75}
{'loss': 4.6706, 'grad_norm': 1.4164388179779053, 'learning_rate': 0.00019045860862806061, 'epoch': 0.75}
{'loss': 4.5247, 'grad_norm': 1.531704068183899, 'learning_rate': 0.0001903420132141469, 'epoch': 0.75}
{'loss': 4.3526, 'grad_norm': 1.6023497581481934, 'learning_rate': 0.00019022541780023316, 'epoch': 0.75}
{'loss': 4.5097, 'grad_norm': 1.6876037120819092, 'learning_rate': 0.00019010882238631945, 'epoch': 0.75}
{'loss': 4.4875, 'grad_norm': 1.7701317071914673, 'learning_rate': 0.00018999222697240573, 'epoch': 0.75}
{'loss': 4.4126, 'grad_norm': 2.7344980239868164, 'learning_rate': 0.00018987563155849202, 'epoch': 0.75}
{'loss': 4.4818, 'grad_norm': 2.018946886062622, 'learning_rate': 0.00018975903614457828, 'epoch': 0.75}
{'loss': 4.1138, 'grad_norm': 2.054769992828369, 'learning_rate': 0.0001896424407306646, 'epoch': 0.75}
{'loss': 4.1388, 'grad_norm': 2.090397834777832, 'learning_rate': 0.00018952584531675085, 'epoch': 0.76}
{'loss': 3.8362, 'grad_norm': 2.9115865230560303, 'learning_rate': 0.00018940924990283711, 'epoch': 0.76}
{'loss': 4.1154, 'grad_norm': 2.053262710571289, 'learning_rate': 0.00018929265448892343, 'epoch': 0.76}
{'loss': 4.7992, 'grad_norm': 1.6363157033920288, 'learning_rate': 0.0001891760590750097, 'epoch': 0.76}
{'loss': 4.7061, 'grad_norm': 1.4315131902694702, 'learning_rate': 0.000189059463661096, 'epoch': 0.76}
{'loss': 4.6367, 'grad_norm': 1.435838222503662, 'learning_rate': 0.00018894286824718226, 'epoch': 0.76}
{'loss': 4.4986, 'grad_norm': 1.5244942903518677, 'learning_rate': 0.00018882627283326855, 'epoch': 0.76}
{'loss': 4.4158, 'grad_norm': 1.5587495565414429, 'learning_rate': 0.0001887096774193548, 'epoch': 0.76}
{'loss': 4.5, 'grad_norm': 1.8408795595169067, 'learning_rate': 0.0001885930820054411, 'epoch': 0.76}
{'loss': 4.2851, 'grad_norm': 1.8346322774887085, 'learning_rate': 0.00018847648659152738, 'epoch': 0.76}
{'loss': 4.397, 'grad_norm': 1.8483234643936157, 'learning_rate': 0.00018835989117761364, 'epoch': 0.76}
{'loss': 4.2877, 'grad_norm': 1.9082447290420532, 'learning_rate': 0.00018824329576369996, 'epoch': 0.76}
{'loss': 4.1193, 'grad_norm': 2.0463669300079346, 'learning_rate': 0.00018812670034978622, 'epoch': 0.76}
{'loss': 3.948, 'grad_norm': 2.5211989879608154, 'learning_rate': 0.00018801010493587253, 'epoch': 0.77}
{'loss': 3.6963, 'grad_norm': 2.874086380004883, 'learning_rate': 0.0001878935095219588, 'epoch': 0.77}
 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 4000/10442 [4:32:59<4:09:06,  2.32s/it][INFO|trainer.py:831] 2024-10-15 06:37:52,673 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 06:37:52,684 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 06:37:52,684 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 06:37:52,684 >>   Batch size = 32
{'eval_loss': 4.441863536834717, 'eval_wer': 0.982992428522997, 'eval_runtime': 280.0893, 'eval_samples_per_second': 5.68, 'eval_steps_per_second': 0.179, 'epoch': 0.77}
{'loss': 4.8303, 'grad_norm': 2.0650413036346436, 'learning_rate': 0.00018777691410804505, 'epoch': 0.77}
{'loss': 4.6776, 'grad_norm': 1.4633501768112183, 'learning_rate': 0.00018766031869413136, 'epoch': 0.77}
{'loss': 4.503, 'grad_norm': 1.519248366355896, 'learning_rate': 0.00018754372328021762, 'epoch': 0.77}
{'loss': 4.5462, 'grad_norm': 1.566638708114624, 'learning_rate': 0.0001874271278663039, 'epoch': 0.77}
{'loss': 4.4312, 'grad_norm': 1.568055272102356, 'learning_rate': 0.0001873105324523902, 'epoch': 0.77}
{'loss': 4.4081, 'grad_norm': 1.66218900680542, 'learning_rate': 0.00018719393703847648, 'epoch': 0.77}
{'loss': 4.4703, 'grad_norm': 1.7190234661102295, 'learning_rate': 0.00018707734162456274, 'epoch': 0.77}
{'loss': 4.3167, 'grad_norm': 1.8198257684707642, 'learning_rate': 0.000186960746210649, 'epoch': 0.77}
{'loss': 4.2354, 'grad_norm': 1.8924492597579956, 'learning_rate': 0.00018684415079673532, 'epoch': 0.77}
{'loss': 4.0787, 'grad_norm': 2.0115084648132324, 'learning_rate': 0.00018672755538282158, 'epoch': 0.77}
{'loss': 4.0057, 'grad_norm': 2.296895742416382, 'learning_rate': 0.0001866109599689079, 'epoch': 0.77}
{'loss': 3.8276, 'grad_norm': 2.5961596965789795, 'learning_rate': 0.00018649436455499415, 'epoch': 0.78}
{'loss': 4.2041, 'grad_norm': 2.351113796234131, 'learning_rate': 0.00018637776914108044, 'epoch': 0.78}
{'loss': 4.7323, 'grad_norm': 1.6506991386413574, 'learning_rate': 0.00018626117372716672, 'epoch': 0.78}
{'loss': 4.6562, 'grad_norm': 1.5080379247665405, 'learning_rate': 0.000186144578313253, 'epoch': 0.78}
{'loss': 4.5506, 'grad_norm': 1.5189645290374756, 'learning_rate': 0.00018602798289933927, 'epoch': 0.78}
{'loss': 4.4917, 'grad_norm': 1.610903024673462, 'learning_rate': 0.00018591138748542556, 'epoch': 0.78}
{'loss': 4.3361, 'grad_norm': 1.5358408689498901, 'learning_rate': 0.00018579479207151184, 'epoch': 0.78}
{'loss': 4.3769, 'grad_norm': 1.7227002382278442, 'learning_rate': 0.0001856781966575981, 'epoch': 0.78}
{'loss': 4.4209, 'grad_norm': 1.7862378358840942, 'learning_rate': 0.00018556160124368442, 'epoch': 0.78}
{'loss': 4.3499, 'grad_norm': 1.960785984992981, 'learning_rate': 0.00018544500582977068, 'epoch': 0.78}
{'loss': 4.3632, 'grad_norm': 1.909866213798523, 'learning_rate': 0.00018532841041585696, 'epoch': 0.78}
{'loss': 4.2501, 'grad_norm': 2.1212852001190186, 'learning_rate': 0.00018521181500194325, 'epoch': 0.78}
{'loss': 3.9818, 'grad_norm': 2.3330347537994385, 'learning_rate': 0.0001850952195880295, 'epoch': 0.78}
{'loss': 3.6176, 'grad_norm': 2.8662941455841064, 'learning_rate': 0.0001849786241741158, 'epoch': 0.79}
{'loss': 4.8267, 'grad_norm': 2.2062385082244873, 'learning_rate': 0.00018486202876020208, 'epoch': 0.79}
{'loss': 4.7237, 'grad_norm': 1.568570613861084, 'learning_rate': 0.00018474543334628837, 'epoch': 0.79}
{'loss': 4.5983, 'grad_norm': 1.5584255456924438, 'learning_rate': 0.00018462883793237463, 'epoch': 0.79}
{'loss': 4.4308, 'grad_norm': 1.5155316591262817, 'learning_rate': 0.00018451224251846095, 'epoch': 0.79}
{'loss': 4.4034, 'grad_norm': 1.5606416463851929, 'learning_rate': 0.0001843956471045472, 'epoch': 0.79}
{'loss': 4.4562, 'grad_norm': 1.6162344217300415, 'learning_rate': 0.00018427905169063347, 'epoch': 0.79}
{'loss': 4.4828, 'grad_norm': 1.6704652309417725, 'learning_rate': 0.00018416245627671978, 'epoch': 0.79}
{'loss': 4.3198, 'grad_norm': 1.7973791360855103, 'learning_rate': 0.00018404586086280604, 'epoch': 0.79}
{'loss': 4.2869, 'grad_norm': 1.8894745111465454, 'learning_rate': 0.00018392926544889233, 'epoch': 0.79}
{'loss': 4.1306, 'grad_norm': 2.091637134552002, 'learning_rate': 0.0001838126700349786, 'epoch': 0.79}
{'loss': 4.1916, 'grad_norm': 2.3965563774108887, 'learning_rate': 0.0001836960746210649, 'epoch': 0.79}
{'loss': 3.945, 'grad_norm': 2.4349215030670166, 'learning_rate': 0.00018357947920715116, 'epoch': 0.79}
{'loss': 4.085, 'grad_norm': 2.5383923053741455, 'learning_rate': 0.00018346288379323745, 'epoch': 0.8}
{'loss': 4.6423, 'grad_norm': 1.6624279022216797, 'learning_rate': 0.00018334628837932373, 'epoch': 0.8}
{'loss': 4.6409, 'grad_norm': 1.42168128490448, 'learning_rate': 0.00018322969296541, 'epoch': 0.8}
{'loss': 4.6611, 'grad_norm': 1.4516080617904663, 'learning_rate': 0.0001831130975514963, 'epoch': 0.8}
{'loss': 4.4806, 'grad_norm': 1.4934312105178833, 'learning_rate': 0.00018299650213758257, 'epoch': 0.8}
{'loss': 4.424, 'grad_norm': 1.689118504524231, 'learning_rate': 0.00018287990672366888, 'epoch': 0.8}
{'loss': 4.4033, 'grad_norm': 1.624145746231079, 'learning_rate': 0.00018276331130975514, 'epoch': 0.8}
{'loss': 4.3858, 'grad_norm': 1.8113826513290405, 'learning_rate': 0.0001826467158958414, 'epoch': 0.8}
{'loss': 4.3751, 'grad_norm': 1.8046603202819824, 'learning_rate': 0.0001825301204819277, 'epoch': 0.8}
{'loss': 4.3495, 'grad_norm': 1.9721428155899048, 'learning_rate': 0.00018241352506801397, 'epoch': 0.8}
{'loss': 4.0301, 'grad_norm': 2.1493756771087646, 'learning_rate': 0.00018229692965410026, 'epoch': 0.8}
{'loss': 4.0836, 'grad_norm': 2.320887804031372, 'learning_rate': 0.00018218033424018652, 'epoch': 0.8}
{'loss': 3.7002, 'grad_norm': 2.9354724884033203, 'learning_rate': 0.00018206373882627283, 'epoch': 0.8}
{'loss': 4.7548, 'grad_norm': 1.633772611618042, 'learning_rate': 0.0001819471434123591, 'epoch': 0.81}
{'loss': 4.6861, 'grad_norm': 1.4316964149475098, 'learning_rate': 0.00018183054799844535, 'epoch': 0.81}
{'loss': 4.6472, 'grad_norm': 1.424685001373291, 'learning_rate': 0.00018171395258453167, 'epoch': 0.81}
{'loss': 4.6429, 'grad_norm': 1.5766490697860718, 'learning_rate': 0.00018159735717061793, 'epoch': 0.81}
{'loss': 4.5862, 'grad_norm': 1.515932559967041, 'learning_rate': 0.00018148076175670424, 'epoch': 0.81}
{'loss': 4.3648, 'grad_norm': 1.567215919494629, 'learning_rate': 0.0001813641663427905, 'epoch': 0.81}
{'loss': 4.4365, 'grad_norm': 1.6495249271392822, 'learning_rate': 0.0001812475709288768, 'epoch': 0.81}
{'loss': 4.3376, 'grad_norm': 1.869789719581604, 'learning_rate': 0.00018113097551496307, 'epoch': 0.81}
{'loss': 4.1381, 'grad_norm': 1.9706463813781738, 'learning_rate': 0.00018101438010104933, 'epoch': 0.81}
{'loss': 4.3638, 'grad_norm': 2.0564346313476562, 'learning_rate': 0.00018089778468713562, 'epoch': 0.81}
{'loss': 4.0822, 'grad_norm': 2.287581443786621, 'learning_rate': 0.00018078118927322188, 'epoch': 0.81}
{'loss': 3.921, 'grad_norm': 2.5081570148468018, 'learning_rate': 0.0001806645938593082, 'epoch': 0.81}
{'loss': 4.0401, 'grad_norm': 2.3985302448272705, 'learning_rate': 0.00018054799844539445, 'epoch': 0.81}
{'loss': 4.6769, 'grad_norm': 1.6585218906402588, 'learning_rate': 0.00018043140303148077, 'epoch': 0.82}
{'loss': 4.6498, 'grad_norm': 1.4811252355575562, 'learning_rate': 0.00018031480761756703, 'epoch': 0.82}
{'loss': 4.5263, 'grad_norm': 1.5070043802261353, 'learning_rate': 0.0001801982122036533, 'epoch': 0.82}
{'loss': 4.5104, 'grad_norm': 1.5043370723724365, 'learning_rate': 0.0001800816167897396, 'epoch': 0.82}
{'loss': 4.4434, 'grad_norm': 1.6798309087753296, 'learning_rate': 0.00017996502137582586, 'epoch': 0.82}
{'loss': 4.485, 'grad_norm': 1.6151679754257202, 'learning_rate': 0.00017984842596191215, 'epoch': 0.82}
{'loss': 4.3272, 'grad_norm': 1.7332783937454224, 'learning_rate': 0.00017973183054799844, 'epoch': 0.82}
{'loss': 4.3593, 'grad_norm': 1.8084228038787842, 'learning_rate': 0.00017961523513408472, 'epoch': 0.82}
{'loss': 4.2343, 'grad_norm': 2.0162644386291504, 'learning_rate': 0.00017949863972017098, 'epoch': 0.82}
{'loss': 4.3343, 'grad_norm': 2.2758989334106445, 'learning_rate': 0.00017938204430625724, 'epoch': 0.82}
{'loss': 4.0938, 'grad_norm': 2.16564679145813, 'learning_rate': 0.00017926544889234356, 'epoch': 0.82}
{'loss': 3.6736, 'grad_norm': 2.833357572555542, 'learning_rate': 0.00017914885347842982, 'epoch': 0.82}
{'loss': 4.7526, 'grad_norm': 1.8929308652877808, 'learning_rate': 0.00017903225806451613, 'epoch': 0.82}
{'loss': 4.5409, 'grad_norm': 1.5272109508514404, 'learning_rate': 0.0001789156626506024, 'epoch': 0.83}
{'loss': 4.5304, 'grad_norm': 1.571535348892212, 'learning_rate': 0.00017879906723668868, 'epoch': 0.83}
{'loss': 4.5346, 'grad_norm': 1.575181007385254, 'learning_rate': 0.00017868247182277496, 'epoch': 0.83}
{'loss': 4.4671, 'grad_norm': 1.6466859579086304, 'learning_rate': 0.00017856587640886122, 'epoch': 0.83}
{'loss': 4.4638, 'grad_norm': 1.6022897958755493, 'learning_rate': 0.0001784492809949475, 'epoch': 0.83}
{'loss': 4.3561, 'grad_norm': 1.6455671787261963, 'learning_rate': 0.0001783326855810338, 'epoch': 0.83}
{'loss': 4.3373, 'grad_norm': 1.8023622035980225, 'learning_rate': 0.00017821609016712008, 'epoch': 0.83}
{'loss': 4.3673, 'grad_norm': 1.9868489503860474, 'learning_rate': 0.00017809949475320634, 'epoch': 0.83}
{'loss': 4.1906, 'grad_norm': 1.9356733560562134, 'learning_rate': 0.00017798289933929266, 'epoch': 0.83}
{'loss': 4.0869, 'grad_norm': 2.2646145820617676, 'learning_rate': 0.00017786630392537892, 'epoch': 0.83}
{'loss': 3.7536, 'grad_norm': 2.387652635574341, 'learning_rate': 0.0001777497085114652, 'epoch': 0.83}
{'loss': 4.0533, 'grad_norm': 2.2240285873413086, 'learning_rate': 0.0001776331130975515, 'epoch': 0.83}
{'loss': 4.5454, 'grad_norm': 1.5651130676269531, 'learning_rate': 0.00017751651768363775, 'epoch': 0.83}
{'loss': 4.5167, 'grad_norm': 1.5807386636734009, 'learning_rate': 0.00017739992226972404, 'epoch': 0.84}
{'loss': 4.4854, 'grad_norm': 1.4796086549758911, 'learning_rate': 0.00017728332685581032, 'epoch': 0.84}
{'loss': 4.4006, 'grad_norm': 1.548602819442749, 'learning_rate': 0.0001771667314418966, 'epoch': 0.84}
{'loss': 4.4595, 'grad_norm': 1.7251477241516113, 'learning_rate': 0.00017705013602798287, 'epoch': 0.84}
{'loss': 4.3864, 'grad_norm': 3.806110143661499, 'learning_rate': 0.00017693354061406919, 'epoch': 0.84}
{'loss': 4.2608, 'grad_norm': 1.8669241666793823, 'learning_rate': 0.00017681694520015544, 'epoch': 0.84}
{'loss': 4.2255, 'grad_norm': 1.929362177848816, 'learning_rate': 0.0001767003497862417, 'epoch': 0.84}
{'loss': 4.1983, 'grad_norm': 1.8774693012237549, 'learning_rate': 0.00017658375437232802, 'epoch': 0.84}
{'loss': 4.1442, 'grad_norm': 2.1444685459136963, 'learning_rate': 0.00017646715895841428, 'epoch': 0.84}
{'loss': 3.9983, 'grad_norm': 2.444084882736206, 'learning_rate': 0.00017635056354450057, 'epoch': 0.84}
{'loss': 3.623, 'grad_norm': 3.242199659347534, 'learning_rate': 0.00017623396813058685, 'epoch': 0.84}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                         | 4400/10442 [5:00:56<3:54:55,  2.33s/it][INFO|trainer.py:831] 2024-10-15 07:05:49,466 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 07:05:49,477 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 07:05:49,477 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 07:05:49,477 >>   Batch size = 32
{'eval_loss': 4.381528854370117, 'eval_wer': 0.9790371793422986, 'eval_runtime': 275.3562, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 0.182, 'epoch': 0.84}
{'loss': 4.6318, 'grad_norm': 1.9949932098388672, 'learning_rate': 0.00017611737271667314, 'epoch': 0.84}
{'loss': 4.6808, 'grad_norm': 1.5025625228881836, 'learning_rate': 0.0001760007773027594, 'epoch': 0.84}
{'loss': 4.4912, 'grad_norm': 1.676224708557129, 'learning_rate': 0.00017588418188884569, 'epoch': 0.85}
{'loss': 4.4341, 'grad_norm': 1.5980348587036133, 'learning_rate': 0.00017576758647493197, 'epoch': 0.85}
{'loss': 4.3878, 'grad_norm': 1.59641432762146, 'learning_rate': 0.00017565099106101823, 'epoch': 0.85}
{'loss': 4.5491, 'grad_norm': 1.5331896543502808, 'learning_rate': 0.00017553439564710455, 'epoch': 0.85}
{'loss': 4.3939, 'grad_norm': 1.7503165006637573, 'learning_rate': 0.0001754178002331908, 'epoch': 0.85}
{'loss': 4.3168, 'grad_norm': 1.6767410039901733, 'learning_rate': 0.00017530120481927712, 'epoch': 0.85}
{'loss': 4.2096, 'grad_norm': 1.8715629577636719, 'learning_rate': 0.00017518460940536338, 'epoch': 0.85}
{'loss': 4.2322, 'grad_norm': 2.0069973468780518, 'learning_rate': 0.00017506801399144964, 'epoch': 0.85}
{'loss': 4.1139, 'grad_norm': 2.1122822761535645, 'learning_rate': 0.00017495141857753595, 'epoch': 0.85}
{'loss': 3.8601, 'grad_norm': 2.4525957107543945, 'learning_rate': 0.0001748348231636222, 'epoch': 0.85}
{'loss': 3.9027, 'grad_norm': 2.538052797317505, 'learning_rate': 0.0001747182277497085, 'epoch': 0.85}
{'loss': 4.684, 'grad_norm': 1.8904438018798828, 'learning_rate': 0.00017460163233579476, 'epoch': 0.85}
{'loss': 4.5032, 'grad_norm': 1.4357470273971558, 'learning_rate': 0.00017448503692188107, 'epoch': 0.85}
{'loss': 4.4093, 'grad_norm': 1.4810140132904053, 'learning_rate': 0.00017436844150796733, 'epoch': 0.86}
{'loss': 4.5006, 'grad_norm': 1.6416925191879272, 'learning_rate': 0.0001742518460940536, 'epoch': 0.86}
{'loss': 4.432, 'grad_norm': 1.5769188404083252, 'learning_rate': 0.0001741352506801399, 'epoch': 0.86}
{'loss': 4.3906, 'grad_norm': 1.830775499343872, 'learning_rate': 0.00017401865526622617, 'epoch': 0.86}
{'loss': 4.268, 'grad_norm': 1.690500259399414, 'learning_rate': 0.00017390205985231248, 'epoch': 0.86}
{'loss': 4.4231, 'grad_norm': 1.907285213470459, 'learning_rate': 0.00017378546443839874, 'epoch': 0.86}
{'loss': 4.2554, 'grad_norm': 2.125793695449829, 'learning_rate': 0.00017366886902448503, 'epoch': 0.86}
{'loss': 4.069, 'grad_norm': 2.07271671295166, 'learning_rate': 0.00017355227361057131, 'epoch': 0.86}
{'loss': 4.0493, 'grad_norm': 2.481571912765503, 'learning_rate': 0.00017343567819665757, 'epoch': 0.86}
{'loss': 3.654, 'grad_norm': 3.2625808715820312, 'learning_rate': 0.00017331908278274386, 'epoch': 0.86}
{'loss': 4.6508, 'grad_norm': 1.8287798166275024, 'learning_rate': 0.00017320248736883012, 'epoch': 0.86}
{'loss': 4.5005, 'grad_norm': 1.5269042253494263, 'learning_rate': 0.00017308589195491643, 'epoch': 0.86}
{'loss': 4.5704, 'grad_norm': 1.4690520763397217, 'learning_rate': 0.0001729692965410027, 'epoch': 0.86}
{'loss': 4.4026, 'grad_norm': 1.646285891532898, 'learning_rate': 0.000172852701127089, 'epoch': 0.86}
{'loss': 4.353, 'grad_norm': 1.7440975904464722, 'learning_rate': 0.00017273610571317527, 'epoch': 0.87}
{'loss': 4.4459, 'grad_norm': 1.789068579673767, 'learning_rate': 0.00017261951029926153, 'epoch': 0.87}
{'loss': 4.2658, 'grad_norm': 1.8487517833709717, 'learning_rate': 0.00017250291488534784, 'epoch': 0.87}
{'loss': 4.291, 'grad_norm': 1.7539674043655396, 'learning_rate': 0.0001723863194714341, 'epoch': 0.87}
{'loss': 4.356, 'grad_norm': 1.93409264087677, 'learning_rate': 0.0001722697240575204, 'epoch': 0.87}
{'loss': 3.9982, 'grad_norm': 1.947670817375183, 'learning_rate': 0.00017215312864360668, 'epoch': 0.87}
{'loss': 4.1222, 'grad_norm': 2.1792802810668945, 'learning_rate': 0.00017203653322969296, 'epoch': 0.87}
{'loss': 3.7743, 'grad_norm': 2.3071181774139404, 'learning_rate': 0.00017191993781577922, 'epoch': 0.87}
{'loss': 4.0548, 'grad_norm': 2.1102752685546875, 'learning_rate': 0.0001718033424018655, 'epoch': 0.87}
{'loss': 4.706, 'grad_norm': 1.6536402702331543, 'learning_rate': 0.0001716867469879518, 'epoch': 0.87}
{'loss': 4.6587, 'grad_norm': 1.546142578125, 'learning_rate': 0.00017157015157403806, 'epoch': 0.87}
{'loss': 4.3527, 'grad_norm': 1.6108020544052124, 'learning_rate': 0.00017145355616012437, 'epoch': 0.87}
{'loss': 4.428, 'grad_norm': 1.6239262819290161, 'learning_rate': 0.00017133696074621063, 'epoch': 0.87}
{'loss': 4.4583, 'grad_norm': 1.662841796875, 'learning_rate': 0.00017122036533229692, 'epoch': 0.88}
{'loss': 4.4171, 'grad_norm': 1.771020770072937, 'learning_rate': 0.0001711037699183832, 'epoch': 0.88}
{'loss': 4.2682, 'grad_norm': 1.682059645652771, 'learning_rate': 0.00017098717450446946, 'epoch': 0.88}
{'loss': 4.3634, 'grad_norm': 1.7978891134262085, 'learning_rate': 0.00017087057909055575, 'epoch': 0.88}
{'loss': 4.1812, 'grad_norm': 1.9875797033309937, 'learning_rate': 0.00017075398367664204, 'epoch': 0.88}
{'loss': 4.1059, 'grad_norm': 1.9751075506210327, 'learning_rate': 0.00017063738826272832, 'epoch': 0.88}
{'loss': 3.9003, 'grad_norm': 2.3302934169769287, 'learning_rate': 0.00017052079284881458, 'epoch': 0.88}
{'loss': 3.3891, 'grad_norm': 3.0558183193206787, 'learning_rate': 0.0001704041974349009, 'epoch': 0.88}
{'loss': 4.7257, 'grad_norm': 1.8238459825515747, 'learning_rate': 0.00017028760202098716, 'epoch': 0.88}
{'loss': 4.6062, 'grad_norm': 1.3594059944152832, 'learning_rate': 0.00017017100660707344, 'epoch': 0.88}
{'loss': 4.5088, 'grad_norm': 1.523729920387268, 'learning_rate': 0.00017005441119315973, 'epoch': 0.88}
{'loss': 4.4195, 'grad_norm': 1.5888493061065674, 'learning_rate': 0.000169937815779246, 'epoch': 0.88}
{'loss': 4.3561, 'grad_norm': 1.5642365217208862, 'learning_rate': 0.00016982122036533228, 'epoch': 0.88}
{'loss': 4.3853, 'grad_norm': 1.6649335622787476, 'learning_rate': 0.00016970462495141856, 'epoch': 0.89}
{'loss': 4.4289, 'grad_norm': 1.6800260543823242, 'learning_rate': 0.00016958802953750485, 'epoch': 0.89}
{'loss': 4.3475, 'grad_norm': 1.8655954599380493, 'learning_rate': 0.0001694714341235911, 'epoch': 0.89}
{'loss': 4.1712, 'grad_norm': 1.8203487396240234, 'learning_rate': 0.00016935483870967742, 'epoch': 0.89}
{'loss': 4.2261, 'grad_norm': 1.9266141653060913, 'learning_rate': 0.00016923824329576368, 'epoch': 0.89}
{'loss': 4.1478, 'grad_norm': 2.2071244716644287, 'learning_rate': 0.00016912164788184994, 'epoch': 0.89}
{'loss': 3.9022, 'grad_norm': 3.19551420211792, 'learning_rate': 0.00016900505246793626, 'epoch': 0.89}
{'loss': 3.9909, 'grad_norm': 1.906566858291626, 'learning_rate': 0.00016888845705402252, 'epoch': 0.89}
{'loss': 4.6272, 'grad_norm': 1.6618874073028564, 'learning_rate': 0.00016877186164010883, 'epoch': 0.89}
{'loss': 4.6384, 'grad_norm': 2.1786561012268066, 'learning_rate': 0.0001686552662261951, 'epoch': 0.89}
{'loss': 4.4523, 'grad_norm': 1.4563336372375488, 'learning_rate': 0.00016853867081228138, 'epoch': 0.89}
{'loss': 4.4777, 'grad_norm': 1.5343389511108398, 'learning_rate': 0.00016842207539836764, 'epoch': 0.89}
{'loss': 4.3135, 'grad_norm': 1.686974287033081, 'learning_rate': 0.00016830547998445393, 'epoch': 0.89}
{'loss': 4.355, 'grad_norm': 1.8701306581497192, 'learning_rate': 0.0001681888845705402, 'epoch': 0.9}
{'loss': 4.3889, 'grad_norm': 1.9608386754989624, 'learning_rate': 0.00016807228915662647, 'epoch': 0.9}
{'loss': 4.3082, 'grad_norm': 1.8099095821380615, 'learning_rate': 0.00016795569374271279, 'epoch': 0.9}
{'loss': 4.1968, 'grad_norm': 1.8498739004135132, 'learning_rate': 0.00016783909832879905, 'epoch': 0.9}
{'loss': 4.0714, 'grad_norm': 2.1810410022735596, 'learning_rate': 0.00016772250291488536, 'epoch': 0.9}
{'loss': 3.8974, 'grad_norm': 19.016685485839844, 'learning_rate': 0.00016760590750097162, 'epoch': 0.9}
{'loss': 3.7767, 'grad_norm': 2.9676051139831543, 'learning_rate': 0.00016748931208705788, 'epoch': 0.9}
{'loss': 4.6503, 'grad_norm': 1.7807267904281616, 'learning_rate': 0.0001673727166731442, 'epoch': 0.9}
{'loss': 4.5206, 'grad_norm': 1.4352248907089233, 'learning_rate': 0.00016725612125923045, 'epoch': 0.9}
{'loss': 4.5438, 'grad_norm': 1.4938637018203735, 'learning_rate': 0.00016713952584531674, 'epoch': 0.9}
{'loss': 4.3609, 'grad_norm': 1.6756618022918701, 'learning_rate': 0.000167022930431403, 'epoch': 0.9}
{'loss': 4.5145, 'grad_norm': 1.720852017402649, 'learning_rate': 0.0001669063350174893, 'epoch': 0.9}
{'loss': 4.2631, 'grad_norm': 1.6407400369644165, 'learning_rate': 0.00016678973960357557, 'epoch': 0.9}
{'loss': 4.1689, 'grad_norm': 1.7663542032241821, 'learning_rate': 0.00016667314418966183, 'epoch': 0.91}
{'loss': 4.3613, 'grad_norm': 2.1205668449401855, 'learning_rate': 0.00016655654877574815, 'epoch': 0.91}
{'loss': 4.1469, 'grad_norm': 2.0540144443511963, 'learning_rate': 0.0001664399533618344, 'epoch': 0.91}
{'loss': 4.1793, 'grad_norm': 2.1837987899780273, 'learning_rate': 0.00016632335794792072, 'epoch': 0.91}
{'loss': 4.031, 'grad_norm': 2.326838731765747, 'learning_rate': 0.00016620676253400698, 'epoch': 0.91}
{'loss': 3.9105, 'grad_norm': 2.4424407482147217, 'learning_rate': 0.00016609016712009327, 'epoch': 0.91}
{'loss': 4.0316, 'grad_norm': 1.95327889919281, 'learning_rate': 0.00016597357170617955, 'epoch': 0.91}
{'loss': 4.6824, 'grad_norm': 1.5341068506240845, 'learning_rate': 0.00016585697629226581, 'epoch': 0.91}
{'loss': 4.531, 'grad_norm': 1.4684289693832397, 'learning_rate': 0.0001657403808783521, 'epoch': 0.91}
{'loss': 4.4997, 'grad_norm': 1.5322718620300293, 'learning_rate': 0.0001656237854644384, 'epoch': 0.91}
{'loss': 4.3796, 'grad_norm': 1.6978089809417725, 'learning_rate': 0.00016550719005052467, 'epoch': 0.91}
{'loss': 4.3729, 'grad_norm': 1.6468091011047363, 'learning_rate': 0.00016539059463661093, 'epoch': 0.91}
{'loss': 4.3533, 'grad_norm': 1.7224202156066895, 'learning_rate': 0.00016527399922269725, 'epoch': 0.91}
{'loss': 4.274, 'grad_norm': 1.7900160551071167, 'learning_rate': 0.0001651574038087835, 'epoch': 0.92}
{'loss': 4.2104, 'grad_norm': 1.775808334350586, 'learning_rate': 0.00016504080839486977, 'epoch': 0.92}
{'loss': 4.2475, 'grad_norm': 1.9752603769302368, 'learning_rate': 0.00016492421298095608, 'epoch': 0.92}
{'loss': 4.0965, 'grad_norm': 2.172807455062866, 'learning_rate': 0.00016480761756704234, 'epoch': 0.92}
{'loss': 3.9252, 'grad_norm': 2.3705155849456787, 'learning_rate': 0.00016469102215312863, 'epoch': 0.92}
{'loss': 3.718, 'grad_norm': 3.6076221466064453, 'learning_rate': 0.00016457442673921491, 'epoch': 0.92}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 4800/10442 [5:28:49<3:34:53,  2.29s/it][INFO|trainer.py:831] 2024-10-15 07:33:42,650 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 07:33:42,661 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 07:33:42,662 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 07:33:42,662 >>   Batch size = 32
{'eval_loss': 4.3192291259765625, 'eval_wer': 0.9827381625042377, 'eval_runtime': 298.2155, 'eval_samples_per_second': 5.335, 'eval_steps_per_second': 0.168, 'epoch': 0.92}
{'loss': 4.6313, 'grad_norm': 1.6560869216918945, 'learning_rate': 0.0001644578313253012, 'epoch': 0.92}
{'loss': 4.5083, 'grad_norm': 1.430556058883667, 'learning_rate': 0.00016434123591138746, 'epoch': 0.92}
{'loss': 4.4348, 'grad_norm': 1.5053086280822754, 'learning_rate': 0.00016422464049747375, 'epoch': 0.92}
{'loss': 4.4211, 'grad_norm': 1.5440788269042969, 'learning_rate': 0.00016410804508356004, 'epoch': 0.92}
{'loss': 4.3452, 'grad_norm': 1.7190284729003906, 'learning_rate': 0.0001639914496696463, 'epoch': 0.92}
{'loss': 4.3017, 'grad_norm': 1.760350227355957, 'learning_rate': 0.0001638748542557326, 'epoch': 0.92}
{'loss': 4.369, 'grad_norm': 1.8441685438156128, 'learning_rate': 0.00016375825884181887, 'epoch': 0.92}
{'loss': 4.2891, 'grad_norm': 1.9749037027359009, 'learning_rate': 0.00016364166342790516, 'epoch': 0.93}
{'loss': 4.0549, 'grad_norm': 1.8025987148284912, 'learning_rate': 0.00016352506801399144, 'epoch': 0.93}
{'loss': 4.1504, 'grad_norm': 2.063948154449463, 'learning_rate': 0.0001634084726000777, 'epoch': 0.93}
{'loss': 3.9871, 'grad_norm': 2.2540132999420166, 'learning_rate': 0.000163291877186164, 'epoch': 0.93}
{'loss': 3.8717, 'grad_norm': 2.637690782546997, 'learning_rate': 0.00016317528177225028, 'epoch': 0.93}
{'loss': 4.0503, 'grad_norm': 1.9414033889770508, 'learning_rate': 0.00016305868635833656, 'epoch': 0.93}
{'loss': 4.5439, 'grad_norm': 1.6477267742156982, 'learning_rate': 0.00016294209094442282, 'epoch': 0.93}
{'loss': 4.6309, 'grad_norm': 1.5539116859436035, 'learning_rate': 0.00016282549553050914, 'epoch': 0.93}
{'loss': 4.3727, 'grad_norm': 1.5497184991836548, 'learning_rate': 0.0001627089001165954, 'epoch': 0.93}
{'loss': 4.5187, 'grad_norm': 1.5655754804611206, 'learning_rate': 0.00016259230470268166, 'epoch': 0.93}
{'loss': 4.388, 'grad_norm': 1.6205333471298218, 'learning_rate': 0.00016247570928876797, 'epoch': 0.93}
{'loss': 4.2398, 'grad_norm': 1.8239336013793945, 'learning_rate': 0.00016235911387485423, 'epoch': 0.93}
{'loss': 4.1676, 'grad_norm': 1.866518259048462, 'learning_rate': 0.00016224251846094052, 'epoch': 0.93}
{'loss': 4.2833, 'grad_norm': 1.8480428457260132, 'learning_rate': 0.0001621259230470268, 'epoch': 0.94}
{'loss': 4.2044, 'grad_norm': 2.022430419921875, 'learning_rate': 0.0001620093276331131, 'epoch': 0.94}
{'loss': 4.1041, 'grad_norm': 1.996707558631897, 'learning_rate': 0.00016189273221919935, 'epoch': 0.94}
{'loss': 4.061, 'grad_norm': 2.3990838527679443, 'learning_rate': 0.00016177613680528566, 'epoch': 0.94}
{'loss': 3.5528, 'grad_norm': 2.784026861190796, 'learning_rate': 0.00016165954139137192, 'epoch': 0.94}
{'loss': 4.6451, 'grad_norm': 1.6837254762649536, 'learning_rate': 0.00016154294597745818, 'epoch': 0.94}
{'loss': 4.3548, 'grad_norm': 1.5713106393814087, 'learning_rate': 0.0001614263505635445, 'epoch': 0.94}
{'loss': 4.5112, 'grad_norm': 1.5708152055740356, 'learning_rate': 0.00016130975514963076, 'epoch': 0.94}
{'loss': 4.2709, 'grad_norm': 1.5706102848052979, 'learning_rate': 0.00016119315973571707, 'epoch': 0.94}
{'loss': 4.4576, 'grad_norm': 1.657933235168457, 'learning_rate': 0.00016107656432180333, 'epoch': 0.94}
{'loss': 4.2183, 'grad_norm': 1.7323756217956543, 'learning_rate': 0.00016095996890788962, 'epoch': 0.94}
{'loss': 4.4272, 'grad_norm': 1.7490859031677246, 'learning_rate': 0.00016084337349397588, 'epoch': 0.94}
{'loss': 4.2289, 'grad_norm': 1.826099157333374, 'learning_rate': 0.00016072677808006216, 'epoch': 0.94}
{'loss': 4.0341, 'grad_norm': 1.9488015174865723, 'learning_rate': 0.00016061018266614845, 'epoch': 0.95}
{'loss': 4.157, 'grad_norm': 1.9487417936325073, 'learning_rate': 0.0001604935872522347, 'epoch': 0.95}
{'loss': 4.0091, 'grad_norm': 2.1770992279052734, 'learning_rate': 0.00016037699183832103, 'epoch': 0.95}
{'loss': 3.7275, 'grad_norm': 2.499249219894409, 'learning_rate': 0.00016026039642440728, 'epoch': 0.95}
{'loss': 4.0661, 'grad_norm': 1.8967149257659912, 'learning_rate': 0.0001601438010104936, 'epoch': 0.95}
{'loss': 4.5034, 'grad_norm': 1.5825153589248657, 'learning_rate': 0.00016002720559657986, 'epoch': 0.95}
{'loss': 4.5021, 'grad_norm': 1.539136290550232, 'learning_rate': 0.00015991061018266612, 'epoch': 0.95}
{'loss': 4.4131, 'grad_norm': 1.5701392889022827, 'learning_rate': 0.00015979401476875243, 'epoch': 0.95}
{'loss': 4.4362, 'grad_norm': 1.6890006065368652, 'learning_rate': 0.0001596774193548387, 'epoch': 0.95}
{'loss': 4.3841, 'grad_norm': 1.7608243227005005, 'learning_rate': 0.00015956082394092498, 'epoch': 0.95}
{'loss': 4.2798, 'grad_norm': 1.7666053771972656, 'learning_rate': 0.00015944422852701127, 'epoch': 0.95}
{'loss': 4.3086, 'grad_norm': 1.786628246307373, 'learning_rate': 0.00015932763311309755, 'epoch': 0.95}
{'loss': 4.1992, 'grad_norm': 1.870870590209961, 'learning_rate': 0.0001592110376991838, 'epoch': 0.95}
{'loss': 4.1555, 'grad_norm': 2.143768787384033, 'learning_rate': 0.00015909444228527007, 'epoch': 0.96}
{'loss': 4.0563, 'grad_norm': 2.0887563228607178, 'learning_rate': 0.00015897784687135639, 'epoch': 0.96}
{'loss': 4.0357, 'grad_norm': 2.787642478942871, 'learning_rate': 0.00015886125145744265, 'epoch': 0.96}
{'loss': 3.5434, 'grad_norm': 2.864365577697754, 'learning_rate': 0.00015874465604352896, 'epoch': 0.96}
{'loss': 4.5445, 'grad_norm': 1.6696618795394897, 'learning_rate': 0.00015862806062961522, 'epoch': 0.96}
{'loss': 4.4645, 'grad_norm': 1.5843024253845215, 'learning_rate': 0.0001585114652157015, 'epoch': 0.96}
{'loss': 4.3891, 'grad_norm': 1.469306468963623, 'learning_rate': 0.0001583948698017878, 'epoch': 0.96}
{'loss': 4.3258, 'grad_norm': 1.5596840381622314, 'learning_rate': 0.00015827827438787405, 'epoch': 0.96}
{'loss': 4.3801, 'grad_norm': 1.6181130409240723, 'learning_rate': 0.00015816167897396034, 'epoch': 0.96}
{'loss': 4.3384, 'grad_norm': 1.7992123365402222, 'learning_rate': 0.00015804508356004663, 'epoch': 0.96}
{'loss': 4.3869, 'grad_norm': 1.7401305437088013, 'learning_rate': 0.00015792848814613291, 'epoch': 0.96}
{'loss': 4.2446, 'grad_norm': 1.7700166702270508, 'learning_rate': 0.00015781189273221917, 'epoch': 0.96}
{'loss': 4.1485, 'grad_norm': 1.873350739479065, 'learning_rate': 0.0001576952973183055, 'epoch': 0.96}
{'loss': 4.1458, 'grad_norm': 2.0858211517333984, 'learning_rate': 0.00015757870190439175, 'epoch': 0.97}
{'loss': 4.0685, 'grad_norm': 2.206693649291992, 'learning_rate': 0.000157462106490478, 'epoch': 0.97}
{'loss': 3.8434, 'grad_norm': 2.743096113204956, 'learning_rate': 0.00015734551107656432, 'epoch': 0.97}
{'loss': 4.0413, 'grad_norm': 2.059269428253174, 'learning_rate': 0.00015722891566265058, 'epoch': 0.97}
{'loss': 4.5795, 'grad_norm': 1.5674453973770142, 'learning_rate': 0.00015711232024873687, 'epoch': 0.97}
{'loss': 4.4351, 'grad_norm': 1.5408521890640259, 'learning_rate': 0.00015699572483482315, 'epoch': 0.97}
{'loss': 4.4067, 'grad_norm': 1.5934861898422241, 'learning_rate': 0.00015687912942090944, 'epoch': 0.97}
{'loss': 4.5074, 'grad_norm': 1.536385416984558, 'learning_rate': 0.0001567625340069957, 'epoch': 0.97}
{'loss': 4.3527, 'grad_norm': 1.7498060464859009, 'learning_rate': 0.000156645938593082, 'epoch': 0.97}
{'loss': 4.3948, 'grad_norm': 1.7883440256118774, 'learning_rate': 0.00015652934317916827, 'epoch': 0.97}
{'loss': 4.2497, 'grad_norm': 1.7237837314605713, 'learning_rate': 0.00015641274776525453, 'epoch': 0.97}
{'loss': 4.1815, 'grad_norm': 1.8567750453948975, 'learning_rate': 0.00015629615235134085, 'epoch': 0.97}
{'loss': 4.1739, 'grad_norm': 2.007397174835205, 'learning_rate': 0.0001561795569374271, 'epoch': 0.97}
{'loss': 4.1236, 'grad_norm': 2.110457420349121, 'learning_rate': 0.0001560629615235134, 'epoch': 0.98}
{'loss': 4.0344, 'grad_norm': 2.4410345554351807, 'learning_rate': 0.00015594636610959968, 'epoch': 0.98}
{'loss': 3.5692, 'grad_norm': 3.3332769870758057, 'learning_rate': 0.00015582977069568594, 'epoch': 0.98}
{'loss': 4.7758, 'grad_norm': 1.969770073890686, 'learning_rate': 0.00015571317528177223, 'epoch': 0.98}
{'loss': 4.4683, 'grad_norm': 1.4857555627822876, 'learning_rate': 0.00015559657986785852, 'epoch': 0.98}
{'loss': 4.4593, 'grad_norm': 1.6104381084442139, 'learning_rate': 0.0001554799844539448, 'epoch': 0.98}
{'loss': 4.3648, 'grad_norm': 1.539170503616333, 'learning_rate': 0.00015536338904003106, 'epoch': 0.98}
{'loss': 4.4086, 'grad_norm': 1.7008953094482422, 'learning_rate': 0.00015524679362611738, 'epoch': 0.98}
{'loss': 4.2639, 'grad_norm': 1.7313963174819946, 'learning_rate': 0.00015513019821220364, 'epoch': 0.98}
{'loss': 4.3996, 'grad_norm': 1.7780981063842773, 'learning_rate': 0.0001550136027982899, 'epoch': 0.98}
{'loss': 4.2494, 'grad_norm': 1.8953238725662231, 'learning_rate': 0.0001548970073843762, 'epoch': 0.98}
{'loss': 4.336, 'grad_norm': 2.018019199371338, 'learning_rate': 0.00015478041197046247, 'epoch': 0.98}
{'loss': 4.0982, 'grad_norm': 2.059943437576294, 'learning_rate': 0.00015466381655654876, 'epoch': 0.98}
{'loss': 4.1511, 'grad_norm': 2.2904415130615234, 'learning_rate': 0.00015454722114263504, 'epoch': 0.99}
{'loss': 3.9207, 'grad_norm': 2.6848487854003906, 'learning_rate': 0.00015443062572872133, 'epoch': 0.99}
{'loss': 3.8732, 'grad_norm': 1.8137716054916382, 'learning_rate': 0.0001543140303148076, 'epoch': 0.99}
{'loss': 4.4925, 'grad_norm': 1.523423671722412, 'learning_rate': 0.00015419743490089388, 'epoch': 0.99}
{'loss': 4.5079, 'grad_norm': 1.5464459657669067, 'learning_rate': 0.00015408083948698016, 'epoch': 0.99}
{'loss': 4.4103, 'grad_norm': 1.5903109312057495, 'learning_rate': 0.00015396424407306642, 'epoch': 0.99}
{'loss': 4.2965, 'grad_norm': 1.6056710481643677, 'learning_rate': 0.00015384764865915274, 'epoch': 0.99}
{'loss': 4.3331, 'grad_norm': 1.7687761783599854, 'learning_rate': 0.000153731053245239, 'epoch': 0.99}
{'loss': 4.4033, 'grad_norm': 2.6099047660827637, 'learning_rate': 0.0001536144578313253, 'epoch': 0.99}
{'loss': 4.2148, 'grad_norm': 1.8517199754714966, 'learning_rate': 0.00015349786241741157, 'epoch': 0.99}
{'loss': 4.1988, 'grad_norm': 1.7920721769332886, 'learning_rate': 0.00015338126700349786, 'epoch': 0.99}
{'loss': 4.2333, 'grad_norm': 2.032548427581787, 'learning_rate': 0.00015326467158958414, 'epoch': 0.99}
{'loss': 4.0686, 'grad_norm': 2.2119524478912354, 'learning_rate': 0.0001531480761756704, 'epoch': 0.99}
{'loss': 3.9162, 'grad_norm': 2.4084532260894775, 'learning_rate': 0.0001530314807617567, 'epoch': 1.0}
{'loss': 3.539, 'grad_norm': 3.1359457969665527, 'learning_rate': 0.00015291488534784295, 'epoch': 1.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 5200/10442 [5:57:13<3:18:58,  2.28s/it][INFO|trainer.py:831] 2024-10-15 08:02:05,732 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 08:02:05,743 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 08:02:05,743 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 08:02:05,743 >>   Batch size = 32
{'eval_loss': 4.272979736328125, 'eval_wer': 1.0052265792744943, 'eval_runtime': 303.0104, 'eval_samples_per_second': 5.251, 'eval_steps_per_second': 0.165, 'epoch': 1.0}
{'loss': 4.4629, 'grad_norm': 1.6794260740280151, 'learning_rate': 0.00015279828993392926, 'epoch': 1.0}
{'loss': 4.4679, 'grad_norm': 1.7822436094284058, 'learning_rate': 0.00015268169452001552, 'epoch': 1.0}
{'loss': 4.3093, 'grad_norm': 1.9628989696502686, 'learning_rate': 0.00015256509910610184, 'epoch': 1.0}
{'loss': 4.1725, 'grad_norm': 2.3885011672973633, 'learning_rate': 0.0001524485036921881, 'epoch': 1.0}
{'loss': 3.8392, 'grad_norm': 2.718369245529175, 'learning_rate': 0.00015233190827827436, 'epoch': 1.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 5221/10442 [6:03:21<1:27:02,  1.00s/it][INFO|trainer.py:3705] 2024-10-15 08:08:14,512 >> Saving model checkpoint to ./seq2seq_wav2vec2_bart-base/training/checkpoint-5221
[INFO|configuration_utils.py:410] 2024-10-15 08:08:14,545 >> Configuration saved in ./seq2seq_wav2vec2_bart-base/training/checkpoint-5221/config.json
[INFO|configuration_utils.py:868] 2024-10-15 08:08:14,571 >> Configuration saved in ./seq2seq_wav2vec2_bart-base/training/checkpoint-5221/generation_config.json
[INFO|modeling_utils.py:2836] 2024-10-15 08:08:22,941 >> Model weights saved in ./seq2seq_wav2vec2_bart-base/training/checkpoint-5221/model.safetensors
[INFO|feature_extraction_utils.py:435] 2024-10-15 08:08:22,966 >> Feature extractor saved in ./seq2seq_wav2vec2_bart-base/training/checkpoint-5221/preprocessor_config.json
/auto/brno2/home/xhorni20/dp_mit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 4.0075, 'grad_norm': 1.5259162187576294, 'learning_rate': 0.00015221531286436067, 'epoch': 1.0}
{'loss': 4.3928, 'grad_norm': 1.4408371448516846, 'learning_rate': 0.00015209871745044693, 'epoch': 1.0}
{'loss': 4.3478, 'grad_norm': 1.5950543880462646, 'learning_rate': 0.00015198212203653322, 'epoch': 1.0}
{'loss': 4.3249, 'grad_norm': 1.6852023601531982, 'learning_rate': 0.0001518655266226195, 'epoch': 1.0}
{'loss': 4.2423, 'grad_norm': 1.556790828704834, 'learning_rate': 0.0001517489312087058, 'epoch': 1.0}
{'loss': 4.2566, 'grad_norm': 1.782751441001892, 'learning_rate': 0.00015163233579479205, 'epoch': 1.0}
{'loss': 4.2727, 'grad_norm': 1.8020066022872925, 'learning_rate': 0.00015151574038087834, 'epoch': 1.01}
{'loss': 4.0467, 'grad_norm': 1.8986425399780273, 'learning_rate': 0.00015139914496696463, 'epoch': 1.01}
{'loss': 3.9881, 'grad_norm': 1.9929800033569336, 'learning_rate': 0.00015128254955305089, 'epoch': 1.01}
{'loss': 3.9497, 'grad_norm': 2.0168466567993164, 'learning_rate': 0.0001511659541391372, 'epoch': 1.01}
{'loss': 3.9799, 'grad_norm': 2.2606379985809326, 'learning_rate': 0.00015104935872522346, 'epoch': 1.01}
{'loss': 3.8794, 'grad_norm': 2.654759168624878, 'learning_rate': 0.00015093276331130975, 'epoch': 1.01}
{'loss': 3.6551, 'grad_norm': 2.0424537658691406, 'learning_rate': 0.00015081616789739603, 'epoch': 1.01}
{'loss': 4.4666, 'grad_norm': 1.685103416442871, 'learning_rate': 0.0001506995724834823, 'epoch': 1.01}
{'loss': 4.3655, 'grad_norm': 1.5475140810012817, 'learning_rate': 0.00015058297706956858, 'epoch': 1.01}
{'loss': 4.2769, 'grad_norm': 1.6635308265686035, 'learning_rate': 0.00015046638165565487, 'epoch': 1.01}
{'loss': 4.3114, 'grad_norm': 1.6257095336914062, 'learning_rate': 0.00015034978624174115, 'epoch': 1.01}
{'loss': 4.23, 'grad_norm': 1.6862049102783203, 'learning_rate': 0.0001502331908278274, 'epoch': 1.01}
{'loss': 4.2905, 'grad_norm': 1.7660572528839111, 'learning_rate': 0.00015011659541391373, 'epoch': 1.01}
{'loss': 4.2106, 'grad_norm': 1.8716753721237183, 'learning_rate': 0.00015, 'epoch': 1.02}
{'loss': 3.9944, 'grad_norm': 1.8442671298980713, 'learning_rate': 0.00014988340458608627, 'epoch': 1.02}
{'loss': 4.1485, 'grad_norm': 2.062591314315796, 'learning_rate': 0.00014976680917217256, 'epoch': 1.02}
{'loss': 3.9265, 'grad_norm': 2.208866834640503, 'learning_rate': 0.00014965021375825885, 'epoch': 1.02}
{'loss': 3.8496, 'grad_norm': 2.457918405532837, 'learning_rate': 0.0001495336183443451, 'epoch': 1.02}
{'loss': 3.5402, 'grad_norm': 2.634308338165283, 'learning_rate': 0.0001494170229304314, 'epoch': 1.02}
{'loss': 4.122, 'grad_norm': 2.08378005027771, 'learning_rate': 0.00014930042751651768, 'epoch': 1.02}
{'loss': 4.4828, 'grad_norm': 1.5276260375976562, 'learning_rate': 0.00014918383210260394, 'epoch': 1.02}
{'loss': 4.3399, 'grad_norm': 1.5603841543197632, 'learning_rate': 0.00014906723668869023, 'epoch': 1.02}
{'loss': 4.3187, 'grad_norm': 1.7005029916763306, 'learning_rate': 0.00014895064127477651, 'epoch': 1.02}
{'loss': 4.2341, 'grad_norm': 1.788527011871338, 'learning_rate': 0.0001488340458608628, 'epoch': 1.02}
{'loss': 4.2979, 'grad_norm': 1.6807453632354736, 'learning_rate': 0.00014871745044694906, 'epoch': 1.02}
{'loss': 4.1226, 'grad_norm': 1.8181837797164917, 'learning_rate': 0.00014860085503303535, 'epoch': 1.02}
{'loss': 4.1164, 'grad_norm': 1.8591569662094116, 'learning_rate': 0.00014848425961912163, 'epoch': 1.03}
{'loss': 4.0615, 'grad_norm': 1.9454535245895386, 'learning_rate': 0.00014836766420520792, 'epoch': 1.03}
{'loss': 3.94, 'grad_norm': 2.0492231845855713, 'learning_rate': 0.0001482510687912942, 'epoch': 1.03}
{'loss': 3.9626, 'grad_norm': 2.4047415256500244, 'learning_rate': 0.00014813447337738047, 'epoch': 1.03}
{'loss': 3.7844, 'grad_norm': 2.457636594772339, 'learning_rate': 0.00014801787796346676, 'epoch': 1.03}
{'loss': 3.6589, 'grad_norm': 2.0475006103515625, 'learning_rate': 0.00014790128254955304, 'epoch': 1.03}
{'loss': 4.5047, 'grad_norm': 1.6639782190322876, 'learning_rate': 0.0001477846871356393, 'epoch': 1.03}
{'loss': 4.404, 'grad_norm': 1.5617278814315796, 'learning_rate': 0.0001476680917217256, 'epoch': 1.03}
{'loss': 4.3243, 'grad_norm': 1.5889496803283691, 'learning_rate': 0.00014755149630781188, 'epoch': 1.03}
{'loss': 4.3296, 'grad_norm': 1.726300835609436, 'learning_rate': 0.00014743490089389816, 'epoch': 1.03}
{'loss': 4.3934, 'grad_norm': 1.7833986282348633, 'learning_rate': 0.00014731830547998445, 'epoch': 1.03}
{'loss': 4.1084, 'grad_norm': 1.8671224117279053, 'learning_rate': 0.00014720171006607074, 'epoch': 1.03}
{'loss': 4.0438, 'grad_norm': 1.8358476161956787, 'learning_rate': 0.00014708511465215702, 'epoch': 1.03}
{'loss': 4.0194, 'grad_norm': 1.9663447141647339, 'learning_rate': 0.00014696851923824328, 'epoch': 1.04}
{'loss': 4.0458, 'grad_norm': 1.9930564165115356, 'learning_rate': 0.00014685192382432957, 'epoch': 1.04}
{'loss': 3.9801, 'grad_norm': 2.180687189102173, 'learning_rate': 0.00014673532841041583, 'epoch': 1.04}
{'loss': 3.6669, 'grad_norm': 2.2879576683044434, 'learning_rate': 0.00014661873299650212, 'epoch': 1.04}
{'loss': 3.574, 'grad_norm': 3.1202433109283447, 'learning_rate': 0.0001465021375825884, 'epoch': 1.04}
{'loss': 4.0575, 'grad_norm': 1.7798298597335815, 'learning_rate': 0.0001463855421686747, 'epoch': 1.04}
{'loss': 4.4915, 'grad_norm': 1.4980396032333374, 'learning_rate': 0.00014626894675476098, 'epoch': 1.04}
{'loss': 4.3737, 'grad_norm': 1.6586276292800903, 'learning_rate': 0.00014615235134084724, 'epoch': 1.04}
{'loss': 4.248, 'grad_norm': 1.5973539352416992, 'learning_rate': 0.00014603575592693352, 'epoch': 1.04}
{'loss': 4.1127, 'grad_norm': 1.72063410282135, 'learning_rate': 0.0001459191605130198, 'epoch': 1.04}
{'loss': 4.2112, 'grad_norm': 1.8189997673034668, 'learning_rate': 0.0001458025650991061, 'epoch': 1.04}
{'loss': 4.113, 'grad_norm': 2.0220463275909424, 'learning_rate': 0.00014568596968519238, 'epoch': 1.04}
{'loss': 4.2147, 'grad_norm': 1.9103621244430542, 'learning_rate': 0.00014556937427127864, 'epoch': 1.04}
{'loss': 4.0767, 'grad_norm': 1.8968236446380615, 'learning_rate': 0.00014545277885736493, 'epoch': 1.05}
{'loss': 3.9413, 'grad_norm': 2.2425730228424072, 'learning_rate': 0.00014533618344345122, 'epoch': 1.05}
{'loss': 3.7785, 'grad_norm': 2.1536202430725098, 'learning_rate': 0.00014521958802953748, 'epoch': 1.05}
{'loss': 3.7427, 'grad_norm': 2.454573392868042, 'learning_rate': 0.00014510299261562376, 'epoch': 1.05}
{'loss': 3.623, 'grad_norm': 1.9503475427627563, 'learning_rate': 0.00014498639720171005, 'epoch': 1.05}
{'loss': 4.4323, 'grad_norm': 1.6950831413269043, 'learning_rate': 0.00014486980178779634, 'epoch': 1.05}
{'loss': 4.3467, 'grad_norm': 1.5633642673492432, 'learning_rate': 0.00014475320637388262, 'epoch': 1.05}
{'loss': 4.2406, 'grad_norm': 1.5659133195877075, 'learning_rate': 0.0001446366109599689, 'epoch': 1.05}
{'loss': 4.2581, 'grad_norm': 1.644545078277588, 'learning_rate': 0.00014452001554605517, 'epoch': 1.05}
{'loss': 4.2584, 'grad_norm': 1.805965781211853, 'learning_rate': 0.00014440342013214146, 'epoch': 1.05}
{'loss': 4.2061, 'grad_norm': 1.9448295831680298, 'learning_rate': 0.00014428682471822775, 'epoch': 1.05}
{'loss': 4.1558, 'grad_norm': 1.8498563766479492, 'learning_rate': 0.000144170229304314, 'epoch': 1.05}
{'loss': 4.1013, 'grad_norm': 2.088951587677002, 'learning_rate': 0.0001440536338904003, 'epoch': 1.05}
{'loss': 4.1163, 'grad_norm': 1.9866459369659424, 'learning_rate': 0.00014393703847648658, 'epoch': 1.05}
{'loss': 3.9348, 'grad_norm': 2.437790870666504, 'learning_rate': 0.00014382044306257287, 'epoch': 1.06}
{'loss': 3.8762, 'grad_norm': 2.3497846126556396, 'learning_rate': 0.00014370384764865915, 'epoch': 1.06}
{'loss': 3.5369, 'grad_norm': 2.672252893447876, 'learning_rate': 0.0001435872522347454, 'epoch': 1.06}
{'loss': 4.1775, 'grad_norm': 1.9150373935699463, 'learning_rate': 0.0001434706568208317, 'epoch': 1.06}
{'loss': 4.4658, 'grad_norm': 1.5619667768478394, 'learning_rate': 0.00014335406140691799, 'epoch': 1.06}
{'loss': 4.3202, 'grad_norm': 1.5784671306610107, 'learning_rate': 0.00014323746599300427, 'epoch': 1.06}
{'loss': 4.3006, 'grad_norm': 1.5982637405395508, 'learning_rate': 0.00014312087057909056, 'epoch': 1.06}
{'loss': 4.3507, 'grad_norm': 1.6853413581848145, 'learning_rate': 0.00014300427516517682, 'epoch': 1.06}
{'loss': 4.2006, 'grad_norm': 1.9137835502624512, 'learning_rate': 0.0001428876797512631, 'epoch': 1.06}
{'loss': 4.1623, 'grad_norm': 1.8741494417190552, 'learning_rate': 0.00014277108433734937, 'epoch': 1.06}
{'loss': 3.9968, 'grad_norm': 1.8107240200042725, 'learning_rate': 0.00014265448892343565, 'epoch': 1.06}
{'loss': 4.1175, 'grad_norm': 2.0115973949432373, 'learning_rate': 0.00014253789350952194, 'epoch': 1.06}
{'loss': 4.0929, 'grad_norm': 2.064548969268799, 'learning_rate': 0.00014242129809560823, 'epoch': 1.06}
{'loss': 3.9591, 'grad_norm': 2.34629487991333, 'learning_rate': 0.0001423047026816945, 'epoch': 1.07}
{'loss': 3.8435, 'grad_norm': 2.808549165725708, 'learning_rate': 0.0001421881072677808, 'epoch': 1.07}
{'loss': 3.5898, 'grad_norm': 1.9362335205078125, 'learning_rate': 0.0001420715118538671, 'epoch': 1.07}
{'loss': 4.4094, 'grad_norm': 1.6351231336593628, 'learning_rate': 0.00014195491643995335, 'epoch': 1.07}
{'loss': 4.4279, 'grad_norm': 1.5089894533157349, 'learning_rate': 0.00014183832102603963, 'epoch': 1.07}
{'loss': 4.262, 'grad_norm': 1.5218842029571533, 'learning_rate': 0.00014172172561212592, 'epoch': 1.07}
{'loss': 4.2131, 'grad_norm': 1.7081445455551147, 'learning_rate': 0.00014160513019821218, 'epoch': 1.07}
{'loss': 4.2896, 'grad_norm': 1.750433087348938, 'learning_rate': 0.00014148853478429847, 'epoch': 1.07}
{'loss': 4.1169, 'grad_norm': 1.8676878213882446, 'learning_rate': 0.00014137193937038475, 'epoch': 1.07}
{'loss': 4.1431, 'grad_norm': 1.954969048500061, 'learning_rate': 0.00014125534395647104, 'epoch': 1.07}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 5600/10442 [6:26:25<4:02:52,  3.01s/it][INFO|trainer.py:831] 2024-10-15 08:31:18,467 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 08:31:18,477 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 08:31:18,477 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 08:31:18,477 >>   Batch size = 32
{'eval_loss': 4.2535858154296875, 'eval_wer': 1.0615041247598598, 'eval_runtime': 327.3612, 'eval_samples_per_second': 4.86, 'eval_steps_per_second': 0.153, 'epoch': 1.07}
{'loss': 4.0821, 'grad_norm': 1.9422084093093872, 'learning_rate': 0.0001411387485425573, 'epoch': 1.07}
{'loss': 4.0382, 'grad_norm': 2.022815704345703, 'learning_rate': 0.0001410221531286436, 'epoch': 1.07}
{'loss': 3.8892, 'grad_norm': 2.2647063732147217, 'learning_rate': 0.00014090555771472987, 'epoch': 1.07}
{'loss': 3.8695, 'grad_norm': 2.4946579933166504, 'learning_rate': 0.00014078896230081616, 'epoch': 1.08}
{'loss': 3.6714, 'grad_norm': 2.862736463546753, 'learning_rate': 0.00014067236688690245, 'epoch': 1.08}
{'loss': 4.2255, 'grad_norm': 1.5939669609069824, 'learning_rate': 0.0001405557714729887, 'epoch': 1.08}
{'loss': 4.3138, 'grad_norm': 1.556808590888977, 'learning_rate': 0.000140439176059075, 'epoch': 1.08}
{'loss': 4.3035, 'grad_norm': 1.5901131629943848, 'learning_rate': 0.00014032258064516128, 'epoch': 1.08}
{'loss': 4.3081, 'grad_norm': 1.742311954498291, 'learning_rate': 0.00014020598523124754, 'epoch': 1.08}
{'loss': 4.1011, 'grad_norm': 1.6880115270614624, 'learning_rate': 0.00014008938981733383, 'epoch': 1.08}
{'loss': 4.3198, 'grad_norm': 1.9616161584854126, 'learning_rate': 0.00013997279440342012, 'epoch': 1.08}
{'loss': 4.1823, 'grad_norm': 1.850487470626831, 'learning_rate': 0.0001398561989895064, 'epoch': 1.08}
{'loss': 4.2285, 'grad_norm': 1.928225040435791, 'learning_rate': 0.0001397396035755927, 'epoch': 1.08}
{'loss': 3.9127, 'grad_norm': 2.0098865032196045, 'learning_rate': 0.00013962300816167898, 'epoch': 1.08}
{'loss': 3.9672, 'grad_norm': 2.0422353744506836, 'learning_rate': 0.00013950641274776526, 'epoch': 1.08}
{'loss': 3.847, 'grad_norm': 2.2277393341064453, 'learning_rate': 0.00013938981733385152, 'epoch': 1.08}
{'loss': 3.682, 'grad_norm': 2.514151096343994, 'learning_rate': 0.0001392732219199378, 'epoch': 1.09}
{'loss': 3.5464, 'grad_norm': 2.1466355323791504, 'learning_rate': 0.0001391566265060241, 'epoch': 1.09}
{'loss': 4.4635, 'grad_norm': 1.6545220613479614, 'learning_rate': 0.00013904003109211036, 'epoch': 1.09}
{'loss': 4.3309, 'grad_norm': 1.5294678211212158, 'learning_rate': 0.00013892343567819664, 'epoch': 1.09}
{'loss': 4.2416, 'grad_norm': 1.6467434167861938, 'learning_rate': 0.00013880684026428293, 'epoch': 1.09}
{'loss': 4.2059, 'grad_norm': 1.7983940839767456, 'learning_rate': 0.00013869024485036922, 'epoch': 1.09}
{'loss': 4.3003, 'grad_norm': 1.8203787803649902, 'learning_rate': 0.00013857364943645548, 'epoch': 1.09}
{'loss': 4.2473, 'grad_norm': 1.7817187309265137, 'learning_rate': 0.00013845705402254176, 'epoch': 1.09}
{'loss': 4.1614, 'grad_norm': 1.8885319232940674, 'learning_rate': 0.00013834045860862805, 'epoch': 1.09}
{'loss': 4.0646, 'grad_norm': 1.989087700843811, 'learning_rate': 0.00013822386319471434, 'epoch': 1.09}
{'loss': 4.0995, 'grad_norm': 2.1523144245147705, 'learning_rate': 0.00013810726778080062, 'epoch': 1.09}
{'loss': 3.9915, 'grad_norm': 2.2907910346984863, 'learning_rate': 0.00013799067236688688, 'epoch': 1.09}
{'loss': 3.7334, 'grad_norm': 2.2916152477264404, 'learning_rate': 0.00013787407695297317, 'epoch': 1.09}
{'loss': 3.4099, 'grad_norm': 2.939053773880005, 'learning_rate': 0.00013775748153905946, 'epoch': 1.1}
{'loss': 4.1626, 'grad_norm': 1.725917100906372, 'learning_rate': 0.00013764088612514572, 'epoch': 1.1}
{'loss': 4.3737, 'grad_norm': 1.5061774253845215, 'learning_rate': 0.000137524290711232, 'epoch': 1.1}
{'loss': 4.2971, 'grad_norm': 1.5978245735168457, 'learning_rate': 0.0001374076952973183, 'epoch': 1.1}
{'loss': 4.2406, 'grad_norm': 1.626107096672058, 'learning_rate': 0.00013729109988340458, 'epoch': 1.1}
{'loss': 4.2459, 'grad_norm': 1.8497222661972046, 'learning_rate': 0.00013717450446949086, 'epoch': 1.1}
{'loss': 4.1462, 'grad_norm': 1.7269266843795776, 'learning_rate': 0.00013705790905557715, 'epoch': 1.1}
{'loss': 4.0927, 'grad_norm': 1.8687387704849243, 'learning_rate': 0.0001369413136416634, 'epoch': 1.1}
{'loss': 4.0668, 'grad_norm': 1.9851956367492676, 'learning_rate': 0.0001368247182277497, 'epoch': 1.1}
{'loss': 4.028, 'grad_norm': 1.9795379638671875, 'learning_rate': 0.00013670812281383598, 'epoch': 1.1}
{'loss': 4.0276, 'grad_norm': 2.2369422912597656, 'learning_rate': 0.00013659152739992224, 'epoch': 1.1}
{'loss': 3.9472, 'grad_norm': 2.3451731204986572, 'learning_rate': 0.00013647493198600853, 'epoch': 1.1}
{'loss': 3.6701, 'grad_norm': 2.7590315341949463, 'learning_rate': 0.00013635833657209482, 'epoch': 1.1}
{'loss': 3.6772, 'grad_norm': 1.7322182655334473, 'learning_rate': 0.0001362417411581811, 'epoch': 1.11}
{'loss': 4.4723, 'grad_norm': 1.5527653694152832, 'learning_rate': 0.00013612514574426736, 'epoch': 1.11}
{'loss': 4.3005, 'grad_norm': 1.4959675073623657, 'learning_rate': 0.00013600855033035365, 'epoch': 1.11}
{'loss': 4.2731, 'grad_norm': 1.6313985586166382, 'learning_rate': 0.00013589195491643994, 'epoch': 1.11}
{'loss': 4.2732, 'grad_norm': 1.642504334449768, 'learning_rate': 0.00013577535950252623, 'epoch': 1.11}
{'loss': 4.216, 'grad_norm': 2.144984483718872, 'learning_rate': 0.0001356587640886125, 'epoch': 1.11}
{'loss': 4.1739, 'grad_norm': 1.7860304117202759, 'learning_rate': 0.0001355421686746988, 'epoch': 1.11}
{'loss': 4.1615, 'grad_norm': 1.939741849899292, 'learning_rate': 0.00013542557326078506, 'epoch': 1.11}
{'loss': 4.2917, 'grad_norm': 1.9331077337265015, 'learning_rate': 0.00013530897784687135, 'epoch': 1.11}
{'loss': 3.9638, 'grad_norm': 2.0491373538970947, 'learning_rate': 0.0001351923824329576, 'epoch': 1.11}
{'loss': 3.8804, 'grad_norm': 2.2528367042541504, 'learning_rate': 0.0001350757870190439, 'epoch': 1.11}
{'loss': 3.8366, 'grad_norm': 2.4124855995178223, 'learning_rate': 0.00013495919160513018, 'epoch': 1.11}
{'loss': 3.4002, 'grad_norm': 2.6668105125427246, 'learning_rate': 0.00013484259619121647, 'epoch': 1.11}
{'loss': 4.1075, 'grad_norm': 1.7667644023895264, 'learning_rate': 0.00013472600077730275, 'epoch': 1.12}
{'loss': 4.4316, 'grad_norm': 1.5641305446624756, 'learning_rate': 0.00013460940536338904, 'epoch': 1.12}
{'loss': 4.2951, 'grad_norm': 1.5305068492889404, 'learning_rate': 0.00013449280994947533, 'epoch': 1.12}
{'loss': 4.2798, 'grad_norm': 1.619408130645752, 'learning_rate': 0.00013437621453556159, 'epoch': 1.12}
{'loss': 4.1881, 'grad_norm': 1.7291767597198486, 'learning_rate': 0.00013425961912164787, 'epoch': 1.12}
{'loss': 4.1432, 'grad_norm': 1.7949937582015991, 'learning_rate': 0.00013414302370773416, 'epoch': 1.12}
{'loss': 4.0928, 'grad_norm': 1.9712892770767212, 'learning_rate': 0.00013402642829382042, 'epoch': 1.12}
{'loss': 4.1462, 'grad_norm': 2.041271209716797, 'learning_rate': 0.0001339098328799067, 'epoch': 1.12}
{'loss': 4.1595, 'grad_norm': 2.1578621864318848, 'learning_rate': 0.000133793237465993, 'epoch': 1.12}
{'loss': 3.9231, 'grad_norm': 2.0996744632720947, 'learning_rate': 0.00013367664205207928, 'epoch': 1.12}
{'loss': 3.8449, 'grad_norm': 2.292165756225586, 'learning_rate': 0.00013356004663816554, 'epoch': 1.12}
{'loss': 3.7521, 'grad_norm': 2.705305337905884, 'learning_rate': 0.00013344345122425183, 'epoch': 1.12}
{'loss': 3.7597, 'grad_norm': 1.7415831089019775, 'learning_rate': 0.00013332685581033811, 'epoch': 1.12}
{'loss': 4.4867, 'grad_norm': 1.663926124572754, 'learning_rate': 0.0001332102603964244, 'epoch': 1.13}
{'loss': 4.2301, 'grad_norm': 1.4849621057510376, 'learning_rate': 0.0001330936649825107, 'epoch': 1.13}
{'loss': 4.3736, 'grad_norm': 1.5682839155197144, 'learning_rate': 0.00013297706956859697, 'epoch': 1.13}
{'loss': 4.1663, 'grad_norm': 1.6545453071594238, 'learning_rate': 0.00013286047415468323, 'epoch': 1.13}
{'loss': 4.158, 'grad_norm': 1.7946927547454834, 'learning_rate': 0.00013274387874076952, 'epoch': 1.13}
{'loss': 4.1847, 'grad_norm': 1.7498692274093628, 'learning_rate': 0.00013262728332685578, 'epoch': 1.13}
{'loss': 4.1673, 'grad_norm': 1.846834421157837, 'learning_rate': 0.00013251068791294207, 'epoch': 1.13}
{'loss': 4.0596, 'grad_norm': 1.9373371601104736, 'learning_rate': 0.00013239409249902835, 'epoch': 1.13}
{'loss': 4.035, 'grad_norm': 2.186965227127075, 'learning_rate': 0.00013227749708511464, 'epoch': 1.13}
{'loss': 4.0424, 'grad_norm': 2.183267116546631, 'learning_rate': 0.00013216090167120093, 'epoch': 1.13}
{'loss': 3.7309, 'grad_norm': 2.3621327877044678, 'learning_rate': 0.00013204430625728722, 'epoch': 1.13}
{'loss': 3.4808, 'grad_norm': 2.994753122329712, 'learning_rate': 0.00013192771084337347, 'epoch': 1.13}
{'loss': 4.1796, 'grad_norm': 1.6806342601776123, 'learning_rate': 0.00013181111542945976, 'epoch': 1.13}
{'loss': 4.3677, 'grad_norm': 1.492920160293579, 'learning_rate': 0.00013169452001554605, 'epoch': 1.14}
{'loss': 4.3072, 'grad_norm': 1.5865283012390137, 'learning_rate': 0.00013157792460163234, 'epoch': 1.14}
{'loss': 4.2438, 'grad_norm': 1.6321169137954712, 'learning_rate': 0.0001314613291877186, 'epoch': 1.14}
{'loss': 4.13, 'grad_norm': 1.7148281335830688, 'learning_rate': 0.00013134473377380488, 'epoch': 1.14}
{'loss': 4.0571, 'grad_norm': 1.746475100517273, 'learning_rate': 0.00013122813835989117, 'epoch': 1.14}
{'loss': 4.1305, 'grad_norm': 2.1244606971740723, 'learning_rate': 0.00013111154294597746, 'epoch': 1.14}
{'loss': 3.9915, 'grad_norm': 1.9341225624084473, 'learning_rate': 0.00013099494753206372, 'epoch': 1.14}
{'loss': 4.0327, 'grad_norm': 2.097367525100708, 'learning_rate': 0.00013087835211815, 'epoch': 1.14}
{'loss': 3.9787, 'grad_norm': 2.1627306938171387, 'learning_rate': 0.0001307617567042363, 'epoch': 1.14}
{'loss': 3.8904, 'grad_norm': 2.35762357711792, 'learning_rate': 0.00013064516129032258, 'epoch': 1.14}
{'loss': 3.6464, 'grad_norm': 2.7815229892730713, 'learning_rate': 0.00013052856587640886, 'epoch': 1.14}
{'loss': 3.5159, 'grad_norm': 2.1100568771362305, 'learning_rate': 0.00013041197046249512, 'epoch': 1.14}
{'loss': 4.4193, 'grad_norm': 1.6534510850906372, 'learning_rate': 0.0001302953750485814, 'epoch': 1.14}
{'loss': 4.3334, 'grad_norm': 1.5663022994995117, 'learning_rate': 0.0001301787796346677, 'epoch': 1.15}
{'loss': 4.1961, 'grad_norm': 1.5542763471603394, 'learning_rate': 0.00013006218422075396, 'epoch': 1.15}
{'loss': 4.2547, 'grad_norm': 1.6485196352005005, 'learning_rate': 0.00012994558880684024, 'epoch': 1.15}
{'loss': 4.1142, 'grad_norm': 1.838785171508789, 'learning_rate': 0.00012982899339292653, 'epoch': 1.15}
{'loss': 4.1729, 'grad_norm': 1.7495617866516113, 'learning_rate': 0.00012971239797901282, 'epoch': 1.15}
{'loss': 4.1196, 'grad_norm': 1.9320776462554932, 'learning_rate': 0.0001295958025650991, 'epoch': 1.15}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 6000/10442 [6:55:06<3:42:06,  3.00s/it][INFO|trainer.py:831] 2024-10-15 08:59:59,274 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 08:59:59,285 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 08:59:59,285 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 08:59:59,285 >>   Batch size = 32
{'eval_loss': 4.226154804229736, 'eval_wer': 1.1862074810713075, 'eval_runtime': 339.3215, 'eval_samples_per_second': 4.689, 'eval_steps_per_second': 0.147, 'epoch': 1.15}
{'loss': 3.9714, 'grad_norm': 1.900343656539917, 'learning_rate': 0.0001294792071511854, 'epoch': 1.15}
{'loss': 4.102, 'grad_norm': 2.052755832672119, 'learning_rate': 0.00012936261173727165, 'epoch': 1.15}
{'loss': 3.9334, 'grad_norm': 2.181865930557251, 'learning_rate': 0.00012924601632335794, 'epoch': 1.15}
{'loss': 3.8642, 'grad_norm': 2.5541956424713135, 'learning_rate': 0.00012912942090944422, 'epoch': 1.15}
{'loss': 3.3162, 'grad_norm': 2.799882411956787, 'learning_rate': 0.00012901282549553048, 'epoch': 1.15}
{'loss': 4.1803, 'grad_norm': 1.668380618095398, 'learning_rate': 0.00012889623008161677, 'epoch': 1.15}
{'loss': 4.3326, 'grad_norm': 1.5300568342208862, 'learning_rate': 0.00012877963466770306, 'epoch': 1.15}
{'loss': 4.2666, 'grad_norm': 1.6260710954666138, 'learning_rate': 0.00012866303925378934, 'epoch': 1.16}
{'loss': 4.332, 'grad_norm': 1.515679955482483, 'learning_rate': 0.0001285464438398756, 'epoch': 1.16}
{'loss': 4.2837, 'grad_norm': 1.6859583854675293, 'learning_rate': 0.0001284298484259619, 'epoch': 1.16}
{'loss': 4.1506, 'grad_norm': 1.8302274942398071, 'learning_rate': 0.00012831325301204818, 'epoch': 1.16}
{'loss': 4.0762, 'grad_norm': 1.9757179021835327, 'learning_rate': 0.00012819665759813446, 'epoch': 1.16}
{'loss': 4.0258, 'grad_norm': 1.8808459043502808, 'learning_rate': 0.00012808006218422075, 'epoch': 1.16}
{'loss': 4.0564, 'grad_norm': 1.9572559595108032, 'learning_rate': 0.00012796346677030704, 'epoch': 1.16}
{'loss': 3.9583, 'grad_norm': 2.1124987602233887, 'learning_rate': 0.0001278468713563933, 'epoch': 1.16}
{'loss': 3.7933, 'grad_norm': 2.3250489234924316, 'learning_rate': 0.00012773027594247959, 'epoch': 1.16}
{'loss': 3.7011, 'grad_norm': 2.7780251502990723, 'learning_rate': 0.00012761368052856587, 'epoch': 1.16}
{'loss': 3.4903, 'grad_norm': 1.7633821964263916, 'learning_rate': 0.00012749708511465213, 'epoch': 1.16}
{'loss': 4.3739, 'grad_norm': 1.5731489658355713, 'learning_rate': 0.00012738048970073842, 'epoch': 1.16}
{'loss': 4.3277, 'grad_norm': 1.6112240552902222, 'learning_rate': 0.0001272638942868247, 'epoch': 1.16}
{'loss': 4.268, 'grad_norm': 1.644762635231018, 'learning_rate': 0.000127147298872911, 'epoch': 1.17}
{'loss': 4.1488, 'grad_norm': 1.6983673572540283, 'learning_rate': 0.00012703070345899728, 'epoch': 1.17}
{'loss': 4.0022, 'grad_norm': 1.8832268714904785, 'learning_rate': 0.00012691410804508357, 'epoch': 1.17}
{'loss': 4.1593, 'grad_norm': 1.864402413368225, 'learning_rate': 0.00012679751263116983, 'epoch': 1.17}
{'loss': 4.1512, 'grad_norm': 1.8973692655563354, 'learning_rate': 0.0001266809172172561, 'epoch': 1.17}
{'loss': 4.103, 'grad_norm': 1.906316876411438, 'learning_rate': 0.0001265643218033424, 'epoch': 1.17}
{'loss': 3.9733, 'grad_norm': 2.177980661392212, 'learning_rate': 0.00012644772638942866, 'epoch': 1.17}
{'loss': 3.8939, 'grad_norm': 2.1548149585723877, 'learning_rate': 0.00012633113097551495, 'epoch': 1.17}
{'loss': 3.8315, 'grad_norm': 2.6347057819366455, 'learning_rate': 0.00012621453556160123, 'epoch': 1.17}
{'loss': 3.6185, 'grad_norm': 3.171842336654663, 'learning_rate': 0.00012609794014768752, 'epoch': 1.17}
{'loss': 4.1815, 'grad_norm': 1.8212783336639404, 'learning_rate': 0.00012598134473377378, 'epoch': 1.17}
{'loss': 4.3922, 'grad_norm': 1.5227118730545044, 'learning_rate': 0.00012586474931986007, 'epoch': 1.17}
{'loss': 4.3455, 'grad_norm': 1.6495345830917358, 'learning_rate': 0.00012574815390594635, 'epoch': 1.17}
{'loss': 4.1832, 'grad_norm': 1.6988043785095215, 'learning_rate': 0.00012563155849203264, 'epoch': 1.18}
{'loss': 4.1808, 'grad_norm': 1.761966347694397, 'learning_rate': 0.00012551496307811893, 'epoch': 1.18}
{'loss': 4.2213, 'grad_norm': 1.7522319555282593, 'learning_rate': 0.00012539836766420521, 'epoch': 1.18}
{'loss': 4.1304, 'grad_norm': 1.7822061777114868, 'learning_rate': 0.00012528177225029147, 'epoch': 1.18}
{'loss': 4.1452, 'grad_norm': 1.899271011352539, 'learning_rate': 0.00012516517683637776, 'epoch': 1.18}
{'loss': 3.9298, 'grad_norm': 2.0109305381774902, 'learning_rate': 0.00012504858142246402, 'epoch': 1.18}
{'loss': 3.9842, 'grad_norm': 2.1947033405303955, 'learning_rate': 0.0001249319860085503, 'epoch': 1.18}
{'loss': 3.8269, 'grad_norm': 2.2897064685821533, 'learning_rate': 0.0001248153905946366, 'epoch': 1.18}
{'loss': 3.804, 'grad_norm': 2.6080198287963867, 'learning_rate': 0.00012469879518072288, 'epoch': 1.18}
{'loss': 3.5603, 'grad_norm': 2.1380395889282227, 'learning_rate': 0.00012458219976680917, 'epoch': 1.18}
{'loss': 4.4483, 'grad_norm': 1.6529059410095215, 'learning_rate': 0.00012446560435289545, 'epoch': 1.18}
{'loss': 4.2346, 'grad_norm': 1.5738924741744995, 'learning_rate': 0.00012434900893898171, 'epoch': 1.18}
{'loss': 4.1397, 'grad_norm': 1.6360843181610107, 'learning_rate': 0.000124232413525068, 'epoch': 1.18}
{'loss': 4.2684, 'grad_norm': 1.8566917181015015, 'learning_rate': 0.0001241158181111543, 'epoch': 1.19}
{'loss': 4.1838, 'grad_norm': 1.8545030355453491, 'learning_rate': 0.00012399922269724058, 'epoch': 1.19}
{'loss': 4.1961, 'grad_norm': 1.8042218685150146, 'learning_rate': 0.00012388262728332683, 'epoch': 1.19}
{'loss': 4.1442, 'grad_norm': 1.9040733575820923, 'learning_rate': 0.00012376603186941312, 'epoch': 1.19}
{'loss': 4.0873, 'grad_norm': 1.9919345378875732, 'learning_rate': 0.0001236494364554994, 'epoch': 1.19}
{'loss': 4.0189, 'grad_norm': 2.1724185943603516, 'learning_rate': 0.0001235328410415857, 'epoch': 1.19}
{'loss': 3.9193, 'grad_norm': 2.215902090072632, 'learning_rate': 0.00012341624562767196, 'epoch': 1.19}
{'loss': 3.7861, 'grad_norm': 2.4553065299987793, 'learning_rate': 0.00012329965021375824, 'epoch': 1.19}
{'loss': 3.5612, 'grad_norm': 2.8001742362976074, 'learning_rate': 0.00012318305479984453, 'epoch': 1.19}
{'loss': 4.1609, 'grad_norm': 1.8360052108764648, 'learning_rate': 0.00012306645938593082, 'epoch': 1.19}
{'loss': 4.3762, 'grad_norm': 1.61337411403656, 'learning_rate': 0.0001229498639720171, 'epoch': 1.19}
{'loss': 4.2582, 'grad_norm': 1.494213581085205, 'learning_rate': 0.0001228332685581034, 'epoch': 1.19}
{'loss': 4.2443, 'grad_norm': 1.7596745491027832, 'learning_rate': 0.00012271667314418965, 'epoch': 1.19}
{'loss': 4.2509, 'grad_norm': 1.849977731704712, 'learning_rate': 0.00012260007773027594, 'epoch': 1.2}
{'loss': 4.2238, 'grad_norm': 1.8189946413040161, 'learning_rate': 0.0001224834823163622, 'epoch': 1.2}
{'loss': 4.1786, 'grad_norm': 1.893129825592041, 'learning_rate': 0.00012236688690244848, 'epoch': 1.2}
{'loss': 4.0151, 'grad_norm': 2.085008144378662, 'learning_rate': 0.00012225029148853477, 'epoch': 1.2}
{'loss': 4.0548, 'grad_norm': 2.1469247341156006, 'learning_rate': 0.00012213369607462106, 'epoch': 1.2}
{'loss': 3.9456, 'grad_norm': 2.1397452354431152, 'learning_rate': 0.00012201710066070734, 'epoch': 1.2}
{'loss': 3.9869, 'grad_norm': 2.406083583831787, 'learning_rate': 0.00012190050524679362, 'epoch': 1.2}
{'loss': 3.7061, 'grad_norm': 2.5996880531311035, 'learning_rate': 0.00012178390983287989, 'epoch': 1.2}
{'loss': 3.4056, 'grad_norm': 2.0659167766571045, 'learning_rate': 0.00012166731441896618, 'epoch': 1.2}
{'loss': 4.489, 'grad_norm': 1.8551020622253418, 'learning_rate': 0.00012155071900505245, 'epoch': 1.2}
{'loss': 4.5153, 'grad_norm': 1.5942825078964233, 'learning_rate': 0.00012143412359113874, 'epoch': 1.2}
{'loss': 4.3039, 'grad_norm': 1.6348439455032349, 'learning_rate': 0.00012131752817722502, 'epoch': 1.2}
{'loss': 4.2778, 'grad_norm': 1.7274173498153687, 'learning_rate': 0.0001212009327633113, 'epoch': 1.2}
{'loss': 4.1232, 'grad_norm': 1.7459274530410767, 'learning_rate': 0.00012108433734939758, 'epoch': 1.21}
{'loss': 4.2105, 'grad_norm': 1.9407764673233032, 'learning_rate': 0.00012096774193548386, 'epoch': 1.21}
{'loss': 4.1069, 'grad_norm': 1.9082554578781128, 'learning_rate': 0.00012085114652157013, 'epoch': 1.21}
{'loss': 3.9378, 'grad_norm': 2.004558563232422, 'learning_rate': 0.00012073455110765642, 'epoch': 1.21}
{'loss': 4.059, 'grad_norm': 2.077993392944336, 'learning_rate': 0.0001206179556937427, 'epoch': 1.21}
{'loss': 3.9546, 'grad_norm': 2.4517626762390137, 'learning_rate': 0.00012050136027982899, 'epoch': 1.21}
{'loss': 3.8092, 'grad_norm': 2.5480403900146484, 'learning_rate': 0.00012038476486591526, 'epoch': 1.21}
{'loss': 3.5479, 'grad_norm': 2.951288938522339, 'learning_rate': 0.00012026816945200155, 'epoch': 1.21}
{'loss': 4.2122, 'grad_norm': 1.7436364889144897, 'learning_rate': 0.00012015157403808781, 'epoch': 1.21}
{'loss': 4.3602, 'grad_norm': 1.5415666103363037, 'learning_rate': 0.0001200349786241741, 'epoch': 1.21}
{'loss': 4.2474, 'grad_norm': 1.5644094944000244, 'learning_rate': 0.00011991838321026038, 'epoch': 1.21}
{'loss': 4.3062, 'grad_norm': 1.6605528593063354, 'learning_rate': 0.00011980178779634667, 'epoch': 1.21}
{'loss': 4.2447, 'grad_norm': 1.689965009689331, 'learning_rate': 0.00011968519238243295, 'epoch': 1.21}
{'loss': 4.1416, 'grad_norm': 1.7229629755020142, 'learning_rate': 0.00011956859696851923, 'epoch': 1.22}
{'loss': 4.0093, 'grad_norm': 1.8812628984451294, 'learning_rate': 0.00011945200155460552, 'epoch': 1.22}
{'loss': 4.0702, 'grad_norm': 2.05598521232605, 'learning_rate': 0.00011933540614069179, 'epoch': 1.22}
{'loss': 4.0863, 'grad_norm': 1.9884047508239746, 'learning_rate': 0.00011921881072677807, 'epoch': 1.22}
{'loss': 3.9405, 'grad_norm': 2.161618947982788, 'learning_rate': 0.00011910221531286435, 'epoch': 1.22}
{'loss': 3.8079, 'grad_norm': 2.3192594051361084, 'learning_rate': 0.00011898561989895063, 'epoch': 1.22}
{'loss': 3.7735, 'grad_norm': 2.7137322425842285, 'learning_rate': 0.00011886902448503691, 'epoch': 1.22}
{'loss': 3.5674, 'grad_norm': 1.9049851894378662, 'learning_rate': 0.0001187524290711232, 'epoch': 1.22}
{'loss': 4.4176, 'grad_norm': 1.7668960094451904, 'learning_rate': 0.00011863583365720947, 'epoch': 1.22}
{'loss': 4.3415, 'grad_norm': 1.589972972869873, 'learning_rate': 0.00011851923824329576, 'epoch': 1.22}
{'loss': 4.2852, 'grad_norm': 1.5930367708206177, 'learning_rate': 0.00011840264282938203, 'epoch': 1.22}
{'loss': 4.3111, 'grad_norm': 1.7124953269958496, 'learning_rate': 0.0001182860474154683, 'epoch': 1.22}
{'loss': 4.1764, 'grad_norm': 1.8274084329605103, 'learning_rate': 0.00011816945200155459, 'epoch': 1.22}
{'loss': 4.0881, 'grad_norm': 2.0458178520202637, 'learning_rate': 0.00011805285658764088, 'epoch': 1.23}
{'loss': 4.0783, 'grad_norm': 1.9654780626296997, 'learning_rate': 0.00011793626117372715, 'epoch': 1.23}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 6400/10442 [7:23:47<3:19:55,  2.97s/it][INFO|trainer.py:831] 2024-10-15 09:28:40,716 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 09:28:40,728 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 09:28:40,728 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 09:28:40,728 >>   Batch size = 32
{'eval_loss': 4.1773481369018555, 'eval_wer': 1.1068764832184428, 'eval_runtime': 361.4888, 'eval_samples_per_second': 4.401, 'eval_steps_per_second': 0.138, 'epoch': 1.23}
{'loss': 4.1488, 'grad_norm': 2.002272367477417, 'learning_rate': 0.00011781966575981344, 'epoch': 1.23}
{'loss': 3.9311, 'grad_norm': 2.100196361541748, 'learning_rate': 0.00011773221919937815, 'epoch': 1.23}
{'loss': 3.9086, 'grad_norm': 2.144994020462036, 'learning_rate': 0.00011761562378546442, 'epoch': 1.23}
{'loss': 3.7662, 'grad_norm': 2.4661762714385986, 'learning_rate': 0.00011749902837155071, 'epoch': 1.23}
{'loss': 3.524, 'grad_norm': 2.86303448677063, 'learning_rate': 0.000117382432957637, 'epoch': 1.23}
{'loss': 4.078, 'grad_norm': 1.6729661226272583, 'learning_rate': 0.00011726583754372327, 'epoch': 1.23}
{'loss': 4.3236, 'grad_norm': 1.6031705141067505, 'learning_rate': 0.00011714924212980956, 'epoch': 1.23}
{'loss': 4.36, 'grad_norm': 1.7175503969192505, 'learning_rate': 0.00011703264671589584, 'epoch': 1.23}
{'loss': 4.3045, 'grad_norm': 1.728318452835083, 'learning_rate': 0.0001169160513019821, 'epoch': 1.23}
{'loss': 4.1254, 'grad_norm': 1.8458189964294434, 'learning_rate': 0.00011679945588806839, 'epoch': 1.23}
{'loss': 4.0503, 'grad_norm': 1.9326742887496948, 'learning_rate': 0.00011668286047415468, 'epoch': 1.23}
{'loss': 4.2123, 'grad_norm': 2.055131196975708, 'learning_rate': 0.00011656626506024095, 'epoch': 1.24}
{'loss': 4.0447, 'grad_norm': 1.9381628036499023, 'learning_rate': 0.00011644966964632724, 'epoch': 1.24}
{'loss': 3.959, 'grad_norm': 2.1611127853393555, 'learning_rate': 0.00011633307423241352, 'epoch': 1.24}
{'loss': 3.9099, 'grad_norm': 2.14322829246521, 'learning_rate': 0.0001162164788184998, 'epoch': 1.24}
{'loss': 3.8708, 'grad_norm': 2.4103901386260986, 'learning_rate': 0.00011609988340458607, 'epoch': 1.24}
{'loss': 3.7375, 'grad_norm': 2.6028411388397217, 'learning_rate': 0.00011598328799067236, 'epoch': 1.24}
{'loss': 3.6178, 'grad_norm': 1.7284895181655884, 'learning_rate': 0.00011586669257675863, 'epoch': 1.24}
{'loss': 4.2981, 'grad_norm': 1.575080394744873, 'learning_rate': 0.00011575009716284492, 'epoch': 1.24}
{'loss': 4.387, 'grad_norm': 1.5829821825027466, 'learning_rate': 0.0001156335017489312, 'epoch': 1.24}
{'loss': 4.2046, 'grad_norm': 1.6423888206481934, 'learning_rate': 0.00011551690633501749, 'epoch': 1.24}
{'loss': 4.2177, 'grad_norm': 1.6880706548690796, 'learning_rate': 0.00011540031092110376, 'epoch': 1.24}
{'loss': 4.0842, 'grad_norm': 1.9229388236999512, 'learning_rate': 0.00011528371550719004, 'epoch': 1.24}
{'loss': 4.1495, 'grad_norm': 1.8883256912231445, 'learning_rate': 0.00011516712009327631, 'epoch': 1.24}
{'loss': 4.0992, 'grad_norm': 2.018106460571289, 'learning_rate': 0.0001150505246793626, 'epoch': 1.24}
{'loss': 4.0605, 'grad_norm': 1.942830204963684, 'learning_rate': 0.00011493392926544888, 'epoch': 1.25}
{'loss': 3.879, 'grad_norm': 2.12992787361145, 'learning_rate': 0.00011481733385153517, 'epoch': 1.25}
{'loss': 3.9733, 'grad_norm': 2.3532068729400635, 'learning_rate': 0.00011470073843762144, 'epoch': 1.25}
{'loss': 3.8714, 'grad_norm': 2.4870941638946533, 'learning_rate': 0.00011458414302370773, 'epoch': 1.25}
{'loss': 3.4933, 'grad_norm': 3.0377461910247803, 'learning_rate': 0.00011446754760979399, 'epoch': 1.25}
{'loss': 4.1492, 'grad_norm': 1.6566447019577026, 'learning_rate': 0.00011435095219588028, 'epoch': 1.25}
{'loss': 4.2352, 'grad_norm': 1.491192102432251, 'learning_rate': 0.00011423435678196656, 'epoch': 1.25}
{'loss': 4.2747, 'grad_norm': 1.653963565826416, 'learning_rate': 0.00011411776136805285, 'epoch': 1.25}
{'loss': 4.2301, 'grad_norm': 1.6317713260650635, 'learning_rate': 0.00011400116595413912, 'epoch': 1.25}
{'loss': 4.2229, 'grad_norm': 1.7821866273880005, 'learning_rate': 0.00011388457054022541, 'epoch': 1.25}
{'loss': 4.109, 'grad_norm': 1.7553006410598755, 'learning_rate': 0.0001137679751263117, 'epoch': 1.25}
{'loss': 4.0205, 'grad_norm': 1.9574233293533325, 'learning_rate': 0.00011365137971239796, 'epoch': 1.25}
{'loss': 4.1131, 'grad_norm': 1.964177131652832, 'learning_rate': 0.00011353478429848425, 'epoch': 1.25}
{'loss': 4.1098, 'grad_norm': 2.1390020847320557, 'learning_rate': 0.00011341818888457053, 'epoch': 1.26}
{'loss': 3.9272, 'grad_norm': 2.1389012336730957, 'learning_rate': 0.0001133015934706568, 'epoch': 1.26}
{'loss': 3.9796, 'grad_norm': 2.3804516792297363, 'learning_rate': 0.00011318499805674309, 'epoch': 1.26}
{'loss': 3.756, 'grad_norm': 2.405892848968506, 'learning_rate': 0.00011306840264282938, 'epoch': 1.26}
{'loss': 3.647, 'grad_norm': 1.7610446214675903, 'learning_rate': 0.00011295180722891565, 'epoch': 1.26}
{'loss': 4.4747, 'grad_norm': 1.697430968284607, 'learning_rate': 0.00011283521181500194, 'epoch': 1.26}
{'loss': 4.3854, 'grad_norm': 1.556246042251587, 'learning_rate': 0.00011271861640108821, 'epoch': 1.26}
{'loss': 4.2885, 'grad_norm': 1.5384999513626099, 'learning_rate': 0.00011260202098717449, 'epoch': 1.26}
{'loss': 4.1683, 'grad_norm': 1.626989722251892, 'learning_rate': 0.00011248542557326077, 'epoch': 1.26}
{'loss': 4.1381, 'grad_norm': 1.8442003726959229, 'learning_rate': 0.00011236883015934706, 'epoch': 1.26}
{'loss': 4.0066, 'grad_norm': 1.9380412101745605, 'learning_rate': 0.00011225223474543333, 'epoch': 1.26}
{'loss': 4.0789, 'grad_norm': 1.91972017288208, 'learning_rate': 0.00011213563933151962, 'epoch': 1.26}
{'loss': 4.0377, 'grad_norm': 2.031446695327759, 'learning_rate': 0.0001120190439176059, 'epoch': 1.26}
{'loss': 3.7004, 'grad_norm': 2.012491464614868, 'learning_rate': 0.00011190244850369217, 'epoch': 1.27}
{'loss': 3.8601, 'grad_norm': 2.2509567737579346, 'learning_rate': 0.00011178585308977845, 'epoch': 1.27}
{'loss': 3.7526, 'grad_norm': 2.3389368057250977, 'learning_rate': 0.00011166925767586474, 'epoch': 1.27}
{'loss': 3.5504, 'grad_norm': 2.6718556880950928, 'learning_rate': 0.00011155266226195103, 'epoch': 1.27}
{'loss': 4.0597, 'grad_norm': 1.6660743951797485, 'learning_rate': 0.0001114360668480373, 'epoch': 1.27}
{'loss': 4.3068, 'grad_norm': 1.5321155786514282, 'learning_rate': 0.00011131947143412359, 'epoch': 1.27}
{'loss': 4.26, 'grad_norm': 1.6070010662078857, 'learning_rate': 0.00011120287602020987, 'epoch': 1.27}
{'loss': 4.1561, 'grad_norm': 1.7192250490188599, 'learning_rate': 0.00011108628060629613, 'epoch': 1.27}
{'loss': 4.1009, 'grad_norm': 1.8008253574371338, 'learning_rate': 0.00011096968519238242, 'epoch': 1.27}
{'loss': 4.2653, 'grad_norm': 1.835053563117981, 'learning_rate': 0.00011085308977846871, 'epoch': 1.27}
{'loss': 4.1514, 'grad_norm': 1.9165394306182861, 'learning_rate': 0.00011073649436455498, 'epoch': 1.27}
{'loss': 4.0519, 'grad_norm': 2.0576415061950684, 'learning_rate': 0.00011061989895064127, 'epoch': 1.27}
{'loss': 3.9472, 'grad_norm': 2.0990140438079834, 'learning_rate': 0.00011050330353672755, 'epoch': 1.27}
{'loss': 3.9075, 'grad_norm': 2.1804959774017334, 'learning_rate': 0.00011038670812281383, 'epoch': 1.28}
{'loss': 3.8154, 'grad_norm': 2.306553840637207, 'learning_rate': 0.0001102701127089001, 'epoch': 1.28}
{'loss': 3.6453, 'grad_norm': 2.6189424991607666, 'learning_rate': 0.00011015351729498639, 'epoch': 1.28}
{'loss': 3.5592, 'grad_norm': 1.5803073644638062, 'learning_rate': 0.00011003692188107266, 'epoch': 1.28}
{'loss': 4.482, 'grad_norm': 1.579075813293457, 'learning_rate': 0.00010992032646715895, 'epoch': 1.28}
{'loss': 4.287, 'grad_norm': 1.5764553546905518, 'learning_rate': 0.00010980373105324523, 'epoch': 1.28}
{'loss': 4.2005, 'grad_norm': 1.6246161460876465, 'learning_rate': 0.00010968713563933151, 'epoch': 1.28}
{'loss': 4.139, 'grad_norm': 1.7478415966033936, 'learning_rate': 0.0001095705402254178, 'epoch': 1.28}
{'loss': 4.1531, 'grad_norm': 1.9008665084838867, 'learning_rate': 0.00010945394481150408, 'epoch': 1.28}
{'loss': 4.136, 'grad_norm': 1.8988324403762817, 'learning_rate': 0.00010933734939759034, 'epoch': 1.28}
{'loss': 4.1, 'grad_norm': 1.9086765050888062, 'learning_rate': 0.00010922075398367663, 'epoch': 1.28}
{'loss': 3.9614, 'grad_norm': 2.0430216789245605, 'learning_rate': 0.00010910415856976292, 'epoch': 1.28}
{'loss': 3.9641, 'grad_norm': 2.260009527206421, 'learning_rate': 0.00010898756315584919, 'epoch': 1.28}
{'loss': 3.9484, 'grad_norm': 2.264188766479492, 'learning_rate': 0.00010887096774193548, 'epoch': 1.29}
{'loss': 3.7813, 'grad_norm': 2.3623762130737305, 'learning_rate': 0.00010875437232802176, 'epoch': 1.29}
{'loss': 3.478, 'grad_norm': 2.791637420654297, 'learning_rate': 0.00010863777691410805, 'epoch': 1.29}
{'loss': 4.0169, 'grad_norm': 1.5564227104187012, 'learning_rate': 0.00010852118150019431, 'epoch': 1.29}
{'loss': 4.2983, 'grad_norm': 1.5130431652069092, 'learning_rate': 0.0001084045860862806, 'epoch': 1.29}
{'loss': 4.2057, 'grad_norm': 1.6040184497833252, 'learning_rate': 0.00010828799067236687, 'epoch': 1.29}
{'loss': 4.2294, 'grad_norm': 1.6116489171981812, 'learning_rate': 0.00010817139525845316, 'epoch': 1.29}
{'loss': 3.9916, 'grad_norm': 1.7458258867263794, 'learning_rate': 0.00010805479984453944, 'epoch': 1.29}
{'loss': 4.0277, 'grad_norm': 1.8348301649093628, 'learning_rate': 0.00010793820443062573, 'epoch': 1.29}
{'loss': 3.9563, 'grad_norm': 1.936324954032898, 'learning_rate': 0.000107821609016712, 'epoch': 1.29}
{'loss': 4.0138, 'grad_norm': 2.129495859146118, 'learning_rate': 0.00010770501360279828, 'epoch': 1.29}
{'loss': 3.9665, 'grad_norm': 2.228947401046753, 'learning_rate': 0.00010758841818888455, 'epoch': 1.29}
{'loss': 3.897, 'grad_norm': 2.248521327972412, 'learning_rate': 0.00010747182277497084, 'epoch': 1.29}
{'loss': 3.8403, 'grad_norm': 2.293254852294922, 'learning_rate': 0.00010735522736105712, 'epoch': 1.3}
{'loss': 3.5906, 'grad_norm': 2.8227663040161133, 'learning_rate': 0.00010723863194714341, 'epoch': 1.3}
{'loss': 3.3495, 'grad_norm': 1.629481315612793, 'learning_rate': 0.00010712203653322968, 'epoch': 1.3}
{'loss': 4.3233, 'grad_norm': 1.5273431539535522, 'learning_rate': 0.00010700544111931597, 'epoch': 1.3}
{'loss': 4.3183, 'grad_norm': 1.5840202569961548, 'learning_rate': 0.00010688884570540224, 'epoch': 1.3}
{'loss': 4.1168, 'grad_norm': 1.7131708860397339, 'learning_rate': 0.00010677225029148852, 'epoch': 1.3}
{'loss': 3.9676, 'grad_norm': 1.7809362411499023, 'learning_rate': 0.0001066556548775748, 'epoch': 1.3}
{'loss': 4.1755, 'grad_norm': 1.8239891529083252, 'learning_rate': 0.00010653905946366109, 'epoch': 1.3}
{'loss': 4.1478, 'grad_norm': 1.973149061203003, 'learning_rate': 0.00010642246404974736, 'epoch': 1.3}
{'loss': 3.9382, 'grad_norm': 1.8757864236831665, 'learning_rate': 0.00010630586863583365, 'epoch': 1.3}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 6800/10442 [7:52:51<3:04:05,  3.03s/it][INFO|trainer.py:831] 2024-10-15 09:57:44,340 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 09:57:44,355 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 09:57:44,355 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 09:57:44,355 >>   Batch size = 32
{'eval_loss': 4.1691060066223145, 'eval_wer': 1.1178099220250877, 'eval_runtime': 347.9095, 'eval_samples_per_second': 4.573, 'eval_steps_per_second': 0.144, 'epoch': 1.3}
{'loss': 4.0453, 'grad_norm': 1.9306440353393555, 'learning_rate': 0.00010618927322191994, 'epoch': 1.3}
{'loss': 4.0555, 'grad_norm': 2.311221122741699, 'learning_rate': 0.0001060726778080062, 'epoch': 1.3}
{'loss': 3.8604, 'grad_norm': 2.165079355239868, 'learning_rate': 0.00010595608239409248, 'epoch': 1.3}
{'loss': 3.8985, 'grad_norm': 2.52526593208313, 'learning_rate': 0.00010583948698017877, 'epoch': 1.31}
{'loss': 3.3956, 'grad_norm': 3.2039592266082764, 'learning_rate': 0.00010572289156626504, 'epoch': 1.31}
{'loss': 4.0071, 'grad_norm': 1.5871840715408325, 'learning_rate': 0.00010560629615235133, 'epoch': 1.31}
{'loss': 4.3538, 'grad_norm': 1.4774243831634521, 'learning_rate': 0.00010548970073843762, 'epoch': 1.31}
{'loss': 4.2108, 'grad_norm': 1.517824649810791, 'learning_rate': 0.0001053731053245239, 'epoch': 1.31}
{'loss': 4.2209, 'grad_norm': 1.6551172733306885, 'learning_rate': 0.00010525650991061018, 'epoch': 1.31}
{'loss': 4.1473, 'grad_norm': 1.734887719154358, 'learning_rate': 0.00010513991449669645, 'epoch': 1.31}
{'loss': 4.2997, 'grad_norm': 1.9366048574447632, 'learning_rate': 0.00010502331908278273, 'epoch': 1.31}
{'loss': 4.0553, 'grad_norm': 1.9047667980194092, 'learning_rate': 0.00010490672366886901, 'epoch': 1.31}
{'loss': 4.0089, 'grad_norm': 1.8756911754608154, 'learning_rate': 0.0001047901282549553, 'epoch': 1.31}
{'loss': 4.0223, 'grad_norm': 2.071995735168457, 'learning_rate': 0.00010467353284104159, 'epoch': 1.31}
{'loss': 3.8986, 'grad_norm': 2.0721700191497803, 'learning_rate': 0.00010455693742712786, 'epoch': 1.31}
{'loss': 3.8551, 'grad_norm': 2.3842012882232666, 'learning_rate': 0.00010444034201321415, 'epoch': 1.31}
{'loss': 3.5303, 'grad_norm': 2.676737070083618, 'learning_rate': 0.0001043237465993004, 'epoch': 1.32}
{'loss': 3.5669, 'grad_norm': 1.8671081066131592, 'learning_rate': 0.00010420715118538669, 'epoch': 1.32}
{'loss': 4.3312, 'grad_norm': 1.7012063264846802, 'learning_rate': 0.00010409055577147298, 'epoch': 1.32}
{'loss': 4.3341, 'grad_norm': 1.5913041830062866, 'learning_rate': 0.00010397396035755927, 'epoch': 1.32}
{'loss': 4.216, 'grad_norm': 1.6588969230651855, 'learning_rate': 0.00010385736494364554, 'epoch': 1.32}
{'loss': 4.1298, 'grad_norm': 1.722604513168335, 'learning_rate': 0.00010374076952973183, 'epoch': 1.32}
{'loss': 4.1963, 'grad_norm': 1.7288293838500977, 'learning_rate': 0.00010362417411581811, 'epoch': 1.32}
{'loss': 4.0799, 'grad_norm': 1.8293976783752441, 'learning_rate': 0.00010350757870190437, 'epoch': 1.32}
{'loss': 4.1837, 'grad_norm': 1.9067802429199219, 'learning_rate': 0.00010339098328799066, 'epoch': 1.32}
{'loss': 3.9896, 'grad_norm': 2.0209131240844727, 'learning_rate': 0.00010327438787407695, 'epoch': 1.32}
{'loss': 3.9931, 'grad_norm': 2.1178722381591797, 'learning_rate': 0.00010315779246016322, 'epoch': 1.32}
{'loss': 4.0537, 'grad_norm': 2.4625871181488037, 'learning_rate': 0.00010304119704624951, 'epoch': 1.32}
{'loss': 3.7243, 'grad_norm': 2.5462472438812256, 'learning_rate': 0.0001029246016323358, 'epoch': 1.32}
{'loss': 3.4642, 'grad_norm': 2.876725435256958, 'learning_rate': 0.00010280800621842207, 'epoch': 1.33}
{'loss': 4.1334, 'grad_norm': 1.8240370750427246, 'learning_rate': 0.00010269141080450834, 'epoch': 1.33}
{'loss': 4.311, 'grad_norm': 1.5963152647018433, 'learning_rate': 0.00010257481539059463, 'epoch': 1.33}
{'loss': 4.1736, 'grad_norm': 1.6199413537979126, 'learning_rate': 0.0001024582199766809, 'epoch': 1.33}
{'loss': 4.1205, 'grad_norm': 1.7556233406066895, 'learning_rate': 0.00010234162456276719, 'epoch': 1.33}
{'loss': 4.1695, 'grad_norm': 1.8100875616073608, 'learning_rate': 0.00010222502914885347, 'epoch': 1.33}
{'loss': 4.157, 'grad_norm': 1.9050980806350708, 'learning_rate': 0.00010210843373493975, 'epoch': 1.33}
{'loss': 4.0436, 'grad_norm': 1.8916003704071045, 'learning_rate': 0.00010199183832102603, 'epoch': 1.33}
{'loss': 3.8957, 'grad_norm': 1.9589271545410156, 'learning_rate': 0.00010187524290711231, 'epoch': 1.33}
{'loss': 3.8966, 'grad_norm': 2.1195037364959717, 'learning_rate': 0.00010175864749319858, 'epoch': 1.33}
{'loss': 3.8125, 'grad_norm': 2.1287856101989746, 'learning_rate': 0.00010164205207928487, 'epoch': 1.33}
{'loss': 3.9439, 'grad_norm': 2.5602941513061523, 'learning_rate': 0.00010152545666537116, 'epoch': 1.33}
{'loss': 3.6821, 'grad_norm': 2.8844337463378906, 'learning_rate': 0.00010140886125145743, 'epoch': 1.33}
{'loss': 3.6814, 'grad_norm': 1.6994574069976807, 'learning_rate': 0.00010129226583754372, 'epoch': 1.34}
{'loss': 4.2385, 'grad_norm': 1.689038634300232, 'learning_rate': 0.00010117567042363, 'epoch': 1.34}
{'loss': 4.234, 'grad_norm': 1.5976582765579224, 'learning_rate': 0.00010105907500971629, 'epoch': 1.34}
{'loss': 4.1887, 'grad_norm': 1.706059455871582, 'learning_rate': 0.00010094247959580255, 'epoch': 1.34}
{'loss': 4.112, 'grad_norm': 1.8261171579360962, 'learning_rate': 0.00010082588418188884, 'epoch': 1.34}
{'loss': 4.1493, 'grad_norm': 1.8345929384231567, 'learning_rate': 0.00010070928876797512, 'epoch': 1.34}
{'loss': 4.0491, 'grad_norm': 1.9604932069778442, 'learning_rate': 0.0001005926933540614, 'epoch': 1.34}
{'loss': 4.0457, 'grad_norm': 1.9307831525802612, 'learning_rate': 0.00010047609794014768, 'epoch': 1.34}
{'loss': 3.9072, 'grad_norm': 2.0873501300811768, 'learning_rate': 0.00010035950252623397, 'epoch': 1.34}
{'loss': 3.9108, 'grad_norm': 2.3320980072021484, 'learning_rate': 0.00010024290711232024, 'epoch': 1.34}
{'loss': 3.8315, 'grad_norm': 2.321928024291992, 'learning_rate': 0.00010012631169840652, 'epoch': 1.34}
{'loss': 3.7319, 'grad_norm': 2.6882336139678955, 'learning_rate': 0.0001000097162844928, 'epoch': 1.34}
{'loss': 3.5465, 'grad_norm': 3.3714778423309326, 'learning_rate': 9.989312087057908e-05, 'epoch': 1.34}
{'loss': 3.9326, 'grad_norm': 1.5817971229553223, 'learning_rate': 9.977652545666536e-05, 'epoch': 1.35}
{'loss': 4.2347, 'grad_norm': 1.5903081893920898, 'learning_rate': 9.965993004275165e-05, 'epoch': 1.35}
{'loss': 4.253, 'grad_norm': 1.6504979133605957, 'learning_rate': 9.954333462883792e-05, 'epoch': 1.35}
{'loss': 4.0749, 'grad_norm': 1.7214467525482178, 'learning_rate': 9.942673921492421e-05, 'epoch': 1.35}
{'loss': 4.1724, 'grad_norm': 1.8782129287719727, 'learning_rate': 9.931014380101048e-05, 'epoch': 1.35}
{'loss': 4.0951, 'grad_norm': 1.8207030296325684, 'learning_rate': 9.919354838709676e-05, 'epoch': 1.35}
{'loss': 4.1607, 'grad_norm': 1.8540258407592773, 'learning_rate': 9.907695297318304e-05, 'epoch': 1.35}
{'loss': 4.0212, 'grad_norm': 2.049602508544922, 'learning_rate': 9.896035755926933e-05, 'epoch': 1.35}
{'loss': 3.876, 'grad_norm': 2.0364573001861572, 'learning_rate': 9.88437621453556e-05, 'epoch': 1.35}
{'loss': 3.877, 'grad_norm': 2.2479429244995117, 'learning_rate': 9.872716673144189e-05, 'epoch': 1.35}
{'loss': 3.8343, 'grad_norm': 2.304593086242676, 'learning_rate': 9.861057131752818e-05, 'epoch': 1.35}
{'loss': 3.6856, 'grad_norm': 2.5681815147399902, 'learning_rate': 9.849397590361444e-05, 'epoch': 1.35}
{'loss': 3.4709, 'grad_norm': 1.705396294593811, 'learning_rate': 9.837738048970072e-05, 'epoch': 1.35}
{'loss': 4.3251, 'grad_norm': 1.6025187969207764, 'learning_rate': 9.826078507578701e-05, 'epoch': 1.36}
{'loss': 4.2757, 'grad_norm': 1.5894005298614502, 'learning_rate': 9.814418966187328e-05, 'epoch': 1.36}
{'loss': 4.1776, 'grad_norm': 1.5511795282363892, 'learning_rate': 9.802759424795957e-05, 'epoch': 1.36}
{'loss': 4.0433, 'grad_norm': 1.8004337549209595, 'learning_rate': 9.791099883404586e-05, 'epoch': 1.36}
{'loss': 4.1498, 'grad_norm': 1.8101316690444946, 'learning_rate': 9.779440342013214e-05, 'epoch': 1.36}
{'loss': 4.0688, 'grad_norm': 1.9165748357772827, 'learning_rate': 9.76778080062184e-05, 'epoch': 1.36}
{'loss': 4.136, 'grad_norm': 1.9873915910720825, 'learning_rate': 9.756121259230469e-05, 'epoch': 1.36}
{'loss': 3.9795, 'grad_norm': 2.0166258811950684, 'learning_rate': 9.744461717839096e-05, 'epoch': 1.36}
{'loss': 3.9241, 'grad_norm': 2.086639642715454, 'learning_rate': 9.732802176447725e-05, 'epoch': 1.36}
{'loss': 3.9455, 'grad_norm': 2.2935755252838135, 'learning_rate': 9.721142635056354e-05, 'epoch': 1.36}
{'loss': 3.7863, 'grad_norm': 2.567218542098999, 'learning_rate': 9.709483093664983e-05, 'epoch': 1.36}
{'loss': 3.5706, 'grad_norm': 2.8307783603668213, 'learning_rate': 9.69782355227361e-05, 'epoch': 1.36}
{'loss': 4.025, 'grad_norm': 1.8322696685791016, 'learning_rate': 9.686164010882239e-05, 'epoch': 1.36}
{'loss': 4.263, 'grad_norm': 1.6302801370620728, 'learning_rate': 9.674504469490866e-05, 'epoch': 1.37}
{'loss': 4.2064, 'grad_norm': 1.5797061920166016, 'learning_rate': 9.662844928099493e-05, 'epoch': 1.37}
{'loss': 4.1954, 'grad_norm': 1.7047502994537354, 'learning_rate': 9.651185386708122e-05, 'epoch': 1.37}
{'loss': 4.0773, 'grad_norm': 1.7069039344787598, 'learning_rate': 9.63952584531675e-05, 'epoch': 1.37}
{'loss': 3.9634, 'grad_norm': 1.8032411336898804, 'learning_rate': 9.627866303925378e-05, 'epoch': 1.37}
{'loss': 4.1487, 'grad_norm': 1.9137210845947266, 'learning_rate': 9.616206762534007e-05, 'epoch': 1.37}
{'loss': 4.0271, 'grad_norm': 1.94779634475708, 'learning_rate': 9.604547221142635e-05, 'epoch': 1.37}
{'loss': 4.0213, 'grad_norm': 2.0438294410705566, 'learning_rate': 9.592887679751261e-05, 'epoch': 1.37}
{'loss': 3.9605, 'grad_norm': 2.244798421859741, 'learning_rate': 9.58122813835989e-05, 'epoch': 1.37}
{'loss': 3.7295, 'grad_norm': 2.371178150177002, 'learning_rate': 9.569568596968519e-05, 'epoch': 1.37}
{'loss': 3.6503, 'grad_norm': 2.9381203651428223, 'learning_rate': 9.557909055577146e-05, 'epoch': 1.37}
{'loss': 3.3667, 'grad_norm': 1.5385069847106934, 'learning_rate': 9.546249514185775e-05, 'epoch': 1.37}
{'loss': 4.2212, 'grad_norm': 1.5490306615829468, 'learning_rate': 9.534589972794403e-05, 'epoch': 1.37}
{'loss': 4.1267, 'grad_norm': 1.5884588956832886, 'learning_rate': 9.522930431403031e-05, 'epoch': 1.38}
{'loss': 4.0549, 'grad_norm': 1.6419655084609985, 'learning_rate': 9.511270890011658e-05, 'epoch': 1.38}
{'loss': 4.0708, 'grad_norm': 1.719527006149292, 'learning_rate': 9.499611348620287e-05, 'epoch': 1.38}
{'loss': 4.1617, 'grad_norm': 1.8648625612258911, 'learning_rate': 9.487951807228914e-05, 'epoch': 1.38}
{'loss': 4.1295, 'grad_norm': 1.810004472732544, 'learning_rate': 9.476292265837543e-05, 'epoch': 1.38}
{'loss': 4.1449, 'grad_norm': 2.018799304962158, 'learning_rate': 9.464632724446171e-05, 'epoch': 1.38}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 7200/10442 [8:21:44<2:39:36,  2.95s/it][INFO|trainer.py:831] 2024-10-15 10:26:37,658 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 10:26:37,672 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 10:26:37,672 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 10:26:37,672 >>   Batch size = 32
{'eval_loss': 4.150200366973877, 'eval_wer': 1.3507458469883602, 'eval_runtime': 364.8946, 'eval_samples_per_second': 4.36, 'eval_steps_per_second': 0.137, 'epoch': 1.38}
{'loss': 3.9839, 'grad_norm': 2.0539140701293945, 'learning_rate': 9.4529731830548e-05, 'epoch': 1.38}
{'loss': 4.0638, 'grad_norm': 2.195436477661133, 'learning_rate': 9.441313641663427e-05, 'epoch': 1.38}
{'loss': 3.8259, 'grad_norm': 2.247426986694336, 'learning_rate': 9.429654100272055e-05, 'epoch': 1.38}
{'loss': 3.6726, 'grad_norm': 2.417734146118164, 'learning_rate': 9.417994558880682e-05, 'epoch': 1.38}
{'loss': 3.65, 'grad_norm': 2.847722291946411, 'learning_rate': 9.406335017489311e-05, 'epoch': 1.38}
{'loss': 3.9973, 'grad_norm': 1.6692482233047485, 'learning_rate': 9.39467547609794e-05, 'epoch': 1.38}
{'loss': 4.2649, 'grad_norm': 1.6017228364944458, 'learning_rate': 9.383015934706568e-05, 'epoch': 1.38}
{'loss': 4.2646, 'grad_norm': 1.5924115180969238, 'learning_rate': 9.371356393315195e-05, 'epoch': 1.39}
{'loss': 4.192, 'grad_norm': 1.7476142644882202, 'learning_rate': 9.359696851923824e-05, 'epoch': 1.39}
{'loss': 4.1192, 'grad_norm': 1.6903258562088013, 'learning_rate': 9.34803731053245e-05, 'epoch': 1.39}
{'loss': 4.1136, 'grad_norm': 1.9082728624343872, 'learning_rate': 9.336377769141079e-05, 'epoch': 1.39}
{'loss': 3.9623, 'grad_norm': 1.9063818454742432, 'learning_rate': 9.324718227749708e-05, 'epoch': 1.39}
{'loss': 4.1565, 'grad_norm': 2.120091199874878, 'learning_rate': 9.313058686358336e-05, 'epoch': 1.39}
{'loss': 4.0103, 'grad_norm': 2.1508846282958984, 'learning_rate': 9.301399144966964e-05, 'epoch': 1.39}
{'loss': 3.9847, 'grad_norm': 2.2779200077056885, 'learning_rate': 9.289739603575592e-05, 'epoch': 1.39}
{'loss': 3.7635, 'grad_norm': 2.4684817790985107, 'learning_rate': 9.278080062184221e-05, 'epoch': 1.39}
{'loss': 3.7058, 'grad_norm': 2.5450425148010254, 'learning_rate': 9.266420520792848e-05, 'epoch': 1.39}
{'loss': 3.7042, 'grad_norm': 1.6883901357650757, 'learning_rate': 9.254760979401476e-05, 'epoch': 1.39}
{'loss': 4.27, 'grad_norm': 1.5121382474899292, 'learning_rate': 9.243101438010104e-05, 'epoch': 1.39}
{'loss': 4.1686, 'grad_norm': 1.6111117601394653, 'learning_rate': 9.231441896618732e-05, 'epoch': 1.39}
{'loss': 4.2156, 'grad_norm': 1.656705379486084, 'learning_rate': 9.21978235522736e-05, 'epoch': 1.4}
{'loss': 4.1247, 'grad_norm': 1.7933262586593628, 'learning_rate': 9.208122813835989e-05, 'epoch': 1.4}
{'loss': 4.0521, 'grad_norm': 1.813049077987671, 'learning_rate': 9.196463272444616e-05, 'epoch': 1.4}
{'loss': 4.1043, 'grad_norm': 1.8433023691177368, 'learning_rate': 9.184803731053245e-05, 'epoch': 1.4}
{'loss': 3.9408, 'grad_norm': 2.0091118812561035, 'learning_rate': 9.173144189661872e-05, 'epoch': 1.4}
{'loss': 4.0398, 'grad_norm': 1.936263918876648, 'learning_rate': 9.1614846482705e-05, 'epoch': 1.4}
{'loss': 4.0585, 'grad_norm': 2.3234946727752686, 'learning_rate': 9.149825106879128e-05, 'epoch': 1.4}
{'loss': 3.8686, 'grad_norm': 2.2685465812683105, 'learning_rate': 9.138165565487757e-05, 'epoch': 1.4}
{'loss': 3.6913, 'grad_norm': 2.399122953414917, 'learning_rate': 9.126506024096384e-05, 'epoch': 1.4}
{'loss': 3.4238, 'grad_norm': 2.8147761821746826, 'learning_rate': 9.114846482705013e-05, 'epoch': 1.4}
{'loss': 3.9111, 'grad_norm': 1.567391276359558, 'learning_rate': 9.103186941313642e-05, 'epoch': 1.4}
{'loss': 4.2409, 'grad_norm': 1.5471937656402588, 'learning_rate': 9.091527399922268e-05, 'epoch': 1.4}
{'loss': 4.1688, 'grad_norm': 1.6375049352645874, 'learning_rate': 9.079867858530896e-05, 'epoch': 1.4}
{'loss': 4.1992, 'grad_norm': 1.693671703338623, 'learning_rate': 9.068208317139525e-05, 'epoch': 1.41}
{'loss': 4.2324, 'grad_norm': 1.7946879863739014, 'learning_rate': 9.056548775748154e-05, 'epoch': 1.41}
{'loss': 3.9897, 'grad_norm': 1.7780470848083496, 'learning_rate': 9.044889234356781e-05, 'epoch': 1.41}
{'loss': 4.1243, 'grad_norm': 2.146888017654419, 'learning_rate': 9.03322969296541e-05, 'epoch': 1.41}
{'loss': 4.0082, 'grad_norm': 2.071226119995117, 'learning_rate': 9.021570151574038e-05, 'epoch': 1.41}
{'loss': 3.9661, 'grad_norm': 2.0441389083862305, 'learning_rate': 9.009910610182664e-05, 'epoch': 1.41}
{'loss': 3.925, 'grad_norm': 2.3908472061157227, 'learning_rate': 8.998251068791293e-05, 'epoch': 1.41}
{'loss': 3.7738, 'grad_norm': 2.32126784324646, 'learning_rate': 8.986591527399922e-05, 'epoch': 1.41}
{'loss': 3.6275, 'grad_norm': 2.610832452774048, 'learning_rate': 8.974931986008549e-05, 'epoch': 1.41}
{'loss': 3.4837, 'grad_norm': 1.82343590259552, 'learning_rate': 8.963272444617178e-05, 'epoch': 1.41}
{'loss': 4.3343, 'grad_norm': 1.6980174779891968, 'learning_rate': 8.951612903225806e-05, 'epoch': 1.41}
{'loss': 4.3218, 'grad_norm': 1.6285067796707153, 'learning_rate': 8.939953361834434e-05, 'epoch': 1.41}
{'loss': 4.0839, 'grad_norm': 1.5930649042129517, 'learning_rate': 8.928293820443061e-05, 'epoch': 1.41}
{'loss': 4.1042, 'grad_norm': 1.6758960485458374, 'learning_rate': 8.91663427905169e-05, 'epoch': 1.42}
{'loss': 4.0253, 'grad_norm': 1.940871000289917, 'learning_rate': 8.904974737660317e-05, 'epoch': 1.42}
{'loss': 4.1185, 'grad_norm': 2.0751101970672607, 'learning_rate': 8.893315196268946e-05, 'epoch': 1.42}
{'loss': 4.0021, 'grad_norm': 2.0434329509735107, 'learning_rate': 8.881655654877575e-05, 'epoch': 1.42}
{'loss': 3.9161, 'grad_norm': 33.322509765625, 'learning_rate': 8.869996113486202e-05, 'epoch': 1.42}
{'loss': 3.9205, 'grad_norm': 2.1697394847869873, 'learning_rate': 8.85833657209483e-05, 'epoch': 1.42}
{'loss': 3.8563, 'grad_norm': 2.311004400253296, 'learning_rate': 8.846677030703459e-05, 'epoch': 1.42}
{'loss': 3.8693, 'grad_norm': 2.7527215480804443, 'learning_rate': 8.835017489312085e-05, 'epoch': 1.42}
{'loss': 3.4603, 'grad_norm': 3.0717592239379883, 'learning_rate': 8.823357947920714e-05, 'epoch': 1.42}
{'loss': 4.1385, 'grad_norm': 1.5505287647247314, 'learning_rate': 8.811698406529343e-05, 'epoch': 1.42}
{'loss': 4.264, 'grad_norm': 1.5455296039581299, 'learning_rate': 8.80003886513797e-05, 'epoch': 1.42}
{'loss': 4.2041, 'grad_norm': 1.594065546989441, 'learning_rate': 8.788379323746599e-05, 'epoch': 1.42}
{'loss': 4.1119, 'grad_norm': 1.5913528203964233, 'learning_rate': 8.776719782355227e-05, 'epoch': 1.42}
{'loss': 4.0917, 'grad_norm': 1.8243756294250488, 'learning_rate': 8.765060240963856e-05, 'epoch': 1.43}
{'loss': 4.1635, 'grad_norm': 1.924227237701416, 'learning_rate': 8.753400699572482e-05, 'epoch': 1.43}
{'loss': 3.9956, 'grad_norm': 2.0541446208953857, 'learning_rate': 8.74174115818111e-05, 'epoch': 1.43}
{'loss': 4.026, 'grad_norm': 1.9379799365997314, 'learning_rate': 8.730081616789738e-05, 'epoch': 1.43}
{'loss': 3.9219, 'grad_norm': 2.1705896854400635, 'learning_rate': 8.718422075398367e-05, 'epoch': 1.43}
{'loss': 3.935, 'grad_norm': 2.174792528152466, 'learning_rate': 8.706762534006995e-05, 'epoch': 1.43}
{'loss': 3.7808, 'grad_norm': 2.4298312664031982, 'learning_rate': 8.695102992615624e-05, 'epoch': 1.43}
{'loss': 3.5223, 'grad_norm': 2.5345618724823, 'learning_rate': 8.683443451224251e-05, 'epoch': 1.43}
{'loss': 3.5923, 'grad_norm': 1.8192585706710815, 'learning_rate': 8.671783909832879e-05, 'epoch': 1.43}
{'loss': 4.2993, 'grad_norm': 1.600988745689392, 'learning_rate': 8.660124368441506e-05, 'epoch': 1.43}
{'loss': 4.1996, 'grad_norm': 1.5398194789886475, 'learning_rate': 8.648464827050135e-05, 'epoch': 1.43}
{'loss': 4.2007, 'grad_norm': 1.6744519472122192, 'learning_rate': 8.636805285658763e-05, 'epoch': 1.43}
{'loss': 4.2177, 'grad_norm': 1.611562967300415, 'learning_rate': 8.625145744267392e-05, 'epoch': 1.43}
{'loss': 4.1088, 'grad_norm': 1.7563263177871704, 'learning_rate': 8.61348620287602e-05, 'epoch': 1.43}
{'loss': 4.0961, 'grad_norm': 1.895111083984375, 'learning_rate': 8.601826661484648e-05, 'epoch': 1.44}
{'loss': 4.0652, 'grad_norm': 1.9163732528686523, 'learning_rate': 8.590167120093275e-05, 'epoch': 1.44}
{'loss': 4.0039, 'grad_norm': 2.0994653701782227, 'learning_rate': 8.578507578701903e-05, 'epoch': 1.44}
{'loss': 3.9186, 'grad_norm': 2.227602005004883, 'learning_rate': 8.566848037310531e-05, 'epoch': 1.44}
{'loss': 3.8645, 'grad_norm': 2.214059352874756, 'learning_rate': 8.55518849591916e-05, 'epoch': 1.44}
{'loss': 3.7082, 'grad_norm': 2.504425525665283, 'learning_rate': 8.543528954527787e-05, 'epoch': 1.44}
{'loss': 3.4233, 'grad_norm': 2.6856768131256104, 'learning_rate': 8.531869413136416e-05, 'epoch': 1.44}
{'loss': 3.9779, 'grad_norm': 1.5881470441818237, 'learning_rate': 8.520209871745045e-05, 'epoch': 1.44}
{'loss': 4.2265, 'grad_norm': 1.6265151500701904, 'learning_rate': 8.508550330353672e-05, 'epoch': 1.44}
{'loss': 4.2131, 'grad_norm': 1.6177986860275269, 'learning_rate': 8.4968907889623e-05, 'epoch': 1.44}
{'loss': 4.1527, 'grad_norm': 1.6605963706970215, 'learning_rate': 8.485231247570928e-05, 'epoch': 1.44}
{'loss': 4.0368, 'grad_norm': 1.728135347366333, 'learning_rate': 8.473571706179556e-05, 'epoch': 1.44}
{'loss': 4.0076, 'grad_norm': 1.871343731880188, 'learning_rate': 8.461912164788184e-05, 'epoch': 1.44}
{'loss': 4.0459, 'grad_norm': 1.8401323556900024, 'learning_rate': 8.450252623396813e-05, 'epoch': 1.45}
{'loss': 4.0353, 'grad_norm': 1.9797015190124512, 'learning_rate': 8.438593082005442e-05, 'epoch': 1.45}
{'loss': 3.9889, 'grad_norm': 2.2185776233673096, 'learning_rate': 8.426933540614069e-05, 'epoch': 1.45}
{'loss': 3.8014, 'grad_norm': 2.344343900680542, 'learning_rate': 8.415273999222696e-05, 'epoch': 1.45}
{'loss': 3.8405, 'grad_norm': 2.542870044708252, 'learning_rate': 8.403614457831324e-05, 'epoch': 1.45}
{'loss': 3.5951, 'grad_norm': 2.717100143432617, 'learning_rate': 8.391954916439952e-05, 'epoch': 1.45}
{'loss': 3.5189, 'grad_norm': 11.077301025390625, 'learning_rate': 8.380295375048581e-05, 'epoch': 1.45}
{'loss': 4.4186, 'grad_norm': 1.6354798078536987, 'learning_rate': 8.36863583365721e-05, 'epoch': 1.45}
{'loss': 4.2035, 'grad_norm': 1.5859447717666626, 'learning_rate': 8.356976292265837e-05, 'epoch': 1.45}
{'loss': 4.183, 'grad_norm': 1.6486945152282715, 'learning_rate': 8.345316750874466e-05, 'epoch': 1.45}
{'loss': 4.185, 'grad_norm': 1.7704882621765137, 'learning_rate': 8.333657209483092e-05, 'epoch': 1.45}
{'loss': 4.0607, 'grad_norm': 1.8084700107574463, 'learning_rate': 8.32199766809172e-05, 'epoch': 1.45}
{'loss': 3.9623, 'grad_norm': 1.8481731414794922, 'learning_rate': 8.310338126700349e-05, 'epoch': 1.45}
{'loss': 3.8671, 'grad_norm': 1.944444179534912, 'learning_rate': 8.298678585308978e-05, 'epoch': 1.46}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 7600/10442 [8:50:41<2:20:59,  2.98s/it][INFO|trainer.py:831] 2024-10-15 10:55:34,643 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 10:55:34,654 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 10:55:34,654 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 10:55:34,654 >>   Batch size = 32
{'eval_loss': 4.121748447418213, 'eval_wer': 1.250452028477794, 'eval_runtime': 336.3942, 'eval_samples_per_second': 4.73, 'eval_steps_per_second': 0.149, 'epoch': 1.46}
{'loss': 3.9741, 'grad_norm': 2.146449565887451, 'learning_rate': 8.287019043917605e-05, 'epoch': 1.46}
{'loss': 3.7453, 'grad_norm': 2.243180513381958, 'learning_rate': 8.275359502526234e-05, 'epoch': 1.46}
{'loss': 3.9181, 'grad_norm': 2.354142189025879, 'learning_rate': 8.263699961134862e-05, 'epoch': 1.46}
{'loss': 3.5937, 'grad_norm': 2.401904821395874, 'learning_rate': 8.252040419743488e-05, 'epoch': 1.46}
{'loss': 3.5634, 'grad_norm': 2.9569716453552246, 'learning_rate': 8.240380878352117e-05, 'epoch': 1.46}
{'loss': 3.9719, 'grad_norm': 1.569013237953186, 'learning_rate': 8.228721336960746e-05, 'epoch': 1.46}
{'loss': 4.2171, 'grad_norm': 1.622836947441101, 'learning_rate': 8.217061795569373e-05, 'epoch': 1.46}
{'loss': 4.1753, 'grad_norm': 1.5788514614105225, 'learning_rate': 8.205402254178002e-05, 'epoch': 1.46}
{'loss': 4.1236, 'grad_norm': 1.6916736364364624, 'learning_rate': 8.19374271278663e-05, 'epoch': 1.46}
{'loss': 4.1555, 'grad_norm': 1.7620359659194946, 'learning_rate': 8.182083171395258e-05, 'epoch': 1.46}
{'loss': 3.9664, 'grad_norm': 1.9342526197433472, 'learning_rate': 8.170423630003885e-05, 'epoch': 1.46}
{'loss': 4.0265, 'grad_norm': 1.960288643836975, 'learning_rate': 8.158764088612514e-05, 'epoch': 1.46}
{'loss': 4.1134, 'grad_norm': 2.037588596343994, 'learning_rate': 8.147104547221141e-05, 'epoch': 1.47}
{'loss': 3.9523, 'grad_norm': 2.0592353343963623, 'learning_rate': 8.13544500582977e-05, 'epoch': 1.47}
{'loss': 3.8341, 'grad_norm': 2.200627326965332, 'learning_rate': 8.123785464438399e-05, 'epoch': 1.47}
{'loss': 3.6774, 'grad_norm': 2.4776177406311035, 'learning_rate': 8.112125923047026e-05, 'epoch': 1.47}
{'loss': 3.4881, 'grad_norm': 2.4727933406829834, 'learning_rate': 8.100466381655655e-05, 'epoch': 1.47}
{'loss': 3.4587, 'grad_norm': 1.6502302885055542, 'learning_rate': 8.088806840264283e-05, 'epoch': 1.47}
{'loss': 4.3528, 'grad_norm': 1.6131482124328613, 'learning_rate': 8.077147298872909e-05, 'epoch': 1.47}
{'loss': 4.073, 'grad_norm': 1.5895355939865112, 'learning_rate': 8.065487757481538e-05, 'epoch': 1.47}
{'loss': 4.1511, 'grad_norm': 1.6086335182189941, 'learning_rate': 8.053828216090167e-05, 'epoch': 1.47}
{'loss': 4.119, 'grad_norm': 1.772128701210022, 'learning_rate': 8.042168674698794e-05, 'epoch': 1.47}
{'loss': 4.0465, 'grad_norm': 1.9421507120132446, 'learning_rate': 8.030509133307423e-05, 'epoch': 1.47}
{'loss': 3.9655, 'grad_norm': 1.8669339418411255, 'learning_rate': 8.018849591916051e-05, 'epoch': 1.47}
{'loss': 3.9175, 'grad_norm': 1.925155758857727, 'learning_rate': 8.00719005052468e-05, 'epoch': 1.47}
{'loss': 3.9711, 'grad_norm': 2.07096529006958, 'learning_rate': 7.995530509133306e-05, 'epoch': 1.48}
{'loss': 3.9758, 'grad_norm': 2.1329360008239746, 'learning_rate': 7.983870967741935e-05, 'epoch': 1.48}
{'loss': 3.8805, 'grad_norm': 2.260211706161499, 'learning_rate': 7.972211426350563e-05, 'epoch': 1.48}
{'loss': 3.6642, 'grad_norm': 2.751065731048584, 'learning_rate': 7.96055188495919e-05, 'epoch': 1.48}
{'loss': 3.6744, 'grad_norm': 3.097090721130371, 'learning_rate': 7.948892343567819e-05, 'epoch': 1.48}
{'loss': 3.9029, 'grad_norm': 1.7167178392410278, 'learning_rate': 7.937232802176448e-05, 'epoch': 1.48}
{'loss': 4.1293, 'grad_norm': 1.6319142580032349, 'learning_rate': 7.925573260785075e-05, 'epoch': 1.48}
{'loss': 4.2389, 'grad_norm': 1.7271313667297363, 'learning_rate': 7.913913719393703e-05, 'epoch': 1.48}
{'loss': 4.0936, 'grad_norm': 1.7547941207885742, 'learning_rate': 7.902254178002331e-05, 'epoch': 1.48}
{'loss': 4.0757, 'grad_norm': 1.751448631286621, 'learning_rate': 7.890594636610959e-05, 'epoch': 1.48}
{'loss': 4.13, 'grad_norm': 1.9119585752487183, 'learning_rate': 7.878935095219587e-05, 'epoch': 1.48}
{'loss': 4.0498, 'grad_norm': 2.07450270652771, 'learning_rate': 7.867275553828216e-05, 'epoch': 1.48}
{'loss': 3.9864, 'grad_norm': 1.982179045677185, 'learning_rate': 7.855616012436843e-05, 'epoch': 1.48}
{'loss': 3.9322, 'grad_norm': 2.0607168674468994, 'learning_rate': 7.843956471045472e-05, 'epoch': 1.49}
{'loss': 4.0044, 'grad_norm': 2.1489596366882324, 'learning_rate': 7.8322969296541e-05, 'epoch': 1.49}
{'loss': 3.8265, 'grad_norm': 2.4920666217803955, 'learning_rate': 7.820637388262727e-05, 'epoch': 1.49}
{'loss': 3.5272, 'grad_norm': 2.6188411712646484, 'learning_rate': 7.808977846871355e-05, 'epoch': 1.49}
{'loss': 3.4816, 'grad_norm': 1.5784997940063477, 'learning_rate': 7.797318305479984e-05, 'epoch': 1.49}
{'loss': 4.1906, 'grad_norm': 1.630776286125183, 'learning_rate': 7.785658764088611e-05, 'epoch': 1.49}
{'loss': 4.2916, 'grad_norm': 1.6262913942337036, 'learning_rate': 7.77399922269724e-05, 'epoch': 1.49}
{'loss': 4.2171, 'grad_norm': 1.6661521196365356, 'learning_rate': 7.762339681305869e-05, 'epoch': 1.49}
{'loss': 4.1077, 'grad_norm': 1.6859935522079468, 'learning_rate': 7.750680139914495e-05, 'epoch': 1.49}
{'loss': 4.0648, 'grad_norm': 1.8182986974716187, 'learning_rate': 7.739020598523123e-05, 'epoch': 1.49}
{'loss': 3.9654, 'grad_norm': 1.8077937364578247, 'learning_rate': 7.727361057131752e-05, 'epoch': 1.49}
{'loss': 4.0461, 'grad_norm': 1.91352117061615, 'learning_rate': 7.71570151574038e-05, 'epoch': 1.49}
{'loss': 3.8751, 'grad_norm': 2.0770277976989746, 'learning_rate': 7.704041974349008e-05, 'epoch': 1.49}
{'loss': 3.9107, 'grad_norm': 2.18119478225708, 'learning_rate': 7.692382432957637e-05, 'epoch': 1.5}
{'loss': 3.7445, 'grad_norm': 2.284944534301758, 'learning_rate': 7.680722891566266e-05, 'epoch': 1.5}
{'loss': 3.6806, 'grad_norm': 2.420853614807129, 'learning_rate': 7.669063350174893e-05, 'epoch': 1.5}
{'loss': 3.3199, 'grad_norm': 3.6344103813171387, 'learning_rate': 7.65740380878352e-05, 'epoch': 1.5}
{'loss': 3.9644, 'grad_norm': 1.636756181716919, 'learning_rate': 7.645744267392148e-05, 'epoch': 1.5}
{'loss': 4.2792, 'grad_norm': 1.5571513175964355, 'learning_rate': 7.634084726000776e-05, 'epoch': 1.5}