Singularity> python -m poetry run ./run_voxpopuli.sh
/auto/brno2/home/xhorni20/dp_mit/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
10/15/2024 00:47:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True
10/15/2024 00:47:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=32,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=input_length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./seq2seq_wav2vec2_bart-base/training/runs/Oct15_00-47-17_fau1.natur.cuni.cz,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=4,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=2.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=./seq2seq_wav2vec2_bart-base/training,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./seq2seq_wav2vec2_bart-base/training,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|configuration_utils.py:673] 2024-10-15 00:47:20,474 >> loading configuration file ./seq2seq_wav2vec2_bart-base/config.json
[INFO|configuration_utils.py:742] 2024-10-15 00:47:20,579 >> Model config SpeechEncoderDecoderConfig {
  "_name_or_path": "./seq2seq_wav2vec2_bart-base",
  "architectures": [
    "SpeechEncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "facebook/bart-base",
    "activation_dropout": 0.1,
    "activation_function": "gelu",
    "add_bias_logits": false,
    "add_cross_attention": true,
    "add_final_layer_norm": false,
    "architectures": [
      "BartModel"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "classif_dropout": 0.1,
    "classifier_dropout": 0.0,
    "cross_attention_hidden_size": null,
    "d_model": 768,
    "decoder_attention_heads": 12,
    "decoder_ffn_dim": 3072,
    "decoder_layerdrop": 0.0,
    "decoder_layers": 6,
    "decoder_start_token_id": 2,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": true,
    "encoder_attention_heads": 12,
    "encoder_ffn_dim": 3072,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 6,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": 0,
    "forced_eos_token_id": 2,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1",
      "2": "LABEL_2"
    },
    "init_std": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1,
      "LABEL_2": 2
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 1024,
    "min_length": 0,
    "model_type": "bart",
    "no_repeat_ngram_size": 3,
    "normalize_before": false,
    "normalize_embedding": true,
    "num_beam_groups": 1,
    "num_beams": 4,
    "num_hidden_layers": 6,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_embedding": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": {
      "summarization": {
        "length_penalty": 1.0,
        "max_length": 128,
        "min_length": 12,
        "num_beams": 4
      },
      "summarization_cnn": {
        "length_penalty": 2.0,
        "max_length": 142,
        "min_length": 56,
        "num_beams": 4
      },
      "summarization_xsum": {
        "length_penalty": 1.0,
        "max_length": 62,
        "min_length": 11,
        "num_beams": 6
      }
    },
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": "float32",
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 50265
  },
  "decoder_start_token_id": 0,
  "encoder": {
    "_name_or_path": "facebook/wav2vec2-base",
    "activation_dropout": 0.0,
    "adapter_attn_dim": null,
    "adapter_kernel_size": 3,
    "adapter_stride": 2,
    "add_adapter": true,
    "add_cross_attention": false,
    "apply_spec_augment": true,
    "architectures": [
      "Wav2Vec2ForPreTraining"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 1,
    "chunk_size_feed_forward": 0,
    "classifier_proj_size": 256,
    "codevector_dim": 256,
    "contrastive_logits_temperature": 0.1,
    "conv_bias": false,
    "conv_dim": [
      512,
      512,
      512,
      512,
      512,
      512,
      512
    ],
    "conv_kernel": [
      10,
      3,
      3,
      3,
      3,
      2,
      2
    ],
    "conv_stride": [
      5,
      2,
      2,
      2,
      2,
      2,
      2
    ],
    "cross_attention_hidden_size": null,
    "ctc_loss_reduction": "sum",
    "ctc_zero_infinity": false,
    "decoder_start_token_id": null,
    "diversity_loss_weight": 0.1,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "do_stable_layer_norm": false,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "feat_extract_activation": "gelu",
    "feat_extract_norm": "group",
    "feat_proj_dropout": 0.0,
    "feat_quantizer_dropout": 0.0,
    "final_dropout": 0.0,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "freeze_feat_extract_train": true,
    "hidden_act": "gelu",
    "hidden_dropout": 0.1,
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "layerdrop": 0.0,
    "length_penalty": 1.0,
    "mask_channel_length": 10,
    "mask_channel_min_space": 1,
    "mask_channel_other": 0.0,
    "mask_channel_prob": 0.0,
    "mask_channel_selection": "static",
    "mask_feature_length": 10,
    "mask_feature_min_masks": 0,
    "mask_feature_prob": 0.0,
    "mask_time_length": 10,
    "mask_time_min_masks": 2,
    "mask_time_min_space": 1,
    "mask_time_other": 0.0,
    "mask_time_prob": 0.0,
    "mask_time_selection": "static",
    "max_length": 20,
    "min_length": 0,
    "model_type": "wav2vec2",
    "no_mask_channel_overlap": false,
    "no_mask_time_overlap": false,
    "no_repeat_ngram_size": 0,
    "num_adapter_layers": 3,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_codevector_groups": 2,
    "num_codevectors_per_group": 320,
    "num_conv_pos_embedding_groups": 16,
    "num_conv_pos_embeddings": 128,
    "num_feat_extract_layers": 7,
    "num_hidden_layers": 12,
    "num_negatives": 100,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_size": 768,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "proj_codevector_dim": 256,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": null,
    "tdnn_dilation": [
      1,
      2,
      3,
      1,
      1
    ],
    "tdnn_dim": [
      512,
      512,
      512,
      512,
      1500
    ],
    "tdnn_kernel": [
      5,
      3,
      3,
      1,
      1
    ],
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_weighted_layer_sum": false,
    "vocab_size": 32,
    "xvector_output_dim": 512
  },
  "eos_token_id": 2,
  "is_encoder_decoder": true,
  "max_length": null,
  "model_type": "speech-encoder-decoder",
  "pad_token_id": 1,
  "processor_class": "Wav2Vec2Processor",
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": false
}

[INFO|feature_extraction_utils.py:547] 2024-10-15 00:47:20,590 >> loading configuration file ./seq2seq_wav2vec2_bart-base/preprocessor_config.json
[INFO|feature_extraction_utils.py:596] 2024-10-15 00:47:20,601 >> Feature extractor Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:47:20,718 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3729] 2024-10-15 00:47:20,946 >> loading weights file ./seq2seq_wav2vec2_bart-base/model.safetensors
[WARNING|logging.py:328] 2024-10-15 00:47:20,992 >> SpeechEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
[INFO|configuration_utils.py:1099] 2024-10-15 00:47:20,994 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 2,
  "pad_token_id": 1,
  "use_cache": false
}

[INFO|configuration_utils.py:1099] 2024-10-15 00:47:26,934 >> Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1
}

[WARNING|modeling_utils.py:4564] 2024-10-15 00:47:28,682 >> Some weights of the model checkpoint at ./seq2seq_wav2vec2_bart-base were not used when initializing SpeechEncoderDecoderModel: ['encoder.masked_spec_embed']
- This IS expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4582] 2024-10-15 00:47:28,682 >> All the weights of SpeechEncoderDecoderModel were initialized from the model checkpoint at ./seq2seq_wav2vec2_bart-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechEncoderDecoderModel for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-10-15 00:47:28,695 >> loading configuration file ./seq2seq_wav2vec2_bart-base/generation_config.json
[INFO|configuration_utils.py:1099] 2024-10-15 00:47:28,695 >> Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 0,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "max_length": 40,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "use_cache": false
}

preprocess train dataset (num_proc=32): 100%|██████████████████████| 182482/182482 [03:54<00:00, 778.08 examples/s]
preprocess train dataset (num_proc=32): 100%|███████████████████████████| 1753/1753 [00:30<00:00, 57.55 examples/s]
Filter (num_proc=32): 100%|█████████████████████████████████████| 182482/182482 [00:00<00:00, 387658.98 examples/s]
Filter (num_proc=32): 100%|███████████████████████████████████████████| 1753/1753 [00:00<00:00, 4177.61 examples/s]
[INFO|feature_extraction_utils.py:435] 2024-10-15 00:52:02,979 >> Feature extractor saved in ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:52:02,998 >> tokenizer config file saved in ./seq2seq_wav2vec2_bart-base/training/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:52:03,013 >> Special tokens file saved in ./seq2seq_wav2vec2_bart-base/training/special_tokens_map.json
[INFO|configuration_utils.py:410] 2024-10-15 00:52:03,230 >> Configuration saved in ./seq2seq_wav2vec2_bart-base/training/config.json
[INFO|image_processing_base.py:373] 2024-10-15 00:52:03,237 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|feature_extraction_utils.py:547] 2024-10-15 00:52:03,244 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|configuration_utils.py:673] 2024-10-15 00:52:03,259 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/config.json
[INFO|configuration_utils.py:742] 2024-10-15 00:52:03,262 >> Model config SpeechEncoderDecoderConfig {
  "_name_or_path": "./seq2seq_wav2vec2_bart-base/training",
  "architectures": [
    "SpeechEncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "facebook/bart-base",
    "activation_dropout": 0.1,
    "activation_function": "gelu",
    "add_bias_logits": false,
    "add_cross_attention": true,
    "add_final_layer_norm": false,
    "architectures": [
      "BartModel"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "classif_dropout": 0.1,
    "classifier_dropout": 0.0,
    "cross_attention_hidden_size": null,
    "d_model": 768,
    "decoder_attention_heads": 12,
    "decoder_ffn_dim": 3072,
    "decoder_layerdrop": 0.0,
    "decoder_layers": 6,
    "decoder_start_token_id": 2,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": true,
    "encoder_attention_heads": 12,
    "encoder_ffn_dim": 3072,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 6,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": 0,
    "forced_eos_token_id": 2,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1",
      "2": "LABEL_2"
    },
    "init_std": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1,
      "LABEL_2": 2
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 1024,
    "min_length": 0,
    "model_type": "bart",
    "no_repeat_ngram_size": 3,
    "normalize_before": false,
    "normalize_embedding": true,
    "num_beam_groups": 1,
    "num_beams": 4,
    "num_hidden_layers": 6,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_embedding": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": {
      "summarization": {
        "length_penalty": 1.0,
        "max_length": 128,
        "min_length": 12,
        "num_beams": 4
      },
      "summarization_cnn": {
        "length_penalty": 2.0,
        "max_length": 142,
        "min_length": 56,
        "num_beams": 4
      },
      "summarization_xsum": {
        "length_penalty": 1.0,
        "max_length": 62,
        "min_length": 11,
        "num_beams": 6
      }
    },
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": "float32",
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 50265
  },
  "decoder_start_token_id": 0,
  "encoder": {
    "_name_or_path": "facebook/wav2vec2-base",
    "activation_dropout": 0.0,
    "adapter_attn_dim": null,
    "adapter_kernel_size": 3,
    "adapter_stride": 2,
    "add_adapter": true,
    "add_cross_attention": false,
    "apply_spec_augment": true,
    "architectures": [
      "Wav2Vec2ForPreTraining"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": 1,
    "chunk_size_feed_forward": 0,
    "classifier_proj_size": 256,
    "codevector_dim": 256,
    "contrastive_logits_temperature": 0.1,
    "conv_bias": false,
    "conv_dim": [
      512,
      512,
      512,
      512,
      512,
      512,
      512
    ],
    "conv_kernel": [
      10,
      3,
      3,
      3,
      3,
      2,
      2
    ],
    "conv_stride": [
      5,
      2,
      2,
      2,
      2,
      2,
      2
    ],
    "cross_attention_hidden_size": null,
    "ctc_loss_reduction": "sum",
    "ctc_zero_infinity": false,
    "decoder_start_token_id": null,
    "diversity_loss_weight": 0.1,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "do_stable_layer_norm": false,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "exponential_decay_length_penalty": null,
    "feat_extract_activation": "gelu",
    "feat_extract_norm": "group",
    "feat_proj_dropout": 0.0,
    "feat_quantizer_dropout": 0.0,
    "final_dropout": 0.0,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "freeze_feat_extract_train": true,
    "hidden_act": "gelu",
    "hidden_dropout": 0.1,
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "layerdrop": 0.0,
    "length_penalty": 1.0,
    "mask_channel_length": 10,
    "mask_channel_min_space": 1,
    "mask_channel_other": 0.0,
    "mask_channel_prob": 0.0,
    "mask_channel_selection": "static",
    "mask_feature_length": 10,
    "mask_feature_min_masks": 0,
    "mask_feature_prob": 0.0,
    "mask_time_length": 10,
    "mask_time_min_masks": 2,
    "mask_time_min_space": 1,
    "mask_time_other": 0.0,
    "mask_time_prob": 0.0,
    "mask_time_selection": "static",
    "max_length": 20,
    "min_length": 0,
    "model_type": "wav2vec2",
    "no_mask_channel_overlap": false,
    "no_mask_time_overlap": false,
    "no_repeat_ngram_size": 0,
    "num_adapter_layers": 3,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_codevector_groups": 2,
    "num_codevectors_per_group": 320,
    "num_conv_pos_embedding_groups": 16,
    "num_conv_pos_embeddings": 128,
    "num_feat_extract_layers": 7,
    "num_hidden_layers": 12,
    "num_negatives": 100,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_size": 768,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "proj_codevector_dim": 256,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "suppress_tokens": null,
    "task_specific_params": null,
    "tdnn_dilation": [
      1,
      2,
      3,
      1,
      1
    ],
    "tdnn_dim": [
      512,
      512,
      512,
      512,
      1500
    ],
    "tdnn_kernel": [
      5,
      3,
      3,
      1,
      1
    ],
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_weighted_layer_sum": false,
    "vocab_size": 32,
    "xvector_output_dim": 512
  },
  "eos_token_id": 2,
  "is_encoder_decoder": true,
  "max_length": null,
  "model_type": "speech-encoder-decoder",
  "pad_token_id": 1,
  "processor_class": "Wav2Vec2Processor",
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": false
}

[INFO|feature_extraction_utils.py:547] 2024-10-15 00:52:03,347 >> loading configuration file ./seq2seq_wav2vec2_bart-base/training/preprocessor_config.json
[INFO|feature_extraction_utils.py:596] 2024-10-15 00:52:03,348 >> Feature extractor Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,362 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,362 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,362 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,363 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,363 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-10-15 00:52:03,363 >> loading file tokenizer_config.json
[INFO|processing_utils.py:744] 2024-10-15 00:52:03,618 >> Processor Wav2Vec2Processor:
- feature_extractor: Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}

- tokenizer: BartTokenizerFast(name_or_path='./seq2seq_wav2vec2_bart-base/training', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
        0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),
}

{
  "processor_class": "Wav2Vec2Processor"
}

[INFO|trainer.py:667] 2024-10-15 00:52:04,335 >> Using auto half precision backend
[INFO|trainer.py:831] 2024-10-15 00:52:04,462 >> The following columns in the training set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:2243] 2024-10-15 00:52:15,073 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-10-15 00:52:15,073 >>   Num examples = 167,046
[INFO|trainer.py:2245] 2024-10-15 00:52:15,073 >>   Num Epochs = 2
[INFO|trainer.py:2246] 2024-10-15 00:52:15,073 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:2249] 2024-10-15 00:52:15,073 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-10-15 00:52:15,073 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2251] 2024-10-15 00:52:15,073 >>   Total optimization steps = 20,882
[INFO|trainer.py:2252] 2024-10-15 00:52:15,074 >>   Number of trainable parameters = 196,895,616
  0%|                                                                                    | 0/20882 [00:00<?, ?it/s][WARNING|logging.py:313] 2024-10-15 00:52:17,932 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:328] 2024-10-15 00:52:24,292 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/auto/brno2/home/xhorni20/dp_mit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
[WARNING|modeling_utils.py:1279] 2024-10-15 00:52:26,381 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
{'loss': 11.1648, 'grad_norm': 9.277180671691895, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 10.8383, 'grad_norm': 5.704806804656982, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.0}
{'loss': 10.463, 'grad_norm': 4.692335605621338, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.0}
{'loss': 10.1558, 'grad_norm': 4.979343891143799, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.0}
{'loss': 9.7207, 'grad_norm': 5.817785739898682, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.0}
{'loss': 9.1869, 'grad_norm': 5.512307643890381, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.0}
{'loss': 9.037, 'grad_norm': 4.133026123046875, 'learning_rate': 4.2e-05, 'epoch': 0.0}
{'loss': 8.6705, 'grad_norm': 4.328378677368164, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.0}
  0%|                                                                         | 32/20882 [01:23<8:29:22,  1.47s/it][INFO|trainer.py:831] 2024-10-15 00:53:38,214 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 00:53:38,223 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 00:53:38,223 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 00:53:38,223 >>   Batch size = 16
{'eval_loss': 8.398536682128906, 'eval_wer': 1.0, 'eval_runtime': 248.5748, 'eval_samples_per_second': 6.4, 'eval_steps_per_second': 0.402, 'epoch': 0.0}
{'loss': 8.1775, 'grad_norm': 4.397696018218994, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.0}
{'loss': 7.8763, 'grad_norm': 4.688193321228027, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.0}
{'loss': 7.5963, 'grad_norm': 5.1522135734558105, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.0}
{'loss': 7.093, 'grad_norm': 6.886847019195557, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.0}
{'loss': 7.2485, 'grad_norm': 17.597572326660156, 'learning_rate': 7.649999999999999e-05, 'epoch': 0.0}
{'loss': 7.6539, 'grad_norm': 3.7847037315368652, 'learning_rate': 8.25e-05, 'epoch': 0.01}
{'loss': 7.4818, 'grad_norm': 3.7567360401153564, 'learning_rate': 8.849999999999998e-05, 'epoch': 0.01}
{'loss': 7.3718, 'grad_norm': 3.268570899963379, 'learning_rate': 9.449999999999999e-05, 'epoch': 0.01}
  0%|▏                                                                       | 64/20882 [06:32<14:46:52,  2.56s/it][INFO|trainer.py:831] 2024-10-15 00:58:47,597 >> The following columns in the evaluation set don't have a corresponding argument in `SpeechEncoderDecoderModel.forward` and have been ignored: input_length. If input_length are not expected by `SpeechEncoderDecoderModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:4021] 2024-10-15 00:58:47,609 >>
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-15 00:58:47,610 >>   Num examples = 1591
[INFO|trainer.py:4026] 2024-10-15 00:58:47,610 >>   Batch size = 16
{'eval_loss': 6.926307201385498, 'eval_wer': 0.9528477794101028, 'eval_runtime': 248.687, 'eval_samples_per_second': 6.398, 'eval_steps_per_second': 0.402, 'epoch': 0.01}
{'loss': 6.9559, 'grad_norm': 3.3849306106567383, 'learning_rate': 0.0001005, 'epoch': 0.01}
{'loss': 7.1277, 'grad_norm': 2.922618865966797, 'learning_rate': 0.00010649999999999999, 'epoch': 0.01}
{'loss': 6.9084, 'grad_norm': 2.968245506286621, 'learning_rate': 0.0001125, 'epoch': 0.01}
{'loss': 6.7035, 'grad_norm': 3.0863091945648193, 'learning_rate': 0.0001185, 'epoch': 0.01}
{'loss': 6.6863, 'grad_norm': 4.019412994384766, 'learning_rate': 0.0001245, 'epoch': 0.01}
{'loss': 6.5804, 'grad_norm': 3.317866325378418, 'learning_rate': 0.0001305, 'epoch': 0.01}
{'loss': 6.4861, 'grad_norm': 4.430680274963379, 'learning_rate': 0.00013649999999999998, 'epoch': 0.01}
{'loss': 6.2938, 'grad_norm': 6.039823532104492, 'learning_rate': 0.0001425, 'epoch': 0.01}