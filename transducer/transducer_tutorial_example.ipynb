{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}, \\t Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model, AutoFeatureExtractor\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 494, 768]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(dataset[3][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.transcriber(**inputs)\n",
    "\n",
    "list(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 4\n",
      "Sources shape: torch.Size([4, 15, 80])\n",
      "Source Lengths: tensor([12, 15, 11, 14])\n",
      "Targets shape: torch.Size([4, 10])\n",
      "Target Lengths: tensor([ 8, 10,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchaudio.models import emformer_rnnt_base\n",
    "\n",
    "emformer = emformer_rnnt_base(num_symbols=4097)\n",
    "\n",
    "\n",
    "# Parameters for batch size 4\n",
    "B = 4\n",
    "T = 15                # maximum source sequence length (including padding/right context)\n",
    "D = 80               # feature dimension for sources\n",
    "U = 10                # maximum target sequence length\n",
    "\n",
    "# Create a random sources tensor of shape (B, T, D)\n",
    "sources = torch.randn(B, T, D)\n",
    "sources_wav2vec2 = torch.randn(B, 320000)\n",
    "# source_lengths: valid source frame counts for each sample\n",
    "source_lengths = torch.tensor([12, 15, 11, 14])\n",
    "\n",
    "# For targets, assume our vocabulary size is 50\n",
    "vocab_size = 50\n",
    "targets = torch.randint(low=0, high=vocab_size, size=(B, U), dtype=torch.long)\n",
    "# Target lengths for each sample in the batch\n",
    "target_lengths = torch.tensor([8, 10, 9, 10])\n",
    "\n",
    "# Optional predictor_state; set to None for this test\n",
    "predictor_state = None\n",
    "\n",
    "# Display the shapes and values\n",
    "print(\"Batch Size 4\")\n",
    "print(\"Sources shape:\", sources.shape)           # Expected shape: (4, 15, 40)\n",
    "print(\"Source Lengths:\", source_lengths)           # Expected: e.g., [12, 15, 11, 14]\n",
    "print(\"Targets shape:\", targets.shape)             # Expected shape: (4, 10)\n",
    "print(\"Target Lengths:\", target_lengths)           # Expected: e.g., [8, 10, 9, 10]\n",
    "\n",
    "output, source_lengths, target_lengths, pred_state = emformer(sources, source_lengths, targets, target_lengths)\n",
    "output_wav2vec2, source_lengths_wav2vec2, target_lengths_wav2vec2, pred_state_wav2vec2 = model(sources_wav2vec2, None, targets, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 2, 10, 4097])\n",
      "Source Lengths: tensor([3, 3, 2, 3])\n",
      "Target Lengths: tensor([ 8, 10,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Source Lengths:\", source_lengths)\n",
    "print(\"Target Lengths:\", target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 999, 10, 4097])\n",
      "Source Lengths: tensor([999, 999, 999, 999], dtype=torch.int32)\n",
      "Target Lengths: tensor([ 8, 10,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", output_wav2vec2.shape)\n",
    "print(\"Source Lengths:\", source_lengths_wav2vec2)\n",
    "print(\"Target Lengths:\", target_lengths_wav2vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk/conda-envs/transducer/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "from torchaudio.models.rnnt import _Predictor, _Joiner\n",
    "from torchaudio.models import RNNT\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "class Wav2Vec2HiddenStates(Wav2Vec2Model):\n",
    "    \"\"\"\n",
    "    Wav2Vec2Model with a modified forward method to return just last hidden state.\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self,\n",
    "        input_values: Optional[torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mask_time_indices: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        outputs = super().forward(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class Wav2vec2RNNT(RNNT):\n",
    "    @torch.jit.export\n",
    "    def transcribe_streaming(\n",
    "        self,\n",
    "        sources: torch.Tensor,\n",
    "        source_lengths,\n",
    "        state: Optional[List[List[torch.Tensor]]],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        raise NotImplementedError(\"No streaming for Wav2Vec2Model.\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sources: torch.Tensor,\n",
    "        source_lengths,\n",
    "        targets: torch.Tensor,\n",
    "        target_lengths: torch.Tensor,\n",
    "        predictor_state: Optional[List[List[torch.Tensor]]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        source_encodings = self.transcriber(\n",
    "            input_values=sources,\n",
    "        )\n",
    "        source_lengths = torch.full((source_encodings.size(0),), source_encodings.size(1), dtype=torch.int32)\n",
    "        target_encodings, target_lengths, predictor_state = self.predictor(\n",
    "            input=targets,\n",
    "            lengths=target_lengths,\n",
    "            state=predictor_state,\n",
    "        )\n",
    "        output, source_lengths, target_lengths = self.joiner(\n",
    "            source_encodings=source_encodings,\n",
    "            source_lengths=source_lengths,\n",
    "            target_encodings=target_encodings,\n",
    "            target_lengths=target_lengths,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            source_lengths,\n",
    "            target_lengths,\n",
    "            predictor_state,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "def wav2vec2_rnnt_model(\n",
    "    *,\n",
    "    encoding_dim: int,\n",
    "    num_symbols: int,\n",
    "    symbol_embedding_dim: int,\n",
    "    num_lstm_layers: int,\n",
    "    lstm_layer_norm: bool,\n",
    "    lstm_layer_norm_epsilon: float,\n",
    "    lstm_dropout: float,\n",
    ") -> Wav2vec2RNNT:\n",
    "    encoder = Wav2Vec2HiddenStates.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    predictor = _Predictor(\n",
    "        num_symbols,\n",
    "        encoding_dim,\n",
    "        symbol_embedding_dim=symbol_embedding_dim,\n",
    "        num_lstm_layers=num_lstm_layers,\n",
    "        lstm_hidden_dim=symbol_embedding_dim,\n",
    "        lstm_layer_norm=lstm_layer_norm,\n",
    "        lstm_layer_norm_epsilon=lstm_layer_norm_epsilon,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "    )\n",
    "    joiner = _Joiner(encoding_dim, num_symbols)\n",
    "    return Wav2vec2RNNT(encoder, predictor, joiner)\n",
    "\n",
    "model = wav2vec2_rnnt_model(\n",
    "    encoding_dim=768, # Wav2Vec2-base output dim\n",
    "    num_symbols=4097,\n",
    "    symbol_embedding_dim=512,\n",
    "    num_lstm_layers=3,\n",
    "    lstm_layer_norm=True,\n",
    "    lstm_layer_norm_epsilon=1e-3,\n",
    "    lstm_dropout=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbwgR5UdNkkm"
   },
   "source": [
    "# Transducer implementation in PyTorch\n",
    "\n",
    "*by Loren Lugosch*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBlJNKsjTtaZ"
   },
   "source": [
    "\n",
    "In this notebook, we will implement a Transducer sequence-to-sequence model for inserting missing vowels into a sentence (\"Hll, Wrld\" --> \"Hello, World\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-iHU02C7fAj",
    "outputId": "6ce0dae6-5036-4fb4-fe33-45aceec263a6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "\n",
    "# # Some training data.\n",
    "# # Poor Tolstoy, once again reduced to grist for the neural network mill!\n",
    "# !wget https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTfRgwxmjv1B"
   },
   "source": [
    "# Building blocks\n",
    "\n",
    "First, we will define the encoder, predictor, and joiner using standard neural nets.\n",
    "\n",
    "<img src=\"https://lorenlugosch.github.io/images/transducer/transducer-model.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7mLFyUG7kJH"
   },
   "outputs": [],
   "source": [
    "NULL_INDEX = 0\n",
    "\n",
    "encoder_dim = 1024\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MABMTjrGY4vz"
   },
   "source": [
    "The encoder is any network that can take as input a variable-length sequence: so, RNNs, CNNs, and self-attention/Transformer encoders will all work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE7j2T5EY33-"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=encoder_dim,\n",
    "            hidden_size=encoder_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(encoder_dim * 2, joiner_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.embed(out)\n",
    "        out = self.rnn(out)[0]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRknN6QRY9-g"
   },
   "source": [
    "The predictor is any _causal_ network (= can't look at the future): in other words, unidirectional RNNs, causal convolutions, or masked self-attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPARF5LmY7-r"
   },
   "outputs": [],
   "source": [
    "class Predictor(torch.nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_outputs, predictor_dim)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=predictor_dim,\n",
    "            hidden_size=predictor_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "\n",
    "        # Updated: initial_state now has shape (num_layers, predictor_dim)\n",
    "        self.initial_state = torch.nn.Parameter(torch.randn(self.rnn.num_layers, predictor_dim))\n",
    "        self.start_symbol = NULL_INDEX  # Using null index for embedding start symbol\n",
    "\n",
    "    def forward_one_step(self, input, previous_state):\n",
    "        # Embed input and add sequence dimension\n",
    "        embedding = self.embed(input).unsqueeze(1)  # Shape: (batch_size, 1, predictor_dim)\n",
    "        # Pass through GRU; GRU returns (output, hidden_state)\n",
    "        output, new_state = self.rnn(embedding, previous_state)\n",
    "        # Remove sequence dimension before the linear layer\n",
    "        out = self.linear(output.squeeze(1))\n",
    "        return out, new_state\n",
    "\n",
    "    def forward(self, y):\n",
    "        batch_size = y.shape[0]\n",
    "        U = y.shape[1]\n",
    "        outs = []\n",
    "        # Expand initial_state to shape (num_layers, batch_size, predictor_dim)\n",
    "        state = self.initial_state.unsqueeze(1).expand(-1, batch_size, -1).to(y.device)\n",
    "        for u in range(U + 1):  # U+1 steps to include final timestep\n",
    "            if u == 0:\n",
    "                decoder_input = torch.tensor(\n",
    "                    [self.start_symbol] * batch_size, device=y.device\n",
    "                )\n",
    "            else:\n",
    "                decoder_input = y[:, u - 1]\n",
    "            out, state = self.forward_one_step(decoder_input, state)\n",
    "            outs.append(out)\n",
    "        out = torch.stack(outs, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHPZ3PATZEAW"
   },
   "source": [
    "The joiner is a feedforward network/MLP with one hidden layer applied independently to each $(t,u)$ index.\n",
    "\n",
    "(The linear part of the hidden layer is contained in the encoder and predictor, so we just do the nonlinearity here and then the output layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlzca1orZDLa"
   },
   "outputs": [],
   "source": [
    "class Joiner(torch.nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Joiner, self).__init__()\n",
    "        self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "    def forward(self, encoder_out, predictor_out):\n",
    "        out = encoder_out + predictor_out\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_-INbhSTApv"
   },
   "source": [
    "# Transducer model + loss function\n",
    "\n",
    "Using the encoder, predictor, and joiner, we will implement the Transducer model and its loss function.\n",
    "\n",
    "<img src=\"https://lorenlugosch.github.io/images/transducer/forward-messages.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdcKwA_lkzxJ"
   },
   "source": [
    "We can use a simple PyTorch implementation of the loss function, relying on automatic differentiation to give us gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYSagKi-gHM4"
   },
   "outputs": [],
   "source": [
    "class Transducer(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Transducer, self).__init__()\n",
    "        self.encoder = Encoder(num_inputs)\n",
    "        self.predictor = Predictor(num_outputs)\n",
    "        self.joiner = Joiner(num_outputs)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "        # elif torch.backends.mps.is_available():\n",
    "        #     self.device = \"mps\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        self.to(self.device)\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "    def compute_forward_prob(self, joiner_out, T, U, y):\n",
    "        \"\"\"\n",
    "        joiner_out: tensor of shape (B, T_max, U_max+1, #labels)\n",
    "        T: list of input lengths\n",
    "        U: list of output lengths\n",
    "        y: label tensor (B, U_max+1)\n",
    "        \"\"\"\n",
    "        B = joiner_out.shape[0]\n",
    "        T_max = joiner_out.shape[1]\n",
    "        U_max = joiner_out.shape[2] - 1\n",
    "        log_alpha = torch.zeros(B, T_max, U_max + 1, device=self.device)\n",
    "        for t in range(T_max):\n",
    "            for u in range(U_max + 1):\n",
    "                if u == 0:\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = 0.0\n",
    "\n",
    "                    else:  # t > 0\n",
    "                        log_alpha[:, t, u] = (\n",
    "                            log_alpha[:, t - 1, u] + joiner_out[:, t - 1, 0, NULL_INDEX]\n",
    "                        )\n",
    "\n",
    "                else:  # u > 0\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = log_alpha[:, t, u - 1] + torch.gather(\n",
    "                            joiner_out[:, t, u - 1],\n",
    "                            dim=1,\n",
    "                            index=y[:, u - 1].view(-1, 1),\n",
    "                        ).reshape(-1)\n",
    "\n",
    "                    else:  # t > 0\n",
    "                        log_alpha[:, t, u] = torch.logsumexp(\n",
    "                            torch.stack(\n",
    "                                [\n",
    "                                    log_alpha[:, t - 1, u]\n",
    "                                    + joiner_out[:, t - 1, u, NULL_INDEX],\n",
    "                                    log_alpha[:, t, u - 1]\n",
    "                                    + torch.gather(\n",
    "                                        joiner_out[:, t, u - 1],\n",
    "                                        dim=1,\n",
    "                                        index=y[:, u - 1].view(-1, 1),\n",
    "                                    ).reshape(-1),\n",
    "                                ]\n",
    "                            ),\n",
    "                            dim=0,\n",
    "                        )\n",
    "\n",
    "        log_probs = []\n",
    "        for b in range(B):\n",
    "            log_prob = (\n",
    "                log_alpha[b, T[b] - 1, U[b]] + joiner_out[b, T[b] - 1, U[b], NULL_INDEX]\n",
    "            )\n",
    "            log_probs.append(log_prob)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        return log_probs\n",
    "\n",
    "    def compute_loss(self, x, y, T, U):\n",
    "        encoder_out = self.encoder.forward(x)\n",
    "        predictor_out = self.predictor.forward(y)\n",
    "        joiner_out = self.joiner.forward(\n",
    "            encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)\n",
    "        ).log_softmax(3)\n",
    "        loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK0c2S2xaARd"
   },
   "source": [
    "Let's first verify that the forward algorithm actually correctly computes the sum (in log space, the [logsumexp](https://lorenlugosch.github.io/posts/2020/06/logsumexp/)) of all possible alignments, using a short input/output pair for which computing all possible alignments is feasible.\n",
    "\n",
    "<img src=\"https://lorenlugosch.github.io/images/transducer/cat-align-1.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWtkoXH6U8Pm"
   },
   "outputs": [],
   "source": [
    "def compute_single_alignment_prob(self, encoder_out, predictor_out, T, U, z, y):\n",
    "    \"\"\"\n",
    "    Computes the probability of one alignment, z.\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    u = 0\n",
    "    t_u_indices = []\n",
    "    y_expanded = []\n",
    "    for step in z:\n",
    "        t_u_indices.append((t, u))\n",
    "        if step == 0:  # right (null)\n",
    "            y_expanded.append(NULL_INDEX)\n",
    "            t += 1\n",
    "        if step == 1:  # down (label)\n",
    "            y_expanded.append(y[u])\n",
    "            u += 1\n",
    "    t_u_indices.append((T - 1, U))\n",
    "    y_expanded.append(NULL_INDEX)\n",
    "\n",
    "    t_indices = [t for (t, u) in t_u_indices]\n",
    "    u_indices = [u for (t, u) in t_u_indices]\n",
    "    encoder_out_expanded = encoder_out[t_indices]\n",
    "    predictor_out_expanded = predictor_out[u_indices]\n",
    "    joiner_out = self.joiner.forward(\n",
    "        encoder_out_expanded, predictor_out_expanded\n",
    "    ).log_softmax(1)\n",
    "    logprob = -torch.nn.functional.nll_loss(\n",
    "        input=joiner_out,\n",
    "        target=torch.tensor(y_expanded).long().to(self.device),\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "    return logprob\n",
    "\n",
    "\n",
    "Transducer.compute_single_alignment_prob = compute_single_alignment_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional import rnnt_loss\n",
    "\n",
    "\n",
    "def compute_loss(self, x, y, T, U):\n",
    "    encoder_out = self.encoder(x)\n",
    "    predictor_out = self.predictor(y)\n",
    "    # Compute the joiner logits without applying log_softmax manually:\n",
    "    logits = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1))\n",
    "    logits = logits.float()\n",
    "    # Ensure lengths are on the same device as logits.\n",
    "    T = T.to(logits.device)\n",
    "    U = U.to(logits.device)\n",
    "    loss = rnnt_loss(\n",
    "        logits,  # shape: (batch, T_max, U_max+1, num_outputs)\n",
    "        y.to(torch.int32),  # shape: (batch, U_max+?); targets padded with zeros\n",
    "        T.to(torch.int32),  # tensor of input lengths\n",
    "        U.to(torch.int32),  # tensor of target lengths\n",
    "        blank=NULL_INDEX,  # using your defined NULL_INDEX (should match the blank label)\n",
    "        clamp=-1,  # default clamp value (no clamping)\n",
    "        reduction=\"mean\",\n",
    "        fused_log_softmax=True,\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "Transducer.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8xzM0dZfea9",
    "outputId": "241648b8-5484-4220-d6a5-469bb05f5253"
   },
   "outputs": [],
   "source": [
    "# Generate example inputs/outputs\n",
    "num_outputs = len(string.ascii_uppercase) + 1  # [null, A, B, ... Z]\n",
    "model = Transducer(1, num_outputs)\n",
    "y_letters = \"CAT\"\n",
    "y = (\n",
    "    torch.tensor([string.ascii_uppercase.index(l) + 1 for l in y_letters])\n",
    "    .unsqueeze(0)\n",
    "    .to(model.device)\n",
    ")\n",
    "T = torch.tensor([4])\n",
    "U = torch.tensor([len(y_letters)])\n",
    "B = 1\n",
    "\n",
    "\n",
    "encoder_out = torch.randn(B, T, joiner_dim).to(model.device)\n",
    "predictor_out = torch.randn(B, U + 1, joiner_dim).to(model.device)\n",
    "joiner_out = model.joiner.forward(\n",
    "    encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)\n",
    ").log_softmax(3)\n",
    "\n",
    "#######################################################\n",
    "# Compute loss by enumerating all possible alignments #\n",
    "#######################################################\n",
    "all_permutations = list(itertools.permutations([0] * (T - 1) + [1] * U))\n",
    "all_distinct_permutations = list(Counter(all_permutations).keys())\n",
    "alignment_probs = []\n",
    "for z in all_distinct_permutations:\n",
    "    alignment_prob = model.compute_single_alignment_prob(\n",
    "        encoder_out[0], predictor_out[0], T.item(), U.item(), z, y[0]\n",
    "    )\n",
    "    alignment_probs.append(alignment_prob)\n",
    "loss_enumerate = -torch.tensor(alignment_probs).logsumexp(0)\n",
    "\n",
    "#######################################################\n",
    "# Compute loss using the forward algorithm            #\n",
    "#######################################################\n",
    "loss_forward = -model.compute_forward_prob(joiner_out, T, U, y)\n",
    "\n",
    "#######################################################\n",
    "# Compute loss using torchaudio implementation        #\n",
    "#######################################################\n",
    "\n",
    "\n",
    "loss_torchaudio = rnnt_loss(\n",
    "    joiner_out,  # shape: (B, T, U+1, num_outputs)\n",
    "    y.to(torch.int32).to(joiner_out.device),  # shape: (B, U)\n",
    "    T.to(torch.int32).to(joiner_out.device),  # tensor of input lengths\n",
    "    U.to(torch.int32).to(joiner_out.device),  # tensor of target lengths\n",
    "    blank=NULL_INDEX,  # using the defined NULL_INDEX\n",
    "    reduction=\"mean\",\n",
    "    fused_log_softmax=True,\n",
    ")\n",
    "\n",
    "print(\"Loss computed by enumerating all possible alignments: \", loss_enumerate)\n",
    "print(\"Loss computed using the forward algorithm: \", loss_forward)\n",
    "print(\"Loss computed using torchaudio implementation: \", loss_torchaudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSBAwQONf3z9"
   },
   "source": [
    "Now let's add the greedy search algorithm for predicting an output sequence.\n",
    "\n",
    "(Note that I've assumed we're using RNNs for the predictor here. You would have to modify this code a bit if you want to use convolutions/self-attention instead.) \n",
    "<br/><br/>\n",
    "<img src=\"https://lorenlugosch.github.io/images/transducer/greedy-search.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0xeyb7Jf18_"
   },
   "outputs": [],
   "source": [
    "def greedy_search(self, x, T):\n",
    "    y_batch = []\n",
    "    B = len(x)\n",
    "    encoder_out = self.encoder.forward(x)\n",
    "    U_max = 200\n",
    "    for b in range(B):\n",
    "        t = 0\n",
    "        u = 0\n",
    "        y = [self.predictor.start_symbol]\n",
    "        predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "        while t < T[b] and u < U_max:\n",
    "            predictor_input = torch.tensor([y[-1]], device=x.device)\n",
    "            g_u, predictor_state = self.predictor.forward_one_step(\n",
    "                predictor_input, predictor_state\n",
    "            )\n",
    "            f_t = encoder_out[b, t]\n",
    "            h_t_u = self.joiner.forward(f_t, g_u)\n",
    "            argmax = h_t_u.max(-1)[1].item()\n",
    "            if argmax == NULL_INDEX:\n",
    "                t += 1\n",
    "            else:  # argmax == a label\n",
    "                u += 1\n",
    "                y.append(argmax)\n",
    "        y_batch.append(y[1:])  # remove start symbol\n",
    "    return y_batch\n",
    "\n",
    "\n",
    "Transducer.greedy_search = greedy_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82XU9-gr3goI"
   },
   "source": [
    "The code above will work, but training will be very slow because the Transducer loss is written in pure Python. You can use the fast implementation from SpeechBrain instead by running the block below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff9raB0jVGzN"
   },
   "source": [
    "# Some utilities\n",
    "\n",
    "Here we will add a bit of boilerplate code for training and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b17OQm4WdVy",
    "outputId": "25498f94-9543-40f8-dd1e-fbfd7857debf"
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lines, batch_size):\n",
    "        lines = list(filter((\"\\n\").__ne__, lines))\n",
    "\n",
    "        self.lines = lines  # list of strings\n",
    "        collate = Collate()\n",
    "        self.loader = torch.utils.data.DataLoader(\n",
    "            self, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.lines[idx].replace(\"\\n\", \"\")\n",
    "        line = unidecode.unidecode(line)  # remove special characters\n",
    "        x = \"\".join(\n",
    "            c for c in line if c not in \"AEIOUaeiou\"\n",
    "        )  # remove vowels from input\n",
    "        y = line\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "def encode_string(s):\n",
    "    for c in s:\n",
    "        if c not in string.printable:\n",
    "            print(s)\n",
    "    return [string.printable.index(c) + 1 for c in s]\n",
    "\n",
    "\n",
    "def decode_labels(l):\n",
    "    return \"\".join([string.printable[c - 1] for c in l])\n",
    "\n",
    "\n",
    "class Collate:\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        batch: list of tuples (input string, output string)\n",
    "        Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        y = []\n",
    "        batch_size = len(batch)\n",
    "        for index in range(batch_size):\n",
    "            x_, y_ = batch[index]\n",
    "            x.append(encode_string(x_))\n",
    "            y.append(encode_string(y_))\n",
    "\n",
    "        # pad all sequences to have same length\n",
    "        T = [len(x_) for x_ in x]\n",
    "        U = [len(y_) for y_ in y]\n",
    "        T_max = max(T)\n",
    "        U_max = max(U)\n",
    "        for index in range(batch_size):\n",
    "            x[index] += [NULL_INDEX] * (T_max - len(x[index]))\n",
    "            x[index] = torch.tensor(x[index])\n",
    "            y[index] += [NULL_INDEX] * (U_max - len(y[index]))\n",
    "            y[index] = torch.tensor(y[index])\n",
    "\n",
    "        # stack into single tensor\n",
    "        x = torch.stack(x).to(torch.int32)\n",
    "        y = torch.stack(y).to(torch.int32)\n",
    "        T = torch.tensor(T).to(torch.int32)\n",
    "        U = torch.tensor(U).to(torch.int32)\n",
    "\n",
    "        return (x, y, T, U)\n",
    "\n",
    "\n",
    "with open(\"war_and_peace.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "end = round(0.9 * len(lines))\n",
    "train_lines = lines[:end]\n",
    "test_lines = lines[end:]\n",
    "train_set = TextDataset(train_lines, batch_size=64)  # 8)\n",
    "test_set = TextDataset(test_lines, batch_size=64)  # 8)\n",
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaZEQYzfFEQ0"
   },
   "outputs": [],
   "source": [
    "import torch.amp as amp  # ensure we import amp\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self, dataset, print_interval=20, use_bf16=False, device=None):\n",
    "        train_loss = 0\n",
    "        num_samples = 0\n",
    "        self.model.train()\n",
    "\n",
    "        pbar = tqdm(dataset.loader)\n",
    "        for idx, batch in enumerate(pbar):\n",
    "            x, y, T, U = batch\n",
    "            # Use non_blocking transfers (ensure your DataLoader uses pin_memory=True)\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True).to(torch.int32)\n",
    "            T = T.to(device, non_blocking=True).to(torch.int32)\n",
    "            U = U.to(device, non_blocking=True).to(torch.int32)\n",
    "            batch_size = len(x)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # Forward pass under autocast for BF16 support if enabled.\n",
    "            with amp.autocast(\n",
    "                device_type=device,\n",
    "                dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "            ):\n",
    "                joint_out, output_lengths, target_lengths, _ = self.model(\n",
    "                    x,          # sources\n",
    "                    T,          # source_lengths\n",
    "                    y,          # targets\n",
    "                    U           # target_lengths\n",
    "                )\n",
    "                output_lengths = output_lengths.clamp(max=joint_out.size(1))\n",
    "\n",
    "                loss = rnnt_loss(\n",
    "                    joint_out.float(),              # shape: (batch, T_max, U_max+1, num_outputs)\n",
    "                    y,                              # targets (batch, U_max+?)\n",
    "                    output_lengths,                              # tensor of input lengths\n",
    "                    target_lengths,                              # tensor of target lengths\n",
    "                    blank=NULL_INDEX,               # defined blank label\n",
    "                    reduction=\"mean\",\n",
    "                    fused_log_softmax=True,\n",
    "                )\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            pbar.set_description(\"%.2f\" % loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * batch_size\n",
    "\n",
    "        train_loss /= num_samples\n",
    "        return train_loss\n",
    "\n",
    "    def test(self, dataset, print_interval=1):\n",
    "        test_loss = 0\n",
    "        num_samples = 0\n",
    "        self.model.eval()\n",
    "        pbar = tqdm(dataset.loader)\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(pbar):\n",
    "                x, y, T, U = batch\n",
    "                x = x.to(self.model.device)\n",
    "                y = y.to(self.model.device)\n",
    "                batch_size = len(x)\n",
    "                num_samples += batch_size\n",
    "                loss = self.model.compute_loss(x, y, T, U)\n",
    "                pbar.set_description(\"%.2f\" % loss.item())\n",
    "                test_loss += loss.item() * batch_size\n",
    "        test_loss /= num_samples\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}, \\t Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models import Emformer\n",
    "from torchaudio.models.rnnt import _TimeReduction, _Transcriber, RNNT, _Predictor\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class _EmformerEncoderEmbed(torch.nn.Module, _Transcriber):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_tokens: int,              # New parameter: vocabulary size (e.g., num_chars + 1)\n",
    "        input_dim: int,               # Dimension of the token embeddings (should match expected feature dim)\n",
    "        output_dim: int,\n",
    "        segment_length: int,\n",
    "        right_context_length: int,\n",
    "        time_reduction_input_dim: int,\n",
    "        time_reduction_stride: int,\n",
    "        transformer_num_heads: int,\n",
    "        transformer_ffn_dim: int,\n",
    "        transformer_num_layers: int,\n",
    "        transformer_left_context_length: int,\n",
    "        transformer_dropout: float = 0.0,\n",
    "        transformer_activation: str = \"relu\",\n",
    "        transformer_max_memory_size: int = 0,\n",
    "        transformer_weight_init_scale_strategy: str = \"depthwise\",\n",
    "        transformer_tanh_on_mem: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Add an embedding layer to map token indices to continuous embeddings.\n",
    "        self.embedding = torch.nn.Embedding(num_tokens, input_dim)\n",
    "        self.input_linear = torch.nn.Linear(\n",
    "            input_dim,\n",
    "            time_reduction_input_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.time_reduction = _TimeReduction(time_reduction_stride)\n",
    "        transformer_input_dim = time_reduction_input_dim * time_reduction_stride\n",
    "        self.transformer = Emformer(\n",
    "            transformer_input_dim,\n",
    "            transformer_num_heads,\n",
    "            transformer_ffn_dim,\n",
    "            transformer_num_layers,\n",
    "            segment_length // time_reduction_stride,\n",
    "            dropout=transformer_dropout,\n",
    "            activation=transformer_activation,\n",
    "            left_context_length=transformer_left_context_length,\n",
    "            right_context_length=right_context_length // time_reduction_stride,\n",
    "            max_memory_size=transformer_max_memory_size,\n",
    "            weight_init_scale_strategy=transformer_weight_init_scale_strategy,\n",
    "            tanh_on_mem=transformer_tanh_on_mem,\n",
    "        )\n",
    "        self.output_linear = torch.nn.Linear(transformer_input_dim, output_dim)\n",
    "        self.layer_norm = torch.nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Assume input is token indices of shape (B, T)\n",
    "        embedded = self.embedding(input)  # Convert token indices to embeddings, shape: (B, T, input_dim)\n",
    "        input_linear_out = self.input_linear(embedded)\n",
    "        time_reduction_out, time_reduction_lengths = self.time_reduction(input_linear_out, lengths)\n",
    "        transformer_out, transformer_lengths = self.transformer(time_reduction_out, time_reduction_lengths)\n",
    "        output_linear_out = self.output_linear(transformer_out)\n",
    "        layer_norm_out = self.layer_norm(output_linear_out)\n",
    "        return layer_norm_out, transformer_lengths\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        lengths: torch.Tensor,\n",
    "        states: Optional[List[List[torch.Tensor]]],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        # Same modification for inference: embed token indices before further processing.\n",
    "        embedded = self.embedding(input)\n",
    "        input_linear_out = self.input_linear(embedded)\n",
    "        time_reduction_out, time_reduction_lengths = self.time_reduction(input_linear_out, lengths)\n",
    "        (\n",
    "            transformer_out,\n",
    "            transformer_lengths,\n",
    "            transformer_states,\n",
    "        ) = self.transformer.infer(time_reduction_out, time_reduction_lengths, states)\n",
    "        output_linear_out = self.output_linear(transformer_out)\n",
    "        layer_norm_out = self.layer_norm(output_linear_out)\n",
    "        return layer_norm_out, transformer_lengths, transformer_states\n",
    "\n",
    "class _JoinerPad(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, activation: str = \"relu\") -> None:\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation {activation}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source_encodings: torch.Tensor,   # shape: (B, T, D)\n",
    "        source_lengths: torch.Tensor,\n",
    "        target_encodings: torch.Tensor,   # shape: (B, U, D)\n",
    "        target_lengths: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Pad target_encodings with an extra blank token representation.\n",
    "        # This creates a tensor of shape (B, 1, D) with zeros.\n",
    "        blank = torch.zeros(target_encodings.size(0), 1, target_encodings.size(2), \n",
    "                            device=target_encodings.device, dtype=target_encodings.dtype)\n",
    "        # Concatenate the blank token to the target encodings.\n",
    "        target_encodings_padded = torch.cat([target_encodings, blank], dim=1)  # shape: (B, U+1, D)\n",
    "        \n",
    "        # Unsqueeze and add to get joint representations.\n",
    "        joint_encodings = source_encodings.unsqueeze(2) + target_encodings_padded.unsqueeze(1)  # shape: (B, T, U+1, D)\n",
    "        activation_out = self.activation(joint_encodings)\n",
    "        output = self.linear(activation_out)\n",
    "        return output, source_lengths, target_lengths\n",
    "\n",
    "\n",
    "input_dim=80\n",
    "encoding_dim=1024\n",
    "num_symbols=len(string.printable)\n",
    "segment_length=16\n",
    "right_context_length=4\n",
    "time_reduction_input_dim=128\n",
    "time_reduction_stride=4\n",
    "transformer_num_heads=8\n",
    "transformer_ffn_dim=2048\n",
    "transformer_num_layers=20\n",
    "transformer_dropout=0.1\n",
    "transformer_activation=\"gelu\"\n",
    "transformer_left_context_length=30\n",
    "transformer_max_memory_size=0\n",
    "transformer_weight_init_scale_strategy=\"depthwise\"\n",
    "transformer_tanh_on_mem=True\n",
    "symbol_embedding_dim=512\n",
    "num_lstm_layers=3\n",
    "lstm_layer_norm=True\n",
    "lstm_layer_norm_epsilon=1e-3\n",
    "lstm_dropout=0.3\n",
    "    \n",
    "encoder = _EmformerEncoderEmbed(\n",
    "        num_tokens=num_symbols,\n",
    "        input_dim=input_dim,\n",
    "        output_dim=encoding_dim,\n",
    "        segment_length=segment_length,\n",
    "        right_context_length=right_context_length,\n",
    "        time_reduction_input_dim=time_reduction_input_dim,\n",
    "        time_reduction_stride=time_reduction_stride,\n",
    "        transformer_num_heads=transformer_num_heads,\n",
    "        transformer_ffn_dim=transformer_ffn_dim,\n",
    "        transformer_num_layers=transformer_num_layers,\n",
    "        transformer_dropout=transformer_dropout,\n",
    "        transformer_activation=transformer_activation,\n",
    "        transformer_left_context_length=transformer_left_context_length,\n",
    "        transformer_max_memory_size=transformer_max_memory_size,\n",
    "        transformer_weight_init_scale_strategy=transformer_weight_init_scale_strategy,\n",
    "        transformer_tanh_on_mem=transformer_tanh_on_mem,\n",
    "    )\n",
    "predictor = _Predictor(\n",
    "    num_symbols,\n",
    "    encoding_dim,\n",
    "    symbol_embedding_dim=symbol_embedding_dim,\n",
    "    num_lstm_layers=num_lstm_layers,\n",
    "    lstm_hidden_dim=symbol_embedding_dim,\n",
    "    lstm_layer_norm=lstm_layer_norm,\n",
    "    lstm_layer_norm_epsilon=lstm_layer_norm_epsilon,\n",
    "    lstm_dropout=lstm_dropout,\n",
    ")\n",
    "joiner = _JoinerPad(encoding_dim, num_symbols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4PupgBKWe6p"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now we will train a model. This will generate some output sequences every 20 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = len(string.printable)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Number of characters: {num_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transducer(num_inputs=num_chars + 1, num_outputs=num_chars + 1)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNT(encoder, predictor, joiner).to(device)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TSrbH9xGPEC",
    "outputId": "102f780d-145c-481f-d667-1cd2b76d6111"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, lr=0.00001)\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "torch.compile(model)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = trainer.train(train_set, print_interval=10000, use_bf16=False, device=device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(\"Epoch %d: train loss = %f, test loss = %f\" % (epoch, train_loss, train_loss))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpzayGZFacNsCxMByK+VUg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "transducer-tutorial-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
