{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "# load env var huggingface token\n",
    "load_dotenv(os.path.join(\"../\", \".env\"))\n",
    "# login to the hub\n",
    "huggingface_hub.login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "voxpopuli = datasets.load_dataset(\n",
    "    \"facebook/voxpopuli\", \"en\", streaming=True, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'audio_id': '20180418-0900-PLENARY-3-en_20180418-08:50:36_17', 'language': 0, 'audio': {'path': 'train_part_0/20180418-0900-PLENARY-3-en_20180418-08:50:36_17.wav', 'array': array([-0.00030518,  0.00119019,  0.00506592, ..., -0.00036621,\n",
      "       -0.00027466, -0.00018311]), 'sampling_rate': 16000}, 'raw_text': 'If you do not address this problem, the ground is there for populist nationalist forces to go on growing all over Europe.', 'normalized_text': 'if you do not address this problem the ground is there for populist nationalist forces to go on growing all over europe.', 'gender': 'female', 'speaker_id': '124737', 'is_gold_transcript': True, 'accent': 'None'}, {'audio_id': '20170614-0900-PLENARY-5-en_20170614-10:03:08_5', 'language': 0, 'audio': {'path': 'train_part_0/20170614-0900-PLENARY-5-en_20170614-10:03:08_5.wav', 'array': array([-0.00036621, -0.00030518, -0.00042725, ...,  0.00012207,\n",
      "        0.00119019,  0.00027466]), 'sampling_rate': 16000}, 'raw_text': 'they attacked and removed the voices of resistance from our radio and TV stations. They attack and abuse the President who was elected in the United States of America, yet celebrate the globalist placeman they installed in France.', 'normalized_text': 'they attacked and removed the voices of resistance from our radio and tv stations. they attack and abuse the president who was elected in the united states of america yet celebrate the globalist placeman they installed in france.', 'gender': 'male', 'speaker_id': '124966', 'is_gold_transcript': True, 'accent': 'None'}, {'audio_id': '20180314-0900-PLENARY-13-en_20180314-16:03:18_3', 'language': 0, 'audio': {'path': 'train_part_0/20180314-0900-PLENARY-13-en_20180314-16:03:18_3.wav', 'array': array([ 0.00201416, -0.00344849, -0.00460815, ...,  0.00076294,\n",
      "        0.00198364,  0.0012207 ]), 'sampling_rate': 16000}, 'raw_text': 'In order to increase the preparedness at national and EU level, the key word to overcome those challenges is cooperation and the multidimensional aspects public private cooperation, cooperation between Member States, economic cross border and cross sector collaboration,', 'normalized_text': 'in order to increase the preparedness at national and eu level the key word to overcome those challenges is cooperation and the multidimensional aspects public private cooperation cooperation between member states economic cross border and cross sector collaboration', 'gender': 'female', 'speaker_id': 'None', 'is_gold_transcript': True, 'accent': 'None'}, {'audio_id': '20160413-0900-PLENARY-17-en_20160413-15:33:39_4', 'language': 0, 'audio': {'path': 'train_part_0/20160413-0900-PLENARY-17-en_20160413-15:33:39_4.wav', 'array': array([-0.00012207, -0.00048828,  0.        , ..., -0.0007019 ,\n",
      "        0.00045776,  0.00128174]), 'sampling_rate': 16000}, 'raw_text': 'These are not easy issues to resolve.', 'normalized_text': 'these are not easy issues to resolve.', 'gender': 'male', 'speaker_id': '28497', 'is_gold_transcript': True, 'accent': 'None'}, {'audio_id': '20170215-0900-PLENARY-4-en_20170215-10:17:28_5', 'language': 0, 'audio': {'path': 'train_part_0/20170215-0900-PLENARY-4-en_20170215-10:17:28_5.wav', 'array': array([ 0.00259399,  0.00048828, -0.00491333, ...,  0.05822754,\n",
      "        0.0586853 ,  0.05407715]), 'sampling_rate': 16000}, 'raw_text': 'So it is a good deal.', 'normalized_text': 'so it is a good deal.', 'gender': 'male', 'speaker_id': '96912', 'is_gold_transcript': True, 'accent': 'None'}]\n"
     ]
    }
   ],
   "source": [
    "voxpopuli_head = list(voxpopuli[\"train\"].take(5))\n",
    "SAMPLING_RATE = voxpopuli_head[0][\"audio\"][\"sampling_rate\"]\n",
    "print(voxpopuli_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoFeatureExtractor,\n",
    "    SpeechEncoderDecoderModel,\n",
    "    SpeechEncoderDecoderConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n",
    "decoder_id = \"facebook/bart-base\"  # text decoder\n",
    "\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "\n",
    "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_id, decoder_id, encoder_add_adapter=True\n",
    ")\n",
    "model.config.encoder.feat_proj_dropout = 0.0\n",
    "model.config.encoder.mask_time_prob = 0.0\n",
    "model.config.decoder_start_token_id = model.decoder.config.bos_token_id\n",
    "model.config.pad_token_id = model.decoder.config.pad_token_id\n",
    "model.config.eos_token_id = model.decoder.config.eos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.encoder.layerdrop = 0.0\n",
    "model.config.use_cache = False\n",
    "model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "# Load model without pretrained weights\n",
    "config_encoder = AutoConfig.from_pretrained(encoder_id)\n",
    "config_encoder.add_adapter = True\n",
    "config_decoder = AutoConfig.from_pretrained(decoder_id)\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "\n",
    "config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "    config_encoder, config_decoder\n",
    ")\n",
    "model_no_weights = SpeechEncoderDecoderModel(config=config)\n",
    "\n",
    "model_no_weights.config.encoder.feat_proj_dropout = 0.0\n",
    "model_no_weights.config.encoder.mask_time_prob = 0.0\n",
    "model_no_weights.config.decoder_start_token_id = (\n",
    "    model_no_weights.decoder.config.bos_token_id\n",
    ")\n",
    "model_no_weights.config.pad_token_id = model_no_weights.decoder.config.pad_token_id\n",
    "model_no_weights.config.eos_token_id = model_no_weights.decoder.config.eos_token_id\n",
    "model_no_weights.config.max_length = 128\n",
    "model_no_weights.config.encoder.layerdrop = 0.0\n",
    "model_no_weights.config.use_cache = False\n",
    "model_no_weights.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "# # Accessing the model configuration\n",
    "# config_encoder = model_no_weights.config.encoder\n",
    "# config_encoder.add_adapter = True\n",
    "# config_decoder = model_no_weights.config.decoder\n",
    "# # set decoder config to causal lm\n",
    "# config_decoder.is_decoder = True\n",
    "# config_decoder.add_cross_attention = True\n",
    "\n",
    "\n",
    "# input_values = feature_extractor(\n",
    "#     voxpopuli_head[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=SAMPLING_RATE\n",
    "# ).input_values\n",
    "\n",
    "# generated_ids = model.generate(input_values, decoder_start_token_id=tokenizer.cls_token_id, )\n",
    "# generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "\n",
    "# # load its corresponding transcription and tokenize to generate labels\n",
    "# labels = tokenizer(voxpopuli_head[0][\"text\"], return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matej/fitvut/dp_mit/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.adapter.layers.0.conv.bias', 'wav2vec2.adapter.layers.0.conv.weight', 'wav2vec2.adapter.layers.1.conv.bias', 'wav2vec2.adapter.layers.1.conv.weight', 'wav2vec2.adapter.layers.2.conv.bias', 'wav2vec2.adapter.layers.2.conv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/matej/fitvut/dp_mit/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/matej/fitvut/dp_mit/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../seq2seq_wav2vec2_bart-base/tokenizer_config.json',\n",
       " '../../seq2seq_wav2vec2_bart-base/special_tokens_map.json',\n",
       " '../../seq2seq_wav2vec2_bart-base/vocab.json',\n",
       " '../../seq2seq_wav2vec2_bart-base/merges.txt',\n",
       " '../../seq2seq_wav2vec2_bart-base/added_tokens.json',\n",
       " '../../seq2seq_wav2vec2_bart-base/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import SpeechEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-base\"\n",
    "decoder_id = \"facebook/bart-base\"\n",
    "SAVE_PATH = \"../../seq2seq_wav2vec2_bart-base\"\n",
    "\n",
    "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_id, decoder_id, encoder_add_adapter=True\n",
    ")\n",
    "model.config.encoder.feat_proj_dropout = 0.0\n",
    "model.config.encoder.mask_time_prob = 0.0\n",
    "model.config.decoder_start_token_id = model.decoder.config.bos_token_id\n",
    "model.config.pad_token_id = model.decoder.config.pad_token_id\n",
    "model.config.eos_token_id = model.decoder.config.eos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.encoder.layerdrop = 0.0\n",
    "model.config.use_cache = False\n",
    "model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "# check if generation works\n",
    "# _ = model.generate(torch.ones((1, 2000)))\n",
    "\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "\n",
    "feature_etxractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "feature_etxractor.save_pretrained(SAVE_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def display_weight_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            mean = param.data.mean().item()\n",
    "            std = param.data.std().item()\n",
    "            print(f\"Layer: {name}\\t| Mean: {mean:.4f}\\t| Std: {std:.4f}\")\n",
    "        else:\n",
    "            print(f\"Layer: {name}\\t| No gradients\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    encoder_params = (\n",
    "        sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)\n",
    "        if hasattr(model, \"encoder\")\n",
    "        else 0\n",
    "    )\n",
    "    decoder_params = (\n",
    "        sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "        if hasattr(model, \"decoder\")\n",
    "        else 0\n",
    "    )\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return encoder_params, decoder_params, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matej/fitvut/dp_mit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../seq2seq_wav2vec2_bart-base were not used when initializing SpeechEncoderDecoderModel: ['encoder.masked_spec_embed']\n",
      "- This IS expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"../seq2seq_wav2vec2_bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: 104992384\t| Decoder: 96103680\t| Total: 201096064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpeechEncoderDecoderModel(\n",
       "  (encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (adapter): Wav2Vec2Adapter(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x Wav2Vec2AdapterLayer(\n",
       "          (conv): Conv1d(768, 1536, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): BartForCausalLM(\n",
       "    (model): BartDecoderWrapper(\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc, dec, total = count_parameters(model)\n",
    "print(f\"Encoder: {enc}\\t| Decoder: {dec}\\t| Total: {total}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encoder.masked_spec_embed\t| Mean: 0.1925\t| Std: 0.2534\n",
      "Layer: encoder.feature_extractor.conv_layers.0.conv.weight\t| Mean: -0.0005\t| Std: 0.1797\n",
      "Layer: encoder.feature_extractor.conv_layers.0.layer_norm.weight\t| Mean: 0.4893\t| Std: 0.5753\n",
      "Layer: encoder.feature_extractor.conv_layers.0.layer_norm.bias\t| Mean: -0.0004\t| Std: 0.0030\n",
      "Layer: encoder.feature_extractor.conv_layers.1.conv.weight\t| Mean: -0.0138\t| Std: 0.0953\n",
      "Layer: encoder.feature_extractor.conv_layers.2.conv.weight\t| Mean: -0.0020\t| Std: 0.0872\n",
      "Layer: encoder.feature_extractor.conv_layers.3.conv.weight\t| Mean: -0.0006\t| Std: 0.0714\n",
      "Layer: encoder.feature_extractor.conv_layers.4.conv.weight\t| Mean: 0.0001\t| Std: 0.0626\n",
      "Layer: encoder.feature_extractor.conv_layers.5.conv.weight\t| Mean: 0.0009\t| Std: 0.0413\n",
      "Layer: encoder.feature_extractor.conv_layers.6.conv.weight\t| Mean: 0.0000\t| Std: 0.0207\n",
      "Layer: encoder.feature_projection.layer_norm.weight\t| Mean: 0.3693\t| Std: 0.0633\n",
      "Layer: encoder.feature_projection.layer_norm.bias\t| Mean: 0.0125\t| Std: 0.0859\n",
      "Layer: encoder.feature_projection.projection.weight\t| Mean: -0.0008\t| Std: 0.0905\n",
      "Layer: encoder.feature_projection.projection.bias\t| Mean: -0.0272\t| Std: 0.1452\n",
      "Layer: encoder.encoder.pos_conv_embed.conv.bias\t| Mean: 0.2039\t| Std: 0.1127\n",
      "Layer: encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0\t| Mean: 0.5421\t| Std: 1.3571\n",
      "Layer: encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1\t| Mean: 0.0123\t| Std: 0.0717\n",
      "Layer: encoder.encoder.layer_norm.weight\t| Mean: 0.3062\t| Std: 0.0793\n",
      "Layer: encoder.encoder.layer_norm.bias\t| Mean: 0.0062\t| Std: 0.0613\n",
      "Layer: encoder.encoder.layers.0.attention.k_proj.weight\t| Mean: -0.0001\t| Std: 0.0778\n",
      "Layer: encoder.encoder.layers.0.attention.k_proj.bias\t| Mean: 0.0000\t| Std: 0.0006\n",
      "Layer: encoder.encoder.layers.0.attention.v_proj.weight\t| Mean: -0.0000\t| Std: 0.0399\n",
      "Layer: encoder.encoder.layers.0.attention.v_proj.bias\t| Mean: -0.0001\t| Std: 0.0108\n",
      "Layer: encoder.encoder.layers.0.attention.q_proj.weight\t| Mean: -0.0000\t| Std: 0.0794\n",
      "Layer: encoder.encoder.layers.0.attention.q_proj.bias\t| Mean: -0.0037\t| Std: 0.2125\n",
      "Layer: encoder.encoder.layers.0.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0328\n",
      "Layer: encoder.encoder.layers.0.attention.out_proj.bias\t| Mean: 0.0003\t| Std: 0.0043\n",
      "Layer: encoder.encoder.layers.0.layer_norm.weight\t| Mean: 0.2500\t| Std: 0.0856\n",
      "Layer: encoder.encoder.layers.0.layer_norm.bias\t| Mean: 0.0035\t| Std: 0.1007\n",
      "Layer: encoder.encoder.layers.0.feed_forward.intermediate_dense.weight\t| Mean: -0.0006\t| Std: 0.0828\n",
      "Layer: encoder.encoder.layers.0.feed_forward.intermediate_dense.bias\t| Mean: -0.0782\t| Std: 0.0383\n",
      "Layer: encoder.encoder.layers.0.feed_forward.output_dense.weight\t| Mean: 0.0000\t| Std: 0.0750\n",
      "Layer: encoder.encoder.layers.0.feed_forward.output_dense.bias\t| Mean: -0.0006\t| Std: 0.0573\n",
      "Layer: encoder.encoder.layers.0.final_layer_norm.weight\t| Mean: 0.2886\t| Std: 0.0809\n",
      "Layer: encoder.encoder.layers.0.final_layer_norm.bias\t| Mean: -0.0019\t| Std: 0.0408\n",
      "Layer: encoder.encoder.layers.1.attention.k_proj.weight\t| Mean: -0.0001\t| Std: 0.0909\n",
      "Layer: encoder.encoder.layers.1.attention.k_proj.bias\t| Mean: -0.0002\t| Std: 0.0153\n",
      "Layer: encoder.encoder.layers.1.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0425\n",
      "Layer: encoder.encoder.layers.1.attention.v_proj.bias\t| Mean: -0.0001\t| Std: 0.0141\n",
      "Layer: encoder.encoder.layers.1.attention.q_proj.weight\t| Mean: -0.0001\t| Std: 0.0905\n",
      "Layer: encoder.encoder.layers.1.attention.q_proj.bias\t| Mean: -0.0063\t| Std: 0.2253\n",
      "Layer: encoder.encoder.layers.1.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0433\n",
      "Layer: encoder.encoder.layers.1.attention.out_proj.bias\t| Mean: -0.0001\t| Std: 0.0099\n",
      "Layer: encoder.encoder.layers.1.layer_norm.weight\t| Mean: 0.3066\t| Std: 0.0923\n",
      "Layer: encoder.encoder.layers.1.layer_norm.bias\t| Mean: 0.0158\t| Std: 0.1742\n",
      "Layer: encoder.encoder.layers.1.feed_forward.intermediate_dense.weight\t| Mean: -0.0015\t| Std: 0.0765\n",
      "Layer: encoder.encoder.layers.1.feed_forward.intermediate_dense.bias\t| Mean: -0.0576\t| Std: 0.0269\n",
      "Layer: encoder.encoder.layers.1.feed_forward.output_dense.weight\t| Mean: -0.0000\t| Std: 0.0694\n",
      "Layer: encoder.encoder.layers.1.feed_forward.output_dense.bias\t| Mean: 0.0011\t| Std: 0.0414\n",
      "Layer: encoder.encoder.layers.1.final_layer_norm.weight\t| Mean: 0.2849\t| Std: 0.0807\n",
      "Layer: encoder.encoder.layers.1.final_layer_norm.bias\t| Mean: -0.0013\t| Std: 0.0615\n",
      "Layer: encoder.encoder.layers.2.attention.k_proj.weight\t| Mean: 0.0000\t| Std: 0.0870\n",
      "Layer: encoder.encoder.layers.2.attention.k_proj.bias\t| Mean: -0.0003\t| Std: 0.0262\n",
      "Layer: encoder.encoder.layers.2.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0496\n",
      "Layer: encoder.encoder.layers.2.attention.v_proj.bias\t| Mean: -0.0008\t| Std: 0.0175\n",
      "Layer: encoder.encoder.layers.2.attention.q_proj.weight\t| Mean: 0.0001\t| Std: 0.0870\n",
      "Layer: encoder.encoder.layers.2.attention.q_proj.bias\t| Mean: 0.0023\t| Std: 0.2006\n",
      "Layer: encoder.encoder.layers.2.attention.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0503\n",
      "Layer: encoder.encoder.layers.2.attention.out_proj.bias\t| Mean: 0.0007\t| Std: 0.0094\n",
      "Layer: encoder.encoder.layers.2.layer_norm.weight\t| Mean: 0.3050\t| Std: 0.1187\n",
      "Layer: encoder.encoder.layers.2.layer_norm.bias\t| Mean: 0.0142\t| Std: 0.1803\n",
      "Layer: encoder.encoder.layers.2.feed_forward.intermediate_dense.weight\t| Mean: -0.0016\t| Std: 0.0829\n",
      "Layer: encoder.encoder.layers.2.feed_forward.intermediate_dense.bias\t| Mean: -0.0477\t| Std: 0.0258\n",
      "Layer: encoder.encoder.layers.2.feed_forward.output_dense.weight\t| Mean: -0.0000\t| Std: 0.0718\n",
      "Layer: encoder.encoder.layers.2.feed_forward.output_dense.bias\t| Mean: 0.0008\t| Std: 0.0419\n",
      "Layer: encoder.encoder.layers.2.final_layer_norm.weight\t| Mean: 0.2886\t| Std: 0.1048\n",
      "Layer: encoder.encoder.layers.2.final_layer_norm.bias\t| Mean: -0.0000\t| Std: 0.0622\n",
      "Layer: encoder.encoder.layers.3.attention.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0887\n",
      "Layer: encoder.encoder.layers.3.attention.k_proj.bias\t| Mean: -0.0002\t| Std: 0.0299\n",
      "Layer: encoder.encoder.layers.3.attention.v_proj.weight\t| Mean: -0.0000\t| Std: 0.0551\n",
      "Layer: encoder.encoder.layers.3.attention.v_proj.bias\t| Mean: 0.0012\t| Std: 0.0239\n",
      "Layer: encoder.encoder.layers.3.attention.q_proj.weight\t| Mean: -0.0001\t| Std: 0.0887\n",
      "Layer: encoder.encoder.layers.3.attention.q_proj.bias\t| Mean: -0.0024\t| Std: 0.2025\n",
      "Layer: encoder.encoder.layers.3.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0564\n",
      "Layer: encoder.encoder.layers.3.attention.out_proj.bias\t| Mean: -0.0008\t| Std: 0.0349\n",
      "Layer: encoder.encoder.layers.3.layer_norm.weight\t| Mean: 0.3052\t| Std: 0.1308\n",
      "Layer: encoder.encoder.layers.3.layer_norm.bias\t| Mean: 0.0153\t| Std: 0.1905\n",
      "Layer: encoder.encoder.layers.3.feed_forward.intermediate_dense.weight\t| Mean: -0.0015\t| Std: 0.0837\n",
      "Layer: encoder.encoder.layers.3.feed_forward.intermediate_dense.bias\t| Mean: -0.0443\t| Std: 0.0245\n",
      "Layer: encoder.encoder.layers.3.feed_forward.output_dense.weight\t| Mean: -0.0000\t| Std: 0.0727\n",
      "Layer: encoder.encoder.layers.3.feed_forward.output_dense.bias\t| Mean: -0.0002\t| Std: 0.0332\n",
      "Layer: encoder.encoder.layers.3.final_layer_norm.weight\t| Mean: 0.2807\t| Std: 0.1112\n",
      "Layer: encoder.encoder.layers.3.final_layer_norm.bias\t| Mean: -0.0022\t| Std: 0.0654\n",
      "Layer: encoder.encoder.layers.4.attention.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0891\n",
      "Layer: encoder.encoder.layers.4.attention.k_proj.bias\t| Mean: -0.0013\t| Std: 0.0656\n",
      "Layer: encoder.encoder.layers.4.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0604\n",
      "Layer: encoder.encoder.layers.4.attention.v_proj.bias\t| Mean: 0.0006\t| Std: 0.0302\n",
      "Layer: encoder.encoder.layers.4.attention.q_proj.weight\t| Mean: -0.0001\t| Std: 0.0907\n",
      "Layer: encoder.encoder.layers.4.attention.q_proj.bias\t| Mean: 0.0005\t| Std: 0.2311\n",
      "Layer: encoder.encoder.layers.4.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0633\n",
      "Layer: encoder.encoder.layers.4.attention.out_proj.bias\t| Mean: -0.0002\t| Std: 0.0352\n",
      "Layer: encoder.encoder.layers.4.layer_norm.weight\t| Mean: 0.3134\t| Std: 0.1674\n",
      "Layer: encoder.encoder.layers.4.layer_norm.bias\t| Mean: 0.0143\t| Std: 0.1841\n",
      "Layer: encoder.encoder.layers.4.feed_forward.intermediate_dense.weight\t| Mean: -0.0013\t| Std: 0.0857\n",
      "Layer: encoder.encoder.layers.4.feed_forward.intermediate_dense.bias\t| Mean: -0.0416\t| Std: 0.0278\n",
      "Layer: encoder.encoder.layers.4.feed_forward.output_dense.weight\t| Mean: -0.0001\t| Std: 0.0774\n",
      "Layer: encoder.encoder.layers.4.feed_forward.output_dense.bias\t| Mean: 0.0005\t| Std: 0.0361\n",
      "Layer: encoder.encoder.layers.4.final_layer_norm.weight\t| Mean: 0.2777\t| Std: 0.0969\n",
      "Layer: encoder.encoder.layers.4.final_layer_norm.bias\t| Mean: -0.0060\t| Std: 0.0585\n",
      "Layer: encoder.encoder.layers.5.attention.k_proj.weight\t| Mean: 0.0000\t| Std: 0.0906\n",
      "Layer: encoder.encoder.layers.5.attention.k_proj.bias\t| Mean: 0.0002\t| Std: 0.0398\n",
      "Layer: encoder.encoder.layers.5.attention.v_proj.weight\t| Mean: -0.0000\t| Std: 0.0564\n",
      "Layer: encoder.encoder.layers.5.attention.v_proj.bias\t| Mean: 0.0008\t| Std: 0.0261\n",
      "Layer: encoder.encoder.layers.5.attention.q_proj.weight\t| Mean: -0.0002\t| Std: 0.0912\n",
      "Layer: encoder.encoder.layers.5.attention.q_proj.bias\t| Mean: 0.0051\t| Std: 0.2159\n",
      "Layer: encoder.encoder.layers.5.attention.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0573\n",
      "Layer: encoder.encoder.layers.5.attention.out_proj.bias\t| Mean: 0.0000\t| Std: 0.0182\n",
      "Layer: encoder.encoder.layers.5.layer_norm.weight\t| Mean: 0.3698\t| Std: 0.1408\n",
      "Layer: encoder.encoder.layers.5.layer_norm.bias\t| Mean: 0.0045\t| Std: 0.2133\n",
      "Layer: encoder.encoder.layers.5.feed_forward.intermediate_dense.weight\t| Mean: -0.0006\t| Std: 0.0891\n",
      "Layer: encoder.encoder.layers.5.feed_forward.intermediate_dense.bias\t| Mean: -0.0410\t| Std: 0.0301\n",
      "Layer: encoder.encoder.layers.5.feed_forward.output_dense.weight\t| Mean: -0.0001\t| Std: 0.0805\n",
      "Layer: encoder.encoder.layers.5.feed_forward.output_dense.bias\t| Mean: 0.0007\t| Std: 0.0421\n",
      "Layer: encoder.encoder.layers.5.final_layer_norm.weight\t| Mean: 0.2845\t| Std: 0.0883\n",
      "Layer: encoder.encoder.layers.5.final_layer_norm.bias\t| Mean: -0.0088\t| Std: 0.0681\n",
      "Layer: encoder.encoder.layers.6.attention.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0932\n",
      "Layer: encoder.encoder.layers.6.attention.k_proj.bias\t| Mean: -0.0042\t| Std: 0.0565\n",
      "Layer: encoder.encoder.layers.6.attention.v_proj.weight\t| Mean: 0.0002\t| Std: 0.0573\n",
      "Layer: encoder.encoder.layers.6.attention.v_proj.bias\t| Mean: -0.0014\t| Std: 0.0296\n",
      "Layer: encoder.encoder.layers.6.attention.q_proj.weight\t| Mean: -0.0000\t| Std: 0.0932\n",
      "Layer: encoder.encoder.layers.6.attention.q_proj.bias\t| Mean: -0.0040\t| Std: 0.2289\n",
      "Layer: encoder.encoder.layers.6.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0577\n",
      "Layer: encoder.encoder.layers.6.attention.out_proj.bias\t| Mean: 0.0001\t| Std: 0.0143\n",
      "Layer: encoder.encoder.layers.6.layer_norm.weight\t| Mean: 0.4180\t| Std: 0.1542\n",
      "Layer: encoder.encoder.layers.6.layer_norm.bias\t| Mean: 0.0067\t| Std: 0.2308\n",
      "Layer: encoder.encoder.layers.6.feed_forward.intermediate_dense.weight\t| Mean: -0.0008\t| Std: 0.0960\n",
      "Layer: encoder.encoder.layers.6.feed_forward.intermediate_dense.bias\t| Mean: -0.0456\t| Std: 0.0359\n",
      "Layer: encoder.encoder.layers.6.feed_forward.output_dense.weight\t| Mean: -0.0002\t| Std: 0.0861\n",
      "Layer: encoder.encoder.layers.6.feed_forward.output_dense.bias\t| Mean: 0.0006\t| Std: 0.0502\n",
      "Layer: encoder.encoder.layers.6.final_layer_norm.weight\t| Mean: 0.2971\t| Std: 0.0806\n",
      "Layer: encoder.encoder.layers.6.final_layer_norm.bias\t| Mean: -0.0105\t| Std: 0.0730\n",
      "Layer: encoder.encoder.layers.7.attention.k_proj.weight\t| Mean: 0.0001\t| Std: 0.0957\n",
      "Layer: encoder.encoder.layers.7.attention.k_proj.bias\t| Mean: 0.0030\t| Std: 0.0987\n",
      "Layer: encoder.encoder.layers.7.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0641\n",
      "Layer: encoder.encoder.layers.7.attention.v_proj.bias\t| Mean: 0.0007\t| Std: 0.0323\n",
      "Layer: encoder.encoder.layers.7.attention.q_proj.weight\t| Mean: -0.0004\t| Std: 0.0952\n",
      "Layer: encoder.encoder.layers.7.attention.q_proj.bias\t| Mean: 0.0105\t| Std: 0.2495\n",
      "Layer: encoder.encoder.layers.7.attention.out_proj.weight\t| Mean: 0.0001\t| Std: 0.0638\n",
      "Layer: encoder.encoder.layers.7.attention.out_proj.bias\t| Mean: 0.0010\t| Std: 0.0157\n",
      "Layer: encoder.encoder.layers.7.layer_norm.weight\t| Mean: 0.4387\t| Std: 0.1542\n",
      "Layer: encoder.encoder.layers.7.layer_norm.bias\t| Mean: 0.0057\t| Std: 0.2215\n",
      "Layer: encoder.encoder.layers.7.feed_forward.intermediate_dense.weight\t| Mean: -0.0008\t| Std: 0.0941\n",
      "Layer: encoder.encoder.layers.7.feed_forward.intermediate_dense.bias\t| Mean: -0.0505\t| Std: 0.0414\n",
      "Layer: encoder.encoder.layers.7.feed_forward.output_dense.weight\t| Mean: -0.0001\t| Std: 0.0866\n",
      "Layer: encoder.encoder.layers.7.feed_forward.output_dense.bias\t| Mean: 0.0017\t| Std: 0.0628\n",
      "Layer: encoder.encoder.layers.7.final_layer_norm.weight\t| Mean: 0.3017\t| Std: 0.0784\n",
      "Layer: encoder.encoder.layers.7.final_layer_norm.bias\t| Mean: -0.0067\t| Std: 0.0807\n",
      "Layer: encoder.encoder.layers.8.attention.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0988\n",
      "Layer: encoder.encoder.layers.8.attention.k_proj.bias\t| Mean: 0.0010\t| Std: 0.1490\n",
      "Layer: encoder.encoder.layers.8.attention.v_proj.weight\t| Mean: -0.0001\t| Std: 0.0645\n",
      "Layer: encoder.encoder.layers.8.attention.v_proj.bias\t| Mean: 0.0005\t| Std: 0.0297\n",
      "Layer: encoder.encoder.layers.8.attention.q_proj.weight\t| Mean: 0.0000\t| Std: 0.0980\n",
      "Layer: encoder.encoder.layers.8.attention.q_proj.bias\t| Mean: -0.0057\t| Std: 0.2940\n",
      "Layer: encoder.encoder.layers.8.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0616\n",
      "Layer: encoder.encoder.layers.8.attention.out_proj.bias\t| Mean: 0.0004\t| Std: 0.0210\n",
      "Layer: encoder.encoder.layers.8.layer_norm.weight\t| Mean: 0.3582\t| Std: 0.1317\n",
      "Layer: encoder.encoder.layers.8.layer_norm.bias\t| Mean: -0.0044\t| Std: 0.1756\n",
      "Layer: encoder.encoder.layers.8.feed_forward.intermediate_dense.weight\t| Mean: -0.0003\t| Std: 0.0897\n",
      "Layer: encoder.encoder.layers.8.feed_forward.intermediate_dense.bias\t| Mean: -0.0545\t| Std: 0.0495\n",
      "Layer: encoder.encoder.layers.8.feed_forward.output_dense.weight\t| Mean: -0.0001\t| Std: 0.0794\n",
      "Layer: encoder.encoder.layers.8.feed_forward.output_dense.bias\t| Mean: 0.0030\t| Std: 0.0805\n",
      "Layer: encoder.encoder.layers.8.final_layer_norm.weight\t| Mean: 0.2936\t| Std: 0.0872\n",
      "Layer: encoder.encoder.layers.8.final_layer_norm.bias\t| Mean: -0.0006\t| Std: 0.0763\n",
      "Layer: encoder.encoder.layers.9.attention.k_proj.weight\t| Mean: -0.0001\t| Std: 0.0900\n",
      "Layer: encoder.encoder.layers.9.attention.k_proj.bias\t| Mean: -0.0009\t| Std: 0.1020\n",
      "Layer: encoder.encoder.layers.9.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0579\n",
      "Layer: encoder.encoder.layers.9.attention.v_proj.bias\t| Mean: -0.0008\t| Std: 0.0401\n",
      "Layer: encoder.encoder.layers.9.attention.q_proj.weight\t| Mean: -0.0001\t| Std: 0.0895\n",
      "Layer: encoder.encoder.layers.9.attention.q_proj.bias\t| Mean: 0.0089\t| Std: 0.2854\n",
      "Layer: encoder.encoder.layers.9.attention.out_proj.weight\t| Mean: 0.0001\t| Std: 0.0572\n",
      "Layer: encoder.encoder.layers.9.attention.out_proj.bias\t| Mean: -0.0012\t| Std: 0.0524\n",
      "Layer: encoder.encoder.layers.9.layer_norm.weight\t| Mean: 0.2701\t| Std: 0.1038\n",
      "Layer: encoder.encoder.layers.9.layer_norm.bias\t| Mean: -0.0100\t| Std: 0.1162\n",
      "Layer: encoder.encoder.layers.9.feed_forward.intermediate_dense.weight\t| Mean: 0.0004\t| Std: 0.0821\n",
      "Layer: encoder.encoder.layers.9.feed_forward.intermediate_dense.bias\t| Mean: -0.0636\t| Std: 0.0546\n",
      "Layer: encoder.encoder.layers.9.feed_forward.output_dense.weight\t| Mean: 0.0000\t| Std: 0.0707\n",
      "Layer: encoder.encoder.layers.9.feed_forward.output_dense.bias\t| Mean: 0.0019\t| Std: 0.0597\n",
      "Layer: encoder.encoder.layers.9.final_layer_norm.weight\t| Mean: 0.2951\t| Std: 0.0796\n",
      "Layer: encoder.encoder.layers.9.final_layer_norm.bias\t| Mean: 0.0010\t| Std: 0.0589\n",
      "Layer: encoder.encoder.layers.10.attention.k_proj.weight\t| Mean: 0.0000\t| Std: 0.0864\n",
      "Layer: encoder.encoder.layers.10.attention.k_proj.bias\t| Mean: -0.0054\t| Std: 0.2177\n",
      "Layer: encoder.encoder.layers.10.attention.v_proj.weight\t| Mean: -0.0001\t| Std: 0.0561\n",
      "Layer: encoder.encoder.layers.10.attention.v_proj.bias\t| Mean: 0.0015\t| Std: 0.0422\n",
      "Layer: encoder.encoder.layers.10.attention.q_proj.weight\t| Mean: 0.0001\t| Std: 0.0866\n",
      "Layer: encoder.encoder.layers.10.attention.q_proj.bias\t| Mean: -0.0011\t| Std: 0.2739\n",
      "Layer: encoder.encoder.layers.10.attention.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0568\n",
      "Layer: encoder.encoder.layers.10.attention.out_proj.bias\t| Mean: -0.0025\t| Std: 0.0712\n",
      "Layer: encoder.encoder.layers.10.layer_norm.weight\t| Mean: 0.2697\t| Std: 0.0890\n",
      "Layer: encoder.encoder.layers.10.layer_norm.bias\t| Mean: -0.0088\t| Std: 0.1025\n",
      "Layer: encoder.encoder.layers.10.feed_forward.intermediate_dense.weight\t| Mean: 0.0003\t| Std: 0.0722\n",
      "Layer: encoder.encoder.layers.10.feed_forward.intermediate_dense.bias\t| Mean: -0.0708\t| Std: 0.0654\n",
      "Layer: encoder.encoder.layers.10.feed_forward.output_dense.weight\t| Mean: 0.0000\t| Std: 0.0580\n",
      "Layer: encoder.encoder.layers.10.feed_forward.output_dense.bias\t| Mean: 0.0009\t| Std: 0.0506\n",
      "Layer: encoder.encoder.layers.10.final_layer_norm.weight\t| Mean: 0.2840\t| Std: 0.0696\n",
      "Layer: encoder.encoder.layers.10.final_layer_norm.bias\t| Mean: 0.0014\t| Std: 0.0480\n",
      "Layer: encoder.encoder.layers.11.attention.k_proj.weight\t| Mean: 0.0001\t| Std: 0.0888\n",
      "Layer: encoder.encoder.layers.11.attention.k_proj.bias\t| Mean: -0.0000\t| Std: 0.1752\n",
      "Layer: encoder.encoder.layers.11.attention.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0544\n",
      "Layer: encoder.encoder.layers.11.attention.v_proj.bias\t| Mean: 0.0003\t| Std: 0.0399\n",
      "Layer: encoder.encoder.layers.11.attention.q_proj.weight\t| Mean: 0.0000\t| Std: 0.0876\n",
      "Layer: encoder.encoder.layers.11.attention.q_proj.bias\t| Mean: 0.0031\t| Std: 0.2242\n",
      "Layer: encoder.encoder.layers.11.attention.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0539\n",
      "Layer: encoder.encoder.layers.11.attention.out_proj.bias\t| Mean: -0.0007\t| Std: 0.0523\n",
      "Layer: encoder.encoder.layers.11.layer_norm.weight\t| Mean: 0.3502\t| Std: 0.0987\n",
      "Layer: encoder.encoder.layers.11.layer_norm.bias\t| Mean: -0.0201\t| Std: 0.1204\n",
      "Layer: encoder.encoder.layers.11.feed_forward.intermediate_dense.weight\t| Mean: 0.0015\t| Std: 0.0571\n",
      "Layer: encoder.encoder.layers.11.feed_forward.intermediate_dense.bias\t| Mean: -0.1004\t| Std: 0.0805\n",
      "Layer: encoder.encoder.layers.11.feed_forward.output_dense.weight\t| Mean: 0.0000\t| Std: 0.0306\n",
      "Layer: encoder.encoder.layers.11.feed_forward.output_dense.bias\t| Mean: -0.0014\t| Std: 0.0193\n",
      "Layer: encoder.encoder.layers.11.final_layer_norm.weight\t| Mean: 0.3165\t| Std: 0.1127\n",
      "Layer: encoder.encoder.layers.11.final_layer_norm.bias\t| Mean: -0.0010\t| Std: 0.0563\n",
      "Layer: encoder.adapter.layers.0.conv.weight\t| Mean: -0.0000\t| Std: 0.0295\n",
      "Layer: encoder.adapter.layers.0.conv.bias\t| Mean: 0.0006\t| Std: 0.0118\n",
      "Layer: encoder.adapter.layers.1.conv.weight\t| Mean: -0.0000\t| Std: 0.0295\n",
      "Layer: encoder.adapter.layers.1.conv.bias\t| Mean: -0.0001\t| Std: 0.0122\n",
      "Layer: encoder.adapter.layers.2.conv.weight\t| Mean: 0.0000\t| Std: 0.0295\n",
      "Layer: encoder.adapter.layers.2.conv.bias\t| Mean: -0.0002\t| Std: 0.0120\n",
      "Layer: decoder.model.decoder.embed_tokens.weight\t| Mean: 0.0000\t| Std: 0.0200\n",
      "Layer: decoder.model.decoder.embed_positions.weight\t| Mean: 0.0000\t| Std: 0.0445\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0832\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.k_proj.bias\t| Mean: -0.0002\t| Std: 0.0062\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.v_proj.weight\t| Mean: -0.0000\t| Std: 0.0367\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.v_proj.bias\t| Mean: -0.0028\t| Std: 0.0792\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.q_proj.weight\t| Mean: -0.0000\t| Std: 0.0803\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.q_proj.bias\t| Mean: -0.0155\t| Std: 0.5790\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0372\n",
      "Layer: decoder.model.decoder.layers.0.self_attn.out_proj.bias\t| Mean: 0.0008\t| Std: 0.0859\n",
      "Layer: decoder.model.decoder.layers.0.self_attn_layer_norm.weight\t| Mean: 0.8208\t| Std: 0.0725\n",
      "Layer: decoder.model.decoder.layers.0.self_attn_layer_norm.bias\t| Mean: -0.1181\t| Std: 0.1170\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.k_proj.weight\t| Mean: -0.0001\t| Std: 0.0853\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.k_proj.bias\t| Mean: 0.0003\t| Std: 0.0057\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.v_proj.weight\t| Mean: -0.0002\t| Std: 0.0669\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.v_proj.bias\t| Mean: -0.0019\t| Std: 0.0476\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.q_proj.weight\t| Mean: 0.0003\t| Std: 0.0895\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.q_proj.bias\t| Mean: -0.0054\t| Std: 0.1661\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0679\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn.out_proj.bias\t| Mean: 0.0017\t| Std: 0.1183\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight\t| Mean: 0.3862\t| Std: 0.0746\n",
      "Layer: decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias\t| Mean: 0.0118\t| Std: 0.1597\n",
      "Layer: decoder.model.decoder.layers.0.fc1.weight\t| Mean: -0.0007\t| Std: 0.0636\n",
      "Layer: decoder.model.decoder.layers.0.fc1.bias\t| Mean: -0.0997\t| Std: 0.0815\n",
      "Layer: decoder.model.decoder.layers.0.fc2.weight\t| Mean: -0.0001\t| Std: 0.0544\n",
      "Layer: decoder.model.decoder.layers.0.fc2.bias\t| Mean: 0.0042\t| Std: 0.0873\n",
      "Layer: decoder.model.decoder.layers.0.final_layer_norm.weight\t| Mean: 0.4488\t| Std: 0.0476\n",
      "Layer: decoder.model.decoder.layers.0.final_layer_norm.bias\t| Mean: -0.0137\t| Std: 0.0613\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0766\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.k_proj.bias\t| Mean: 0.0001\t| Std: 0.0066\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0413\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.v_proj.bias\t| Mean: -0.0008\t| Std: 0.0177\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.q_proj.weight\t| Mean: 0.0001\t| Std: 0.0771\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.q_proj.bias\t| Mean: -0.0047\t| Std: 0.3126\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0400\n",
      "Layer: decoder.model.decoder.layers.1.self_attn.out_proj.bias\t| Mean: 0.0067\t| Std: 0.0540\n",
      "Layer: decoder.model.decoder.layers.1.self_attn_layer_norm.weight\t| Mean: 0.7894\t| Std: 0.0634\n",
      "Layer: decoder.model.decoder.layers.1.self_attn_layer_norm.bias\t| Mean: -0.0970\t| Std: 0.1029\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.k_proj.weight\t| Mean: -0.0000\t| Std: 0.0836\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.k_proj.bias\t| Mean: -0.0002\t| Std: 0.0048\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0686\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.v_proj.bias\t| Mean: -0.0002\t| Std: 0.0344\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.q_proj.weight\t| Mean: -0.0002\t| Std: 0.0863\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.q_proj.bias\t| Mean: -0.0002\t| Std: 0.1872\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.out_proj.weight\t| Mean: -0.0001\t| Std: 0.0701\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn.out_proj.bias\t| Mean: 0.0015\t| Std: 0.0798\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight\t| Mean: 0.4226\t| Std: 0.1138\n",
      "Layer: decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias\t| Mean: 0.0015\t| Std: 0.1527\n",
      "Layer: decoder.model.decoder.layers.1.fc1.weight\t| Mean: -0.0003\t| Std: 0.0574\n",
      "Layer: decoder.model.decoder.layers.1.fc1.bias\t| Mean: -0.1131\t| Std: 0.0793\n",
      "Layer: decoder.model.decoder.layers.1.fc2.weight\t| Mean: 0.0000\t| Std: 0.0475\n",
      "Layer: decoder.model.decoder.layers.1.fc2.bias\t| Mean: -0.0000\t| Std: 0.0793\n",
      "Layer: decoder.model.decoder.layers.1.final_layer_norm.weight\t| Mean: 0.4532\t| Std: 0.0378\n",
      "Layer: decoder.model.decoder.layers.1.final_layer_norm.bias\t| Mean: -0.0174\t| Std: 0.0392\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.k_proj.weight\t| Mean: -0.0002\t| Std: 0.0777\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.k_proj.bias\t| Mean: -0.0002\t| Std: 0.0092\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0471\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.v_proj.bias\t| Mean: -0.0003\t| Std: 0.0114\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.q_proj.weight\t| Mean: 0.0005\t| Std: 0.0786\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.q_proj.bias\t| Mean: -0.0129\t| Std: 0.2978\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0431\n",
      "Layer: decoder.model.decoder.layers.2.self_attn.out_proj.bias\t| Mean: 0.0045\t| Std: 0.0439\n",
      "Layer: decoder.model.decoder.layers.2.self_attn_layer_norm.weight\t| Mean: 0.8603\t| Std: 0.0484\n",
      "Layer: decoder.model.decoder.layers.2.self_attn_layer_norm.bias\t| Mean: -0.0880\t| Std: 0.0935\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.k_proj.weight\t| Mean: -0.0001\t| Std: 0.0857\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.k_proj.bias\t| Mean: -0.0000\t| Std: 0.0094\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.v_proj.weight\t| Mean: -0.0001\t| Std: 0.0644\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.v_proj.bias\t| Mean: -0.0004\t| Std: 0.0345\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.q_proj.weight\t| Mean: -0.0006\t| Std: 0.0885\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.q_proj.bias\t| Mean: 0.0087\t| Std: 0.1857\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0663\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn.out_proj.bias\t| Mean: -0.0001\t| Std: 0.0564\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight\t| Mean: 0.4471\t| Std: 0.1588\n",
      "Layer: decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias\t| Mean: 0.0313\t| Std: 0.1146\n",
      "Layer: decoder.model.decoder.layers.2.fc1.weight\t| Mean: -0.0031\t| Std: 0.0542\n",
      "Layer: decoder.model.decoder.layers.2.fc1.bias\t| Mean: -0.1085\t| Std: 0.0753\n",
      "Layer: decoder.model.decoder.layers.2.fc2.weight\t| Mean: -0.0000\t| Std: 0.0429\n",
      "Layer: decoder.model.decoder.layers.2.fc2.bias\t| Mean: 0.0003\t| Std: 0.0835\n",
      "Layer: decoder.model.decoder.layers.2.final_layer_norm.weight\t| Mean: 0.4497\t| Std: 0.0347\n",
      "Layer: decoder.model.decoder.layers.2.final_layer_norm.bias\t| Mean: -0.0218\t| Std: 0.0206\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.k_proj.weight\t| Mean: 0.0000\t| Std: 0.0783\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.k_proj.bias\t| Mean: 0.0000\t| Std: 0.0112\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.v_proj.weight\t| Mean: -0.0001\t| Std: 0.0436\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.v_proj.bias\t| Mean: 0.0002\t| Std: 0.0152\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.q_proj.weight\t| Mean: 0.0006\t| Std: 0.0796\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.q_proj.bias\t| Mean: -0.0128\t| Std: 0.3422\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0395\n",
      "Layer: decoder.model.decoder.layers.3.self_attn.out_proj.bias\t| Mean: 0.0013\t| Std: 0.0281\n",
      "Layer: decoder.model.decoder.layers.3.self_attn_layer_norm.weight\t| Mean: 0.8673\t| Std: 0.0510\n",
      "Layer: decoder.model.decoder.layers.3.self_attn_layer_norm.bias\t| Mean: 0.0764\t| Std: 0.0860\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.k_proj.weight\t| Mean: 0.0002\t| Std: 0.0861\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.k_proj.bias\t| Mean: -0.0001\t| Std: 0.0257\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.v_proj.weight\t| Mean: 0.0001\t| Std: 0.0616\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.v_proj.bias\t| Mean: 0.0041\t| Std: 0.0442\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.q_proj.weight\t| Mean: 0.0001\t| Std: 0.0874\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.q_proj.bias\t| Mean: 0.0028\t| Std: 0.2130\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0594\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn.out_proj.bias\t| Mean: -0.0004\t| Std: 0.0624\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight\t| Mean: 0.4438\t| Std: 0.0815\n",
      "Layer: decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias\t| Mean: -0.0319\t| Std: 0.1026\n",
      "Layer: decoder.model.decoder.layers.3.fc1.weight\t| Mean: 0.0027\t| Std: 0.0538\n",
      "Layer: decoder.model.decoder.layers.3.fc1.bias\t| Mean: -0.1177\t| Std: 0.0898\n",
      "Layer: decoder.model.decoder.layers.3.fc2.weight\t| Mean: 0.0000\t| Std: 0.0403\n",
      "Layer: decoder.model.decoder.layers.3.fc2.bias\t| Mean: 0.0010\t| Std: 0.0769\n",
      "Layer: decoder.model.decoder.layers.3.final_layer_norm.weight\t| Mean: 0.4562\t| Std: 0.0352\n",
      "Layer: decoder.model.decoder.layers.3.final_layer_norm.bias\t| Mean: -0.0175\t| Std: 0.0296\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.k_proj.weight\t| Mean: 0.0002\t| Std: 0.0790\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.k_proj.bias\t| Mean: 0.0007\t| Std: 0.0224\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.v_proj.weight\t| Mean: 0.0001\t| Std: 0.0447\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.v_proj.bias\t| Mean: -0.0004\t| Std: 0.0211\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.q_proj.weight\t| Mean: -0.0004\t| Std: 0.0801\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.q_proj.bias\t| Mean: 0.0127\t| Std: 0.3303\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0396\n",
      "Layer: decoder.model.decoder.layers.4.self_attn.out_proj.bias\t| Mean: 0.0010\t| Std: 0.0419\n",
      "Layer: decoder.model.decoder.layers.4.self_attn_layer_norm.weight\t| Mean: 0.8561\t| Std: 0.0427\n",
      "Layer: decoder.model.decoder.layers.4.self_attn_layer_norm.bias\t| Mean: -0.0623\t| Std: 0.1192\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.k_proj.weight\t| Mean: 0.0001\t| Std: 0.0864\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.k_proj.bias\t| Mean: -0.0018\t| Std: 0.0498\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.v_proj.weight\t| Mean: 0.0001\t| Std: 0.0617\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.v_proj.bias\t| Mean: 0.0010\t| Std: 0.0435\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.q_proj.weight\t| Mean: 0.0001\t| Std: 0.0878\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.q_proj.bias\t| Mean: -0.0021\t| Std: 0.2621\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0606\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn.out_proj.bias\t| Mean: -0.0007\t| Std: 0.1073\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight\t| Mean: 0.4336\t| Std: 0.1704\n",
      "Layer: decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias\t| Mean: -0.0801\t| Std: 0.0703\n",
      "Layer: decoder.model.decoder.layers.4.fc1.weight\t| Mean: 0.0103\t| Std: 0.0507\n",
      "Layer: decoder.model.decoder.layers.4.fc1.bias\t| Mean: -0.1015\t| Std: 0.0660\n",
      "Layer: decoder.model.decoder.layers.4.fc2.weight\t| Mean: 0.0000\t| Std: 0.0398\n",
      "Layer: decoder.model.decoder.layers.4.fc2.bias\t| Mean: 0.0013\t| Std: 0.0740\n",
      "Layer: decoder.model.decoder.layers.4.final_layer_norm.weight\t| Mean: 0.4508\t| Std: 0.0397\n",
      "Layer: decoder.model.decoder.layers.4.final_layer_norm.bias\t| Mean: -0.0194\t| Std: 0.0493\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.k_proj.weight\t| Mean: 0.0003\t| Std: 0.0809\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.k_proj.bias\t| Mean: -0.0082\t| Std: 0.1321\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.v_proj.weight\t| Mean: 0.0002\t| Std: 0.0477\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.v_proj.bias\t| Mean: -0.0007\t| Std: 0.0134\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.q_proj.weight\t| Mean: -0.0011\t| Std: 0.0815\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.q_proj.bias\t| Mean: 0.0296\t| Std: 0.3317\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.out_proj.weight\t| Mean: 0.0000\t| Std: 0.0415\n",
      "Layer: decoder.model.decoder.layers.5.self_attn.out_proj.bias\t| Mean: 0.0001\t| Std: 0.0253\n",
      "Layer: decoder.model.decoder.layers.5.self_attn_layer_norm.weight\t| Mean: 0.7003\t| Std: 0.0785\n",
      "Layer: decoder.model.decoder.layers.5.self_attn_layer_norm.bias\t| Mean: -0.0888\t| Std: 0.1792\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.k_proj.weight\t| Mean: 0.0001\t| Std: 0.0928\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.k_proj.bias\t| Mean: 0.0044\t| Std: 0.6639\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.v_proj.weight\t| Mean: 0.0000\t| Std: 0.0701\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.v_proj.bias\t| Mean: 0.0004\t| Std: 0.0338\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.q_proj.weight\t| Mean: -0.0003\t| Std: 0.0944\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.q_proj.bias\t| Mean: 0.0009\t| Std: 0.1640\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.out_proj.weight\t| Mean: -0.0000\t| Std: 0.0716\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn.out_proj.bias\t| Mean: -0.0012\t| Std: 0.0953\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight\t| Mean: 0.4481\t| Std: 0.0792\n",
      "Layer: decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias\t| Mean: -0.0740\t| Std: 0.1832\n",
      "Layer: decoder.model.decoder.layers.5.fc1.weight\t| Mean: 0.0097\t| Std: 0.0439\n",
      "Layer: decoder.model.decoder.layers.5.fc1.bias\t| Mean: -0.0646\t| Std: 0.0490\n",
      "Layer: decoder.model.decoder.layers.5.fc2.weight\t| Mean: -0.0000\t| Std: 0.0346\n",
      "Layer: decoder.model.decoder.layers.5.fc2.bias\t| Mean: 0.0003\t| Std: 0.0444\n",
      "Layer: decoder.model.decoder.layers.5.final_layer_norm.weight\t| Mean: 1.9438\t| Std: 0.2239\n",
      "Layer: decoder.model.decoder.layers.5.final_layer_norm.bias\t| Mean: 0.0558\t| Std: 0.0900\n",
      "Layer: decoder.model.decoder.layernorm_embedding.weight\t| Mean: 0.5447\t| Std: 0.0841\n",
      "Layer: decoder.model.decoder.layernorm_embedding.bias\t| Mean: 0.0018\t| Std: 0.0814\n"
     ]
    }
   ],
   "source": [
    "display_weight_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEBUGGING TO INSPECT THE AUDIO\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Load and display the original audio\n",
    "audio_orig, sr_orig = librosa.load(\"../tmp/debug/2277-149896-0000_orig.mp3\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_orig, sr=sr_orig)\n",
    "plt.title(\"Original Audio\")\n",
    "plt.show()\n",
    "\n",
    "# Load and display the post-feature extracted audio\n",
    "audio_post, sr_post = librosa.load(\"../tmp/debug/2277-149896-0000.mp3\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_post, sr=sr_post)\n",
    "plt.title(\"Post-Feature Extracted Audio\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
